<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Sparse Layers - BigDL Project</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "SparseLinear", url: "#sparselinear", children: [
          ]},
          {title: "SparseJoinTable", url: "#sparsejointable", children: [
          ]},
          {title: "DenseToSparse", url: "#densetosparse", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

    <hr />
<h2 id="sparselinear">SparseLinear</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = SparseLinear(
  inputSize,
  outputSize,
  withBias = true,
  backwardStart: Int = -1,
  backwardLength: Int = -1,
  wRegularizer = null,
  bRegularizer = null,
  initWeight = null,
  initBias = null,
  initGradWeight = null,
  initGradBias = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = SparseLinear(
  input_size,
  output_size,
  init_method=&quot;default&quot;,
  with_bias=True,
  backwardStart=-1,
  backwardLength=-1,
  wRegularizer=None,
  bRegularizer=None,
  init_weight=None,
  init_bias=None,
  init_grad_weight=None,
  init_grad_bias=None)
</code></pre>

<p>SparseLinear is the sparse version of module Linear. SparseLinear has two different from Linear: firstly, SparseLinear's input Tensor is a SparseTensor. Secondly, SparseLinear doesn't backward gradient to next layer in the backpropagation by default, as the gradInput of SparseLinear is useless and very big in most cases.</p>
<p>But, considering model like Wide&amp;Deep, we provide backwardStart and backwardLength to backward part of the gradient to next layer.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val module = SparseLinear(1000, 5)

val input = Tensor.sparse(Array(Array(0, 0, 0, 1, 1, 1), Array(1, 5, 300, 2, 100, 500)),
    Array(1f, 3f, 5f, 2f, 4f, 6f),
    Array(2, 1000))

println(module.forward(input))
</code></pre>

<p>Gives the output,</p>
<pre><code>0.047791008 0.069045454 0.020120896 0.019826084 0.10610865  
-0.059406646    -0.13536823 -0.13861635 0.070304416 0.009570055 
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.util.common import *
import numpy as np

module = SparseLinear(1000, 5)

input = JTensor.sparse(
    np.array([1, 3, 5, 2, 4, 6]),
    np.array([0, 0, 0, 1, 1, 1, 1, 5, 300, 2, 100, 500]),
    np.array([2, 1000]))

print(module.forward(input))
</code></pre>

<p>Gives the output,</p>
<pre><code>[[ 10.09569263 -10.94844246  -4.1086688    1.02527523  11.80737209]
 [  7.9651413    9.7131443  -10.22719955   0.02345783  -3.74368906]]
</code></pre>

<hr />
<h2 id="sparsejointable">SparseJoinTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = SparseJoinTable(dimension)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = SparseLinear(dimension)
</code></pre>

<p>Experimental layer.</p>
<p>Sparse version of JoinTable. Backward just pass the origin gradOutput back to the next layers without split. So this layer may just works in Wide&amp;Deep like models.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.utils._

val module = SparseJoinTable(2)

val input1 = Tensor.sparse(Array(Array(0, 0, 0, 1, 1, 1), Array(1, 2, 3, 2, 3, 4)),
    Array(1f, 2f, 3f, 4f, 5f, 6f),
    Array(2, 5))
val input2 = Tensor.sparse(Array(Array(0, 0, 0, 1, 1, 1), Array(2, 3, 4, 1, 2, 3)),
    Array(7f, 8f, 9f, 10f, 11f, 12f),
    Array(2, 5))

println(module.forward(T(input1, input2)))
</code></pre>

<p>Gives the output,</p>
<pre><code>(0, 1) : 1.0
(0, 2) : 2.0
(0, 3) : 3.0
(0, 7) : 7.0
(0, 8) : 8.0
(0, 9) : 9.0
(1, 2) : 4.0
(1, 3) : 5.0
(1, 4) : 6.0
(1, 6) : 10.0
(1, 7) : 11.0
(1, 8) : 12.0
[com.intel.analytics.bigdl.tensor.SparseTensor of size 2x10]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.util.common import *
import numpy as np

module = SparseJoinTable(2)

input1 = JTensor.sparse(np.array([1, 2, 3, 4, 5, 6]),
    np.array([0, 0, 0, 1, 1, 1, 1, 2, 3, 2, 3, 4]),
    np.array([2, 5]))
input2 = JTensor.sparse(np.array([7, 8, 9, 10, 11, 12]),
    np.array([0, 0, 0, 1, 1, 1, 2, 3, 4, 1, 2, 3]),
    np.array([2, 5]))

print(module.forward([input1, input2]))
</code></pre>

<p>Gives the output,
this output is a dense numpy array, due to we couldn't pick SparseTensor back to python currently.</p>
<pre><code>[[  0.   1.   2.   3.   0.   0.   0.   7.   8.   9.]
 [  0.   0.   4.   5.   6.   0.  10.  11.  12.   0.]]
</code></pre>

<hr />
<h2 id="densetosparse">DenseToSparse</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = DenseToSparse()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = DenseToSparse()
</code></pre>

<p>Convert DenseTensor to SparseTensor.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val module = DenseToSparse()

val input = Tensor(2, 3)
input.setValue(1, 1, 1)
input.setValue(2, 2, 2)

println(module.forward(input))
</code></pre>

<p>Gives the output,</p>
<pre><code>(0, 0) : 1.0
(1, 1) : 2.0
[com.intel.analytics.bigdl.tensor.SparseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.util.common import *
import numpy as np

module = DenseToSparse()

input = np.zeros([2, 3])
input[0, 0] = 1
input[1, 1] = 2

print(module.forward(input))
</code></pre>

<p>Gives the output,
this output is a dense numpy array, due to we couldn't pick SparseTensor back to python currently.</p>
<pre><code>[[ 1.  0.  0.]
 [ 0.  2.  0.]]
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>