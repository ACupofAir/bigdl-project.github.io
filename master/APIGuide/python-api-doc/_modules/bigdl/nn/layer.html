
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>bigdl.nn.layer &#8212; BigDL  documentation</title>
    <link rel="stylesheet" href="../../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">BigDL  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for bigdl.nn.layer</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Copyright 2016 The BigDL Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>


<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">JTensor</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">JavaValue</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">callBigDlFunc</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">callJavaFunc</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">get_spark_context</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">to_list</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">INTMAX</span><span class="p">,</span> <span class="n">INTMIN</span><span class="p">,</span> <span class="n">DOUBLEMAX</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">get_activation_by_name</span>
<span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="k">import</span> <span class="n">L1Regularizer</span><span class="p">,</span> <span class="n">L2Regularizer</span><span class="p">,</span> <span class="n">L1L2Regularizer</span>
<span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="k">import</span> <span class="n">JavaObject</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="k">import</span> <span class="n">RDD</span>
<span class="kn">from</span> <span class="nn">bigdl.transform.vision.image</span> <span class="k">import</span> <span class="n">ImageFrame</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">long</span> <span class="o">=</span> <span class="nb">int</span>
    <span class="n">unicode</span> <span class="o">=</span> <span class="nb">str</span>

<div class="viewcode-block" id="Node"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node">[docs]</a><span class="k">class</span> <span class="nc">Node</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represent a node in a graph. The connections between nodes are directed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span> <span class="k">if</span> <span class="n">jvalue</span> <span class="k">else</span> <span class="n">callBigDlFunc</span><span class="p">(</span>
            <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">JavaValue</span><span class="o">.</span><span class="n">jvm_class_constructor</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>

<div class="viewcode-block" id="Node.of"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node.of">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">of</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Node</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="Node.element"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node.element">[docs]</a>    <span class="k">def</span> <span class="nf">element</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">element</span><span class="p">())</span></div>

<div class="viewcode-block" id="Node.remove_pre_edges"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node.remove_pre_edges">[docs]</a>    <span class="k">def</span> <span class="nf">remove_pre_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">removePreEdges</span><span class="p">)</span></div>

<div class="viewcode-block" id="Node.remove_next_edges"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node.remove_next_edges">[docs]</a>    <span class="k">def</span> <span class="nf">remove_next_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">removeNextEdges</span><span class="p">)</span></div></div>



<div class="viewcode-block" id="Layer"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer">[docs]</a><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Layer is the basic component of a neural network</span>
<span class="sd">    and it&#39;s also the base class of layers.</span>
<span class="sd">    Layer can connect to others to construct a complex neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">jvalue</span><span class="p">):</span>
            <span class="k">assert</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">jvalue</span><span class="p">)</span> <span class="o">==</span> <span class="n">JavaObject</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span>
                <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">JavaValue</span><span class="o">.</span><span class="n">jvm_class_constructor</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>

<div class="viewcode-block" id="Layer.set_running_mean"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_running_mean">[docs]</a>    <span class="k">def</span> <span class="nf">set_running_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param running_mean: a ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setRunningMean&quot;</span><span class="p">,</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">running_mean</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.set_running_std"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_running_std">[docs]</a>    <span class="k">def</span> <span class="nf">set_running_std</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">running_std</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param running_mean: a ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setRunningStd&quot;</span><span class="p">,</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">running_std</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &gt;&gt;&gt; conv2 = SpatialConvolution(6, 12, 5, 5).set_name(&quot;conv2&quot;)</span>
<span class="sd">        creating: createSpatialConvolution</span>
<span class="sd">        &gt;&gt;&gt; print(conv2)</span>
<span class="sd">        SpatialConvolution[conv2](6 -&gt; 12, 5 x 5, 1, 1, 0, 0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">toString</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Some other modules point to current module</span>
<span class="sd">        :param x: upstream module nodes. x is either a Node or list of Node.</span>
<span class="sd">        :return: node containing current module</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">Node</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="s2">&quot;createNode&quot;</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="p">,</span>
                                     <span class="n">to_list</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<div class="viewcode-block" id="Layer.of"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.of">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">of</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a Python Layer base on the given java value</span>
<span class="sd">        :param jvalue: Java object create by Py4j</span>
<span class="sd">        :return: A Python Layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="Layer.set_name"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_name">[docs]</a>    <span class="k">def</span> <span class="nf">set_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Give this model a name. There would be a generated name</span>
<span class="sd">        consist of class name and UUID if user doesn&#39;t set it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">setName</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.name"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.name">[docs]</a>    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Name of this layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">getName</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.set_seed"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_seed">[docs]</a>    <span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        You can control the random seed which used to init weights for this model.</span>

<span class="sd">        :param seed: random seed</span>
<span class="sd">        :return: Model itself.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setModelSeed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.get_dtype"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.get_dtype">[docs]</a>    <span class="k">def</span> <span class="nf">get_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;float&quot;</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;float32&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;float64&quot;</span></div>

<div class="viewcode-block" id="Layer.check_input"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.check_input">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">check_input</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param input: ndarray or list of ndarray or JTensor or list of JTensor.</span>
<span class="sd">        :return: (list of JTensor, isTable)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">to_jtensor</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">JTensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">i</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Error unknown input type </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Error when checking: empty input&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">to_jtensor</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="nb">input</span><span class="p">)),</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">to_jtensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)],</span> <span class="kc">False</span></div>

<div class="viewcode-block" id="Layer.convert_output"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.convert_output">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">convert_output</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">is</span> <span class="n">JTensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span>
        <span class="k">elif</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span></div>

<div class="viewcode-block" id="Layer.forward"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        Takes an input object, and computes the corresponding output of the module</span>

<span class="sd">        :param input: ndarray or list of ndarray</span>
<span class="sd">        :param input: ndarray or list of ndarray or JTensor or list of JTensor.</span>
<span class="sd">        :return: ndarray or list of ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jinput</span><span class="p">,</span> <span class="n">input_is_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                               <span class="s2">&quot;modelForward&quot;</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="n">jinput</span><span class="p">,</span>
                               <span class="n">input_is_table</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.backward"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        Performs a back-propagation step through the module, with respect to the given input. In</span>
<span class="sd">        general this method makes the assumption forward(input) has been called before, with the same</span>
<span class="sd">        input. This is necessary for optimization reasons. If you do not respect this rule, backward()</span>
<span class="sd">        will compute incorrect gradients.</span>

<span class="sd">        :param input: ndarray or list of ndarray or JTensor or list of JTensor.</span>
<span class="sd">        :param grad_output: ndarray or list of ndarray or JTensor or list of JTensor.</span>
<span class="sd">        :return: ndarray or list of ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jinput</span><span class="p">,</span> <span class="n">input_is_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">jgrad_output</span><span class="p">,</span> <span class="n">grad_output_is_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                               <span class="s2">&quot;modelBackward&quot;</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="n">jinput</span><span class="p">,</span>
                               <span class="n">input_is_table</span><span class="p">,</span>
                               <span class="n">jgrad_output</span><span class="p">,</span>
                               <span class="n">grad_output_is_table</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.zero_grad_parameters"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.zero_grad_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">zero_grad_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        If the module has parameters, this will zero the accumulation of the gradients with respect</span>
<span class="sd">        to these parameters. Otherwise, it does nothing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">zeroGradParameters</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.update_parameters"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.update_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                      <span class="s2">&quot;updateParameters&quot;</span><span class="p">,</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">learning_rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.reset"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the model weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">reset</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.parameters"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.parameters">[docs]</a>    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the model parameters which containing: weight, bias, gradBias, gradWeight</span>

<span class="sd">        :return: dict(layername -&gt; dict(parametername -&gt; ndarray))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">name_to_params</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="s2">&quot;modelGetParameters&quot;</span><span class="p">,</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">to_ndarray</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">dict</span><span class="p">((</span><span class="n">param_name</span><span class="p">,</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_dtype</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                             <span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span>
                        <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">((</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span>
                <span class="n">name_to_params</span><span class="o">.</span><span class="n">items</span><span class="p">())</span></div>

<div class="viewcode-block" id="Layer.evaluate"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.evaluate">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        No argument passed in:</span>
<span class="sd">        Evaluate the model to set train = false, useful when doing test/forward</span>
<span class="sd">        :return: layer itself</span>

<span class="sd">        Three arguments passed in:</span>
<span class="sd">        A method to benchmark the model quality.</span>

<span class="sd">        :param val_rdd: the input data</span>
<span class="sd">        :param batch_size: batch size</span>
<span class="sd">        :param val_methods: a list of validation methods. i.e: Top1Accuracy,Top5Accuracy and Loss.</span>
<span class="sd">        :return: a list of the metrics result</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                          <span class="s2">&quot;evaluate&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">val_rdd</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">val_methods</span> <span class="o">=</span> <span class="n">args</span>
            <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                                 <span class="s2">&quot;modelEvaluate&quot;</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                 <span class="n">val_rdd</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">val_methods</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Error when calling evaluate(): it takes no argument or exactly three arguments only&quot;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_to_jtensors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">to_list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">JTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Not supported type: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>


<div class="viewcode-block" id="Layer.predict_local"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict_local">[docs]</a>    <span class="k">def</span> <span class="nf">predict_local</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param X: X can be a ndarray or list of ndarray if the model has multiple inputs.</span>
<span class="sd">                  The first dimension of X should be batch.</span>
<span class="sd">        :return: a ndarray as the prediction result.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">jresults</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                             <span class="s2">&quot;predictLocal&quot;</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">_to_jtensors</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">j</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">jresults</span><span class="p">])</span></div>

<div class="viewcode-block" id="Layer.predict_class_local"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict_class_local">[docs]</a>    <span class="k">def</span> <span class="nf">predict_class_local</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param X: X can be a ndarray or list of ndarray if the model has multiple inputs.</span>
<span class="sd">                  The first dimension of X should be batch.</span>
<span class="sd">        :return: a ndarray as the prediction result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                             <span class="s2">&quot;predictLocalClass&quot;</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">_to_jtensors</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.predict"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Model inference base on the given data.</span>
<span class="sd">        :param features: it can be a ndarray or list of ndarray for locally inference</span>
<span class="sd">                         or RDD[Sample] for running in distributed fashion</span>
<span class="sd">        :return: ndarray or RDD[Sample] depend on the the type of features.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">RDD</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_distributed</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_local</span><span class="p">(</span><span class="n">features</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.predict_class"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict_class">[docs]</a>    <span class="k">def</span> <span class="nf">predict_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Model inference base on the given data which returning label</span>
<span class="sd">        :param features: it can be a ndarray or list of ndarray for locally inference</span>
<span class="sd">                         or RDD[Sample] for running in distributed fashion</span>
<span class="sd">        :return: ndarray or RDD[Sample] depend on the the type of features.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">RDD</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_class_distributed</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_class_local</span><span class="p">(</span><span class="n">features</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.predict_distributed"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict_distributed">[docs]</a>    <span class="k">def</span> <span class="nf">predict_distributed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_rdd</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Model inference base on the given data.</span>
<span class="sd">        You need to invoke collect() to trigger those action \</span>
<span class="sd">        as the returning result is an RDD.</span>

<span class="sd">        :param data_rdd: the data to be predict.</span>
<span class="sd">        :return: An RDD represent the predict result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                             <span class="s2">&quot;modelPredictRDD&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">data_rdd</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">data</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">())</span></div>

<div class="viewcode-block" id="Layer.predict_class_distributed"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict_class_distributed">[docs]</a>    <span class="k">def</span> <span class="nf">predict_class_distributed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_rdd</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        module predict, return the predict label</span>

<span class="sd">        :param data_rdd: the data to be predict.</span>
<span class="sd">        :return: An RDD represent the predict label.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                               <span class="s2">&quot;modelPredictClass&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">data_rdd</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="Layer.predict_image"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict_image">[docs]</a>    <span class="k">def</span> <span class="nf">predict_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_frame</span><span class="p">,</span> <span class="n">output_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">share_buffer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">batch_per_partition</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">predict_key</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        model predict images, return imageFrame with predicted tensor</span>
<span class="sd">        :param image_frame imageFrame that contains images</span>
<span class="sd">        :param output_layer if output_layer is not null, the output of layer that matches</span>
<span class="sd">        output_layer will be used as predicted output</span>
<span class="sd">        :param share_buffer whether to share same memory for each batch predict results</span>
<span class="sd">        :param batch_per_partition batch size per partition, default is 4</span>
<span class="sd">        :param predict_key key to store predicted results</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">image_frame</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;modelPredictImage&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">image_frame</span><span class="p">,</span>
                             <span class="n">output_layer</span><span class="p">,</span>
                             <span class="n">share_buffer</span><span class="p">,</span>
                             <span class="n">batch_per_partition</span><span class="p">,</span>
                             <span class="n">predict_key</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ImageFrame</span><span class="p">(</span><span class="n">image_frame</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.set_weights"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_weights">[docs]</a>    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set weights for this layer</span>

<span class="sd">        :param weights: a list of numpy arrays which represent weight and bias</span>
<span class="sd">        :return:</span>

<span class="sd">        &gt;&gt;&gt; linear = Linear(3,2)</span>
<span class="sd">        creating: createLinear</span>
<span class="sd">        &gt;&gt;&gt; linear.set_weights([np.array([[1,2,3],[4,5,6]]), np.array([7,8])])</span>
<span class="sd">        &gt;&gt;&gt; weights = linear.get_weights()</span>
<span class="sd">        &gt;&gt;&gt; weights[0].shape == (2,3)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(weights[0][0], np.array([1., 2., 3.]))</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(weights[1], np.array([7., 8.]))</span>
<span class="sd">        &gt;&gt;&gt; relu = ReLU()</span>
<span class="sd">        creating: createReLU</span>
<span class="sd">        &gt;&gt;&gt; from py4j.protocol import Py4JJavaError</span>
<span class="sd">        &gt;&gt;&gt; try:</span>
<span class="sd">        ...     relu.set_weights([np.array([[1,2,3],[4,5,6]]), np.array([7,8])])</span>
<span class="sd">        ... except Py4JJavaError as err:</span>
<span class="sd">        ...     print(err.java_exception)</span>
<span class="sd">        ...</span>
<span class="sd">        java.lang.IllegalArgumentException: requirement failed: this layer does not have weight/bias</span>
<span class="sd">        &gt;&gt;&gt; relu.get_weights()</span>
<span class="sd">        The layer does not have weight/bias</span>
<span class="sd">        &gt;&gt;&gt; add = Add(2)</span>
<span class="sd">        creating: createAdd</span>
<span class="sd">        &gt;&gt;&gt; try:</span>
<span class="sd">        ...     add.set_weights([np.array([7,8]), np.array([1,2])])</span>
<span class="sd">        ... except Py4JJavaError as err:</span>
<span class="sd">        ...     print(err.java_exception)</span>
<span class="sd">        ...</span>
<span class="sd">        java.lang.IllegalArgumentException: requirement failed: the number of input weight/bias is not consistant with number of weight/bias of this layer, number of input 1, number of output 2</span>
<span class="sd">        &gt;&gt;&gt; cAdd = CAdd([4, 1])</span>
<span class="sd">        creating: createCAdd</span>
<span class="sd">        &gt;&gt;&gt; cAdd.set_weights(np.ones([4, 1]))</span>
<span class="sd">        &gt;&gt;&gt; (cAdd.get_weights()[0] == np.ones([4, 1])).all()</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">to_list</span><span class="p">(</span><span class="n">weights</span><span class="p">)]</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setWeights&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">tensors</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.get_weights"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.get_weights">[docs]</a>    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get weights for this layer</span>

<span class="sd">        :return: list of numpy arrays which represent weight and bias</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensorWeights</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                              <span class="s2">&quot;getWeights&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensorWeights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensorWeights</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The layer does not have weight/bias&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="Layer.is_with_weights"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.is_with_weights">[docs]</a>    <span class="k">def</span> <span class="nf">is_with_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                  <span class="s2">&quot;isWithWeights&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.save"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">over_write</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;modelSave&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span>
                      <span class="n">over_write</span><span class="p">)</span></div>
<div class="viewcode-block" id="Layer.saveModel"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.saveModel">[docs]</a>    <span class="k">def</span> <span class="nf">saveModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">weightPath</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">over_write</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;saveBigDLModule&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span>
                      <span class="n">weightPath</span><span class="p">,</span> <span class="n">over_write</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.save_caffe"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.save_caffe">[docs]</a>    <span class="k">def</span> <span class="nf">save_caffe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prototxt_path</span><span class="p">,</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">use_v2</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;saveCaffe&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">prototxt_path</span><span class="p">,</span>
                      <span class="n">model_path</span><span class="p">,</span> <span class="n">use_v2</span><span class="p">,</span> <span class="n">overwrite</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.save_tensorflow"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.save_tensorflow">[docs]</a>    <span class="k">def</span> <span class="nf">save_tensorflow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">byte_order</span><span class="o">=</span><span class="s2">&quot;little_endian&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;nhwc&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save a model to protobuf files so that it can be used in tensorflow inference.</span>

<span class="sd">        When saving the model, placeholders will be added to the tf model as input nodes. So</span>
<span class="sd">        you need to pass in the names and shapes of the placeholders. BigDL model doesn&#39;t have</span>
<span class="sd">        such information. The order of the placeholder information should be same as the inputs</span>
<span class="sd">        of the graph model.</span>
<span class="sd">        :param inputs: placeholder information, should be an array of tuples (input_name, shape)</span>
<span class="sd">                       where &#39;input_name&#39; is a string and shape is an array of integer</span>
<span class="sd">        :param path: the path to be saved to</span>
<span class="sd">        :param byte_order: model byte order</span>
<span class="sd">        :param data_format: model data format, should be &quot;nhwc&quot; or &quot;nchw&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;saveTF&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">byte_order</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<div class="viewcode-block" id="Layer.setWRegularizer"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.setWRegularizer">[docs]</a>    <span class="k">def</span> <span class="nf">setWRegularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        set weight regularizer</span>
<span class="sd">        :param wRegularizer: weight regularizer</span>
<span class="sd">        :return:</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">wRegularizer</span> <span class="o">=</span> <span class="n">wRegularizer</span><span class="o">.</span><span class="n">value</span></div>

<div class="viewcode-block" id="Layer.setBRegularizer"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.setBRegularizer">[docs]</a>    <span class="k">def</span> <span class="nf">setBRegularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        set bias regularizer</span>
<span class="sd">        :param wRegularizer: bias regularizer</span>
<span class="sd">        :return:</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">bRegularizer</span> <span class="o">=</span> <span class="n">bRegularizer</span><span class="o">.</span><span class="n">value</span></div>

<div class="viewcode-block" id="Layer.freeze"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.freeze">[docs]</a>    <span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        freeze module, if names is not None, set an array of layers that match given names</span>
<span class="sd">        to be freezed</span>
<span class="sd">        :param names: an array of layer names</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;freeze&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.unfreeze"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.unfreeze">[docs]</a>    <span class="k">def</span> <span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        unfreeze module, if names is not None, unfreeze layers that match given names</span>
<span class="sd">        :param names: an array of layer names</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;unFreeze&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.training"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.training">[docs]</a>    <span class="k">def</span> <span class="nf">training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Set this layer in the training mode or in predition mode if is_training=False</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
            <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">evaluate</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.is_training"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.is_training">[docs]</a>    <span class="k">def</span> <span class="nf">is_training</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        :return: Whether this layer is in the training mode</span>

<span class="sd">        &gt;&gt;&gt; layer = Dropout()</span>
<span class="sd">        creating: createDropout</span>
<span class="sd">        &gt;&gt;&gt; layer = layer.evaluate()</span>
<span class="sd">        &gt;&gt;&gt; layer.is_training()</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; layer = layer.training()</span>
<span class="sd">        &gt;&gt;&gt; layer.is_training()</span>
<span class="sd">        True</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">isTraining</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.quantize"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.quantize">[docs]</a>    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Clone self and quantize it, at last return a new quantized model.</span>
<span class="sd">        :return: A new quantized model.</span>

<span class="sd">        &gt;&gt;&gt; fc = Linear(4, 2)</span>
<span class="sd">        creating: createLinear</span>
<span class="sd">        &gt;&gt;&gt; fc.set_weights([np.ones((2, 4)), np.ones((2,))])</span>
<span class="sd">        &gt;&gt;&gt; input = np.ones((2, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = fc.forward(input)</span>
<span class="sd">        &gt;&gt;&gt; expected_output = np.array([[5., 5.], [5., 5.]])</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(output, expected_output)</span>
<span class="sd">        &gt;&gt;&gt; quantized_fc = fc.quantize()</span>
<span class="sd">        &gt;&gt;&gt; quantized_output = quantized_fc.forward(input)</span>
<span class="sd">        &gt;&gt;&gt; expected_quantized_output = np.array([[5., 5.], [5., 5.]])</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(quantized_output, expected_quantized_output)</span>

<span class="sd">        &gt;&gt;&gt; assert(&quot;quantized.Linear&quot; in quantized_fc.__str__())</span>
<span class="sd">        &gt;&gt;&gt; conv = SpatialConvolution(1, 2, 3, 3)</span>
<span class="sd">        creating: createSpatialConvolution</span>
<span class="sd">        &gt;&gt;&gt; conv.set_weights([np.ones((2, 1, 3, 3)), np.zeros((2,))])</span>
<span class="sd">        &gt;&gt;&gt; input = np.ones((2, 1, 4, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = conv.forward(input)</span>
<span class="sd">        &gt;&gt;&gt; expected_output = np.array([[[[9., 9.], [9., 9.]], [[9., 9.], [9., 9.]]], [[[9., 9.], [9., 9.]], [[9., 9.], [9., 9.]]]])</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(output, expected_output)</span>
<span class="sd">        &gt;&gt;&gt; quantized_conv = conv.quantize()</span>
<span class="sd">        &gt;&gt;&gt; quantized_output = quantized_conv.forward(input)</span>
<span class="sd">        &gt;&gt;&gt; expected_quantized_output = np.array([[[[9., 9.], [9., 9.]], [[9., 9.], [9., 9.]]], [[[9., 9.], [9., 9.]], [[9., 9.], [9., 9.]]]])</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(quantized_output, expected_quantized_output)</span>
<span class="sd">        &gt;&gt;&gt; assert(&quot;quantized.SpatialConvolution&quot; in quantized_conv.__str__())</span>
<span class="sd">        &gt;&gt;&gt; seq = Sequential()</span>
<span class="sd">        creating: createSequential</span>
<span class="sd">        &gt;&gt;&gt; seq = seq.add(conv)</span>
<span class="sd">        &gt;&gt;&gt; seq = seq.add(Reshape([8, 4], False))</span>
<span class="sd">        creating: createReshape</span>
<span class="sd">        &gt;&gt;&gt; seq = seq.add(fc)</span>
<span class="sd">        &gt;&gt;&gt; input = np.ones([1, 1, 6, 6])</span>
<span class="sd">        &gt;&gt;&gt; output = seq.forward(input)</span>
<span class="sd">        &gt;&gt;&gt; expected_output = np.array([[37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.]])</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(output, expected_output)</span>
<span class="sd">        &gt;&gt;&gt; quantized_seq = seq.quantize()</span>
<span class="sd">        &gt;&gt;&gt; quantized_output = quantized_seq.forward(input)</span>
<span class="sd">        &gt;&gt;&gt; expected_quantized_output = np.array([[37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.], [37., 37.]])</span>
<span class="sd">        &gt;&gt;&gt; np.testing.assert_allclose(quantized_output, expected_quantized_output)</span>
<span class="sd">        &gt;&gt;&gt; assert(&quot;quantized.Linear&quot; in quantized_seq.__str__())</span>
<span class="sd">        &gt;&gt;&gt; assert(&quot;quantized.SpatialConvolution&quot; in quantized_seq.__str__())</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;quantize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Container"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Container">[docs]</a><span class="k">class</span> <span class="nc">Container</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">     [[Container]] is a sub-class of Model that declares methods defined in all containers.</span>
<span class="sd">     A container usually contain some other modules which can be added through the &quot;add&quot; method</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Container</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

<div class="viewcode-block" id="Container.add"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Container.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">jlayers</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;getContainerModules&quot;</span> <span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jlayer</span><span class="p">)</span> <span class="k">for</span> <span class="n">jlayer</span> <span class="ow">in</span> <span class="n">jlayers</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">layers</span>

<div class="viewcode-block" id="Container.flattened_layers"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Container.flattened_layers">[docs]</a>    <span class="k">def</span> <span class="nf">flattened_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">include_container</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">jlayers</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;getFlattenModules&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">include_container</span><span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jlayer</span><span class="p">)</span> <span class="k">for</span> <span class="n">jlayer</span> <span class="ow">in</span> <span class="n">jlayers</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">layers</span></div></div>


<div class="viewcode-block" id="Model"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model">[docs]</a><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A graph container. Each node can have multiple inputs. The output of the node should be a</span>
<span class="sd">    tensor. The output tensor can be connected to multiple nodes. So the module in each node can</span>
<span class="sd">    have a tensor or table input, and should have a tensor output.</span>

<span class="sd">    The graph container can have multiple inputs and multiple outputs. If there&#39;s one input,</span>
<span class="sd">    the input data fed to the graph module should be a tensor. If there&#39;re multiple inputs,</span>
<span class="sd">    the input data fed to the graph module should be a table, which is actually an sequence of</span>
<span class="sd">    tensor. The order of the input tensors should be same with the order of the input nodes.</span>
<span class="sd">    This is also applied to the gradient from the module in the back propagation.</span>

<span class="sd">    If there&#39;s one output, the module output is a tensor. If there&#39;re multiple outputs, the module</span>
<span class="sd">    output is a table, which is actually an sequence of tensor. The order of the output tensors is</span>
<span class="sd">    same with the order of the output modules. This is also applied to the gradient passed to the</span>
<span class="sd">    module in the back propagation.</span>

<span class="sd">    All inputs should be able to connect to outputs through some paths in the graph.</span>
<span class="sd">    It is allowed that some successors of the inputs node are not connect to outputs.</span>
<span class="sd">    If so, these nodes will be excluded in the computation.</span>
<span class="sd">    </span>
<span class="sd">    We also support initializing a Graph directly from a tensorflow module. In this case, you should</span>
<span class="sd">    pass your tensorflow nodes as inputs and outputs and also specify the byte_order parameter (&quot;little_endian&quot;</span>
<span class="sd">     or &quot;big_endian&quot;) and node_type parameter (&quot;bigdl&quot; or &quot;tensorflow&quot;)</span>
<span class="sd">    node_type parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inputs</span><span class="p">,</span>
                 <span class="n">outputs</span><span class="p">,</span>
                 <span class="n">jvalue</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">,</span> <span class="n">byte_order</span><span class="o">=</span><span class="s2">&quot;little_endian&quot;</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;bigdl&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">jvalue</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>
        <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;bigdl&quot;</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">to_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
                                    <span class="n">to_list</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">bigdl.util.tf_utils</span> <span class="k">import</span> <span class="n">convert</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">to_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">to_list</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span> <span class="n">byte_order</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>


<div class="viewcode-block" id="Model.from_jvalue"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.from_jvalue">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_jvalue</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a Python Model base on the given java value</span>
<span class="sd">        :param jvalue: Java object create by Py4j</span>
<span class="sd">        :return: A Python Model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">jvalue</span><span class="o">=</span><span class="n">jvalue</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span>
        <span class="k">return</span> <span class="n">model</span></div>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;-&gt;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">())</span>

<div class="viewcode-block" id="Model.load"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Bigdl model.</span>

<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadBigDL&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.loadModel"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.loadModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadModel</span><span class="p">(</span><span class="n">modelPath</span><span class="p">,</span> <span class="n">weightPath</span> <span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Bigdl model.</span>

<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadBigDLModule&quot;</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">weightPath</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_torch"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_torch">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_torch</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Torch model.</span>

<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadTorch&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_keras"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_keras">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_keras</span><span class="p">(</span><span class="n">json_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hdf5_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">by_name</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Keras model.</span>

<span class="sd">        :param json_path: The json path containing the keras model definition.</span>
<span class="sd">        :param hdf5_path: The HDF5 path containing the pre-trained keras model weights with or without the model architecture.</span>
<span class="sd">        :return: A bigdl model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">os</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">tensorflow</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;KERAS_BACKEND&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;theano&quot;</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Make theano backend compatible with Python3</span>
                <span class="kn">from</span> <span class="nn">theano</span> <span class="k">import</span> <span class="n">ifelse</span>
            <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;No backend is found for Keras. &quot;</span>
                                <span class="s2">&quot;Please install either tensorflow or theano.&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">bigdl.keras.converter</span> <span class="k">import</span> <span class="n">DefinitionLoader</span><span class="p">,</span> <span class="n">WeightLoader</span>
        <span class="k">if</span> <span class="n">json_path</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">hdf5_path</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">DefinitionLoader</span><span class="o">.</span><span class="n">from_json_path</span><span class="p">(</span><span class="n">json_path</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">json_path</span> <span class="ow">and</span> <span class="n">hdf5_path</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">WeightLoader</span><span class="o">.</span><span class="n">load_weights_from_json_hdf5</span><span class="p">(</span><span class="n">json_path</span><span class="p">,</span> <span class="n">hdf5_path</span><span class="p">,</span> <span class="n">by_name</span><span class="o">=</span><span class="n">by_name</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">hdf5_path</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">json_path</span><span class="p">:</span>
            <span class="n">kmodel</span><span class="p">,</span> <span class="n">bmodel</span> <span class="o">=</span> <span class="n">DefinitionLoader</span><span class="o">.</span><span class="n">from_hdf5_path</span><span class="p">(</span><span class="n">hdf5_path</span><span class="p">)</span>
            <span class="n">WeightLoader</span><span class="o">.</span><span class="n">load_weights_from_kmodel</span><span class="p">(</span><span class="n">bmodel</span><span class="p">,</span> <span class="n">kmodel</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">bmodel</span></div>

<div class="viewcode-block" id="Model.load_caffe"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_caffe">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_caffe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">defPath</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">match_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Caffe model.</span>


<span class="sd">        :param model: A bigdl model definition \which equivalent to the pre-trained caffe model.</span>
<span class="sd">        :param defPath: The path containing the caffe model definition.</span>
<span class="sd">        :param modelPath: The path containing the pre-trained caffe model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadCaffe&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">defPath</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">match_all</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_caffe_model"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_caffe_model">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_caffe_model</span><span class="p">(</span><span class="n">defPath</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Caffe model.</span>


<span class="sd">        :param defPath: The path containing the caffe model definition.</span>
<span class="sd">        :param modelPath: The path containing the pre-trained caffe model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadCaffeModel&quot;</span><span class="p">,</span> <span class="n">defPath</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_tensorflow"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_tensorflow">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_tensorflow</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">byte_order</span> <span class="o">=</span> <span class="s2">&quot;little_endian&quot;</span><span class="p">,</span>
                        <span class="n">bin_file</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Tensorflow model.</span>
<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :param inputs: The input node of this graph</span>
<span class="sd">        :param outputs: The output node of this graph</span>
<span class="sd">        :param byte_order: byte_order of the file, `little_endian` or `big_endian`</span>
<span class="sd">        :param bin_file: the optional bin file produced by bigdl dump_model util function to store the weights</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadTF&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">byte_order</span><span class="p">,</span> <span class="n">bin_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.train"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.train">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">opt_method</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">end_when</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.util.tf_utils</span> <span class="k">import</span> <span class="n">get_path</span>
        <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">Sample</span>
        <span class="n">output_name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">get_path</span><span class="p">(</span><span class="n">output_name</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">get_spark_context</span><span class="p">()</span>
        <span class="n">rdd_train_images</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">rdd_train_labels</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        <span class="n">rdd_train_sample</span> <span class="o">=</span> <span class="n">rdd_train_images</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span><span class="n">rdd_train_labels</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="nb">input</span><span class="p">:</span>
                                                                      <span class="n">Sample</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;trainTF&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">output_name</span><span class="p">,</span> <span class="n">rdd_train_sample</span><span class="p">,</span> <span class="n">opt_method</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">end_when</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.stop_gradient"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.stop_gradient">[docs]</a>    <span class="k">def</span> <span class="nf">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stop_layers</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        stop the input gradient of layers that match the given ```names```</span>
<span class="sd">        their input gradient are not computed.</span>
<span class="sd">        And they will not contributed to the input gradient computation of</span>
<span class="sd">        layers that depend on them.</span>
<span class="sd">        :param stop_layers:  an array of layer names</span>
<span class="sd">        :param bigdl_type:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setStopGradient&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">stop_layers</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Model.node"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.node">[docs]</a>    <span class="k">def</span> <span class="nf">node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the corresponding node has the given name. If the given name doesn&#39;t match any node,</span>
<span class="sd">        an exception will be thrown</span>
<span class="sd">        :param name: node name</span>
<span class="sd">        :param bigdl_type: </span>
<span class="sd">        :return: </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jnode</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;findGraphNode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Node</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jnode</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.save_graph_topology"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.save_graph_topology">[docs]</a>    <span class="k">def</span> <span class="nf">save_graph_topology</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_path</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        save current model graph to a folder, which can be display in tensorboard by running</span>
<span class="sd">            tensorboard --logdir logPath</span>
<span class="sd">        :param log_path: path to save the model graph</span>
<span class="sd">        :param bigdl_type:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;saveGraphTopology&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">log_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="Linear"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Linear">[docs]</a><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The [[Linear]] module applies a linear transformation to the input data,</span>
<span class="sd">    i.e. `y = Wx + b`. The input given in `forward(input)` must be either</span>
<span class="sd">    a vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must</span>
<span class="sd">    have the size of `inputSize`. If it is a matrix, then each row is assumed to be</span>
<span class="sd">    an input sample of given batch (the number of rows means the batch size and</span>
<span class="sd">    the number of columns should be equal to the `inputSize`).</span>

<span class="sd">    :param input_size the size the each input sample</span>
<span class="sd">    :param output_size the size of the module output of each sample</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param init_weight: the optional initial value for the weight</span>
<span class="sd">    :param init_bias: the optional initial value for the bias</span>
<span class="sd">    :param init_grad_weight: the optional initial value for the grad_weight</span>
<span class="sd">    :param init_grad_bias: the optional initial value for the grad_bias</span>


<span class="sd">    &gt;&gt;&gt; linear = Linear(100, 10, True, L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(10, 100)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(10)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([10, 100])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([10])</span>
<span class="sd">    &gt;&gt;&gt; linear = Linear(100, 10, True, L1Regularizer(0.5), L1Regularizer(0.5), init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                                     <span class="n">with_bias</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">,</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>

<div class="viewcode-block" id="Linear.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Linear.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                   <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="SparseLinear"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SparseLinear">[docs]</a><span class="k">class</span> <span class="nc">SparseLinear</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    SparseLinear is the sparse version of module Linear. SparseLinear has two different from Linear:</span>
<span class="sd">    firstly, SparseLinear&#39;s input Tensor is a SparseTensor. Secondly, SparseLinear doesn&#39;t backward</span>
<span class="sd">    gradient to next layer in the backpropagation by default, as the gradInput of SparseLinear is</span>
<span class="sd">    useless and very big in most cases.</span>

<span class="sd">    But, considering model like Wide&amp;Deep, we provide backwardStart and backwardLength to backward</span>
<span class="sd">    part of the gradient to next layer.</span>

<span class="sd">    :param input_size the size the each input sample</span>
<span class="sd">    :param output_size the size of the module output of each sample</span>
<span class="sd">    :param backwardStart backwardStart index, counting from 1</span>
<span class="sd">    :param backwardLength backward length</span>
<span class="sd">    :param withBias if has bias</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param init_weight: the optional initial value for the weight</span>
<span class="sd">    :param init_bias: the optional initial value for the bias</span>
<span class="sd">    :param init_grad_weight: the optional initial value for the grad_weight</span>
<span class="sd">    :param init_grad_bias: the optional initial value for the grad_bias</span>


<span class="sd">    &gt;&gt;&gt; sparselinear = SparseLinear(100, 10, True, wRegularizer=L1Regularizer(0.5), bRegularizer=L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createSparseLinear</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(10, 100)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(10)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([10, 100])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([10])</span>
<span class="sd">    &gt;&gt;&gt; sparselinear = SparseLinear(100, 10, True, 1, 5, L1Regularizer(0.5), L1Regularizer(0.5), init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createSparseLinear</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(123)</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(5, 1000)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(5)</span>
<span class="sd">    &gt;&gt;&gt; sparselinear = SparseLinear(1000, 5, init_weight=init_weight, init_bias=init_bias)</span>
<span class="sd">    creating: createSparseLinear</span>
<span class="sd">    &gt;&gt;&gt; input = JTensor.sparse(np.array([1, 3, 5, 2, 4, 6]), np.array([0, 0, 0, 1, 1, 1, 1, 5, 300, 2, 100, 500]), np.array([2, 1000]))</span>
<span class="sd">    &gt;&gt;&gt; output = sparselinear.forward(input)</span>
<span class="sd">    &gt;&gt;&gt; expected_output = np.array([[10.09569263, -10.94844246, -4.1086688, 1.02527523, 11.80737209], [7.9651413, 9.7131443, -10.22719955, 0.02345783, -3.74368906]])</span>
<span class="sd">    &gt;&gt;&gt; np.testing.assert_allclose(output, expected_output, rtol=1e-6, atol=1e-6)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">backwardStart</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">backwardLength</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SparseLinear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                                     <span class="n">with_bias</span><span class="p">,</span> <span class="n">backwardStart</span><span class="p">,</span> <span class="n">backwardLength</span><span class="p">,</span>
                                     <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">,</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>

<div class="viewcode-block" id="SparseLinear.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SparseLinear.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="DenseToSparse"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.DenseToSparse">[docs]</a><span class="k">class</span> <span class="nc">DenseToSparse</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Convert DenseTensor to SparseTensor.</span>


<span class="sd">    &gt;&gt;&gt; DenseToSparse = DenseToSparse()</span>
<span class="sd">    creating: createDenseToSparse</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DenseToSparse</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the rectified linear unit (ReLU) function element-wise to the input Tensor,</span>
<span class="sd">     thus outputting a Tensor of the same dimension.</span>


<span class="sd">    ReLU is defined as: f(x) = max(0, x)</span>
<span class="sd">    Can optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; relu = ReLU()</span>
<span class="sd">    creating: createReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ip</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">ip</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the Tanh function element-wise to the input Tensor, thus outputting a Tensor of the same</span>
<span class="sd">    dimension. Tanh is defined as f(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).</span>


<span class="sd">    &gt;&gt;&gt; tanh = Tanh()</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Tanh</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the Sigmoid function element-wise to the input Tensor,</span>
<span class="sd">    thus outputting a Tensor of the same dimension.</span>

<span class="sd">    &gt;&gt;&gt; sigmoid = Sigmoid()</span>
<span class="sd">    creating: createSigmoid</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Echo"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Echo">[docs]</a><span class="k">class</span> <span class="nc">Echo</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This module is for debug purpose, which can print activation and gradient in your model</span>
<span class="sd">    topology</span>


<span class="sd">    &gt;&gt;&gt; echo = Echo()</span>
<span class="sd">    creating: createEcho</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Echo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSoftMax"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LogSoftMax">[docs]</a><span class="k">class</span> <span class="nc">LogSoftMax</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the LogSoftMax function to an n-dimensional input Tensor.</span>
<span class="sd">    LogSoftmax is defined as: f_i(x) = log(1 / a exp(x_i))</span>
<span class="sd">    where a = sum_j[exp(x_j)].</span>


<span class="sd">    &gt;&gt;&gt; logSoftMax = LogSoftMax()</span>
<span class="sd">    creating: createLogSoftMax</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogSoftMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sequential"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sequential">[docs]</a><span class="k">class</span> <span class="nc">Sequential</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Sequential provides a means to plug layers together</span>
<span class="sd">    in a feed-forward fully connected manner.</span>


<span class="sd">    &gt;&gt;&gt; echo = Echo()</span>
<span class="sd">    creating: createEcho</span>
<span class="sd">    &gt;&gt;&gt; s = Sequential()</span>
<span class="sd">    creating: createSequential</span>
<span class="sd">    &gt;&gt;&gt; s = s.add(echo)</span>
<span class="sd">    &gt;&gt;&gt; s = s.add(s)</span>
<span class="sd">    &gt;&gt;&gt; s = s.add(echo)</span>


<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sequential</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="TemporalConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TemporalConvolution">[docs]</a><span class="k">class</span> <span class="nc">TemporalConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a 1D convolution over an input sequence composed of nInputFrame frames..</span>
<span class="sd">    The input tensor in `forward(input)` is expected to be a 2D tensor</span>
<span class="sd">    (`nInputFrame` x `inputFrameSize`) or a 3D tensor</span>
<span class="sd">    (`nBatchFrame` x `nInputFrame` x `inputFrameSize`).</span>

<span class="sd">    :param input_frame_size The input frame size expected in sequences given into `forward()`</span>
<span class="sd">    :param output_frame_size The output frame size the convolution layer will produce.</span>
<span class="sd">    :param kernel_w The kernel width of the convolution</span>
<span class="sd">    :param stride_w The step of the convolution in the width dimension.</span>
<span class="sd">    :param propagate_back Whether propagate gradient back, default is true.</span>
<span class="sd">    :param weight_regularizer instance of [[Regularizer]]</span>
<span class="sd">                        (eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bias_regularizer instance of [[Regularizer]]</span>
<span class="sd">                         applied to the bias.</span>
<span class="sd">    :param init_weight Initial weight</span>
<span class="sd">    :param init_bias Initial bias</span>
<span class="sd">    :param init_grad_weight Initial gradient weight</span>
<span class="sd">    :param init_grad_bias Initial gradient bias</span>

<span class="sd">    &gt;&gt;&gt; temporalConvolution = TemporalConvolution(6, 12, 5, 5)</span>
<span class="sd">    creating: createTemporalConvolution</span>
<span class="sd">    &gt;&gt;&gt; temporalConvolution.setWRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; temporalConvolution.setBRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_frame_size</span><span class="p">,</span>
                 <span class="n">output_frame_size</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">weight_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bias_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TemporalConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">input_frame_size</span><span class="p">,</span>
                                                 <span class="n">output_frame_size</span><span class="p">,</span>
                                                 <span class="n">kernel_w</span><span class="p">,</span>
                                                 <span class="n">stride_w</span><span class="p">,</span>
                                                 <span class="n">propagate_back</span><span class="p">,</span>
                                                 <span class="n">weight_regularizer</span><span class="p">,</span>
                                                 <span class="n">bias_regularizer</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>
<div class="viewcode-block" id="TemporalConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TemporalConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="LocallyConnected1D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LocallyConnected1D">[docs]</a><span class="k">class</span> <span class="nc">LocallyConnected1D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The `LocallyConnected1D` layer works similarly to</span>
<span class="sd">    the `TemporalConvolution` layer, except that weights are unshared,</span>
<span class="sd">    that is, a different set of filters is applied at each different patch</span>
<span class="sd">    of the input.</span>
<span class="sd">    The input tensor in `forward(input)` is expected to be a 2D tensor</span>
<span class="sd">    (`nInputFrame` x `inputFrameSize`) or a 3D tensor</span>
<span class="sd">    (`nBatchFrame` x `nInputFrame` x `inputFrameSize`).</span>
<span class="sd">    :param nInputFrame the input frame channel</span>
<span class="sd">    :param input_frame_size The input frame size expected in sequences given into `forward()`</span>
<span class="sd">    :param output_frame_size The output frame size the convolution layer will produce.</span>
<span class="sd">    :param kernel_w The kernel width of the convolution</span>
<span class="sd">    :param stride_w The step of the convolution in the width dimension.</span>
<span class="sd">    :param propagate_back Whether propagate gradient back, default is true.</span>
<span class="sd">    :param weight_regularizer instance of [[Regularizer]]</span>
<span class="sd">                        (eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bias_regularizer instance of [[Regularizer]]</span>
<span class="sd">                         applied to the bias.</span>
<span class="sd">    :param init_weight Initial weight</span>
<span class="sd">    :param init_bias Initial bias</span>
<span class="sd">    :param init_grad_weight Initial gradient weight</span>
<span class="sd">    :param init_grad_bias Initial gradient bias</span>
<span class="sd">    &gt;&gt;&gt; locallyConnected1D = LocallyConnected1D(10, 6, 12, 5, 5)</span>
<span class="sd">    creating: createLocallyConnected1D</span>
<span class="sd">    &gt;&gt;&gt; locallyConnected1D.setWRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; locallyConnected1D.setBRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_frame</span><span class="p">,</span>
                 <span class="n">input_frame_size</span><span class="p">,</span>
                 <span class="n">output_frame_size</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">weight_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bias_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LocallyConnected1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_input_frame</span><span class="p">,</span>
                                                 <span class="n">input_frame_size</span><span class="p">,</span>
                                                 <span class="n">output_frame_size</span><span class="p">,</span>
                                                 <span class="n">kernel_w</span><span class="p">,</span>
                                                 <span class="n">stride_w</span><span class="p">,</span>
                                                 <span class="n">propagate_back</span><span class="p">,</span>
                                                 <span class="n">weight_regularizer</span><span class="p">,</span>
                                                 <span class="n">bias_regularizer</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>

<div class="viewcode-block" id="LocallyConnected1D.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LocallyConnected1D.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="BinaryTreeLSTM"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BinaryTreeLSTM">[docs]</a><span class="k">class</span> <span class="nc">BinaryTreeLSTM</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is an implementation of Binary TreeLSTM (Constituency Tree LSTM).</span>
<span class="sd">    :param inputSize input units size</span>
<span class="sd">    :param hiddenSize hidden units size</span>
<span class="sd">    :param gateOutput whether gate output</span>
<span class="sd">    :param withGraph whether create lstms with [[Graph]], the default value is true.</span>
<span class="sd">    &gt;&gt;&gt; treeLSTM = BinaryTreeLSTM(100, 200)</span>
<span class="sd">    creating: createBinaryTreeLSTM</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">gate_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">with_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BinaryTreeLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                                             <span class="n">bigdl_type</span><span class="p">,</span>
                                             <span class="n">input_size</span><span class="p">,</span>
                                             <span class="n">hidden_size</span><span class="p">,</span>
                                             <span class="n">gate_output</span><span class="p">,</span>
                                             <span class="n">with_graph</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocallyConnected2D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LocallyConnected2D">[docs]</a><span class="k">class</span> <span class="nc">LocallyConnected2D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The LocallyConnected2D layer works similarly to the [[SpatialConvolution]] layer,</span>
<span class="sd">    except that weights are unshared, that is, a different set of filters</span>
<span class="sd">    is applied at each different patch of the input.</span>

<span class="sd">    :param n_input_plane The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param input_width The expected width of input</span>
<span class="sd">    :param input_height The expected height of input</span>
<span class="sd">    :param n_output_plane The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kernel_w The kernel width of the convolution</span>
<span class="sd">    :param kernel_h The kernel height of the convolution</span>
<span class="sd">    :param stride_w The step of the convolution in the width dimension.</span>
<span class="sd">    :param stride_h The step of the convolution in the height dimension</span>
<span class="sd">    :param pad_w The additional zeros added per width to the input planes.</span>
<span class="sd">    :param pad_h The additional zeros added per height to the input planes.</span>
<span class="sd">    :param propagate_back Propagate gradient back</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param init_weight: the optional initial value for the weight</span>
<span class="sd">    :param init_bias: the optional initial value for the bias</span>
<span class="sd">    :param init_grad_weight: the optional initial value for the grad_weight</span>
<span class="sd">    :param init_grad_bias: the optional initial value for the grad_bias</span>
<span class="sd">    :param with_bias: the optional initial value for if need bias</span>
<span class="sd">    :param data_format: a string value of &quot;NHWC&quot; or &quot;NCHW&quot; to specify the input data format of this layer. In &quot;NHWC&quot; format</span>
<span class="sd">                       data is stored in the order of [batch_size, height, width, channels], in &quot;NCHW&quot; format data is stored</span>
<span class="sd">                       in the order of [batch_size, channels, height, width].</span>

<span class="sd">    &gt;&gt;&gt; locallyConnected2D = LocallyConnected2D(6, 2, 4, 12, 5, 5)</span>
<span class="sd">    creating: createLocallyConnected2D</span>
<span class="sd">    &gt;&gt;&gt; locallyConnected2D.setWRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; locallyConnected2D.setBRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">input_width</span><span class="p">,</span>
                 <span class="n">input_height</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">kernel_h</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LocallyConnected2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_input_plane</span><span class="p">,</span>
                                                 <span class="n">input_width</span><span class="p">,</span>
                                                 <span class="n">input_height</span><span class="p">,</span>
                                                 <span class="n">n_output_plane</span><span class="p">,</span>
                                                 <span class="n">kernel_w</span><span class="p">,</span>
                                                 <span class="n">kernel_h</span><span class="p">,</span>
                                                 <span class="n">stride_w</span><span class="p">,</span>
                                                 <span class="n">stride_h</span><span class="p">,</span>
                                                 <span class="n">pad_w</span><span class="p">,</span>
                                                 <span class="n">pad_h</span><span class="p">,</span>
                                                 <span class="n">propagate_back</span><span class="p">,</span>
                                                 <span class="n">wRegularizer</span><span class="p">,</span>
                                                 <span class="n">bRegularizer</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">),</span>
                                                 <span class="n">with_bias</span><span class="p">,</span>
                                                 <span class="n">data_format</span><span class="p">)</span>
<div class="viewcode-block" id="LocallyConnected2D.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LocallyConnected2D.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="SpatialConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a 2D convolution over an input image composed of several input planes.</span>
<span class="sd">    The input tensor in forward(input) is expected to be</span>
<span class="sd">    a 3D tensor (nInputPlane x height x width).</span>

<span class="sd">    :param n_input_plane The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param n_output_plane The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kernel_w The kernel width of the convolution</span>
<span class="sd">    :param kernel_h The kernel height of the convolution</span>
<span class="sd">    :param stride_w The step of the convolution in the width dimension.</span>
<span class="sd">    :param stride_h The step of the convolution in the height dimension</span>
<span class="sd">    :param pad_w The additional zeros added per width to the input planes.</span>
<span class="sd">    :param pad_h The additional zeros added per height to the input planes.</span>
<span class="sd">    :param n_group Kernel group number</span>
<span class="sd">    :param propagate_back Propagate gradient back</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param init_weight: the optional initial value for the weight</span>
<span class="sd">    :param init_bias: the optional initial value for the bias</span>
<span class="sd">    :param init_grad_weight: the optional initial value for the grad_weight</span>
<span class="sd">    :param init_grad_bias: the optional initial value for the grad_bias</span>
<span class="sd">    :param with_bias: the optional initial value for if need bias</span>
<span class="sd">    :param data_format: a string value of &quot;NHWC&quot; or &quot;NCHW&quot; to specify the input data format of this layer. In &quot;NHWC&quot; format</span>
<span class="sd">                       data is stored in the order of [batch_size, height, width, channels], in &quot;NCHW&quot; format data is stored</span>
<span class="sd">                       in the order of [batch_size, channels, height, width].</span>

<span class="sd">    &gt;&gt;&gt; spatialConvolution = SpatialConvolution(6, 12, 5, 5)</span>
<span class="sd">    creating: createSpatialConvolution</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolution.setWRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolution.setBRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(1, 12, 6, 5, 5)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(12)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([1, 12, 6, 5, 5])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([12])</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolution = SpatialConvolution(6, 12, 5, 5, 1, 1, 0, 0, 1, True, L1Regularizer(0.5), L1Regularizer(0.5), init_weight, init_bias, init_grad_weight, init_grad_bias, True, &quot;NCHW&quot;)</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createSpatialConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">kernel_h</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_input_plane</span><span class="p">,</span>
                                                 <span class="n">n_output_plane</span><span class="p">,</span>
                                                 <span class="n">kernel_w</span><span class="p">,</span>
                                                 <span class="n">kernel_h</span><span class="p">,</span>
                                                 <span class="n">stride_w</span><span class="p">,</span>
                                                 <span class="n">stride_h</span><span class="p">,</span>
                                                 <span class="n">pad_w</span><span class="p">,</span>
                                                 <span class="n">pad_h</span><span class="p">,</span>
                                                 <span class="n">n_group</span><span class="p">,</span>
                                                 <span class="n">propagate_back</span><span class="p">,</span>
                                                 <span class="n">wRegularizer</span><span class="p">,</span>
                                                 <span class="n">bRegularizer</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">),</span>
                                                 <span class="n">with_bias</span><span class="p">,</span>
                                                 <span class="n">data_format</span><span class="p">)</span>
<div class="viewcode-block" id="SpatialConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                  <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="TemporalMaxPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TemporalMaxPooling">[docs]</a><span class="k">class</span> <span class="nc">TemporalMaxPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 1D max-pooling operation in kW regions by step size dW steps.</span>
<span class="sd">    Input sequence composed of nInputFrame frames.</span>
<span class="sd">    The input tensor in forward(input) is expected to be a 2D tensor (nInputFrame x inputFrameSize)</span>
<span class="sd">     or a 3D tensor (nBatchFrame x nInputFrame x inputFrameSize).</span>

<span class="sd">    If the input sequence is a 2D tensor of dimension nInputFrame x inputFrameSize,</span>
<span class="sd">    the output sequence will be nOutputFrame x inputFrameSize where</span>

<span class="sd">    nOutputFrame = (nInputFrame - k_w) / d_w + 1</span>

<span class="sd">    :param k_w:              kernel width</span>
<span class="sd">    :param d_w:              step size in width, default is -1, means the `d_w` equals `k_w`</span>

<span class="sd">    &gt;&gt;&gt; temporalMaxPooling = TemporalMaxPooling(2, 2)</span>
<span class="sd">    creating: createTemporalMaxPooling</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">k_w</span><span class="p">,</span>
                 <span class="n">d_w</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TemporalMaxPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">k_w</span><span class="p">,</span>
                                                <span class="n">d_w</span><span class="p">)</span></div>
<div class="viewcode-block" id="SpatialMaxPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialMaxPooling">[docs]</a><span class="k">class</span> <span class="nc">SpatialMaxPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>
<span class="sd">    If the input image is a 3D tensor nInputPlane x height x width,</span>
<span class="sd">    the output image size will be nOutputPlane x oheight x owidth where</span>
<span class="sd">    owidth  = op((width  + 2*padW - kW) / dW + 1)</span>
<span class="sd">    oheight = op((height + 2*padH - kH) / dH + 1)</span>
<span class="sd">    op is a rounding operator. By default, it is floor.</span>
<span class="sd">    It can be changed by calling :ceil() or :floor() methods.</span>
<span class="sd">    </span>
<span class="sd">    When padW and padH are both -1, we use a padding algorithm similar to the &quot;SAME&quot;</span>
<span class="sd">    padding of tensorflow. That is</span>
<span class="sd"> </span>
<span class="sd">     outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)</span>
<span class="sd">     outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)</span>
<span class="sd"> </span>
<span class="sd">     padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)</span>
<span class="sd">     padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)</span>
<span class="sd"> </span>
<span class="sd">     padTop = padAlongHeight / 2</span>
<span class="sd">     padLeft = padAlongWidth / 2</span>

<span class="sd">    :param kW:              kernel width</span>
<span class="sd">    :param kH:              kernel height</span>
<span class="sd">    :param dW:              step size in width</span>
<span class="sd">    :param dH:              step size in height</span>
<span class="sd">    :param padW:            padding in width</span>
<span class="sd">    :param padH:            padding in height</span>
<span class="sd">    :param format:          &quot;NCHW&quot; or &quot;NHWC&quot;, indicating the input data format</span>

<span class="sd">    &gt;&gt;&gt; spatialMaxPooling = SpatialMaxPooling(2, 2, 2, 2)</span>
<span class="sd">    creating: createSpatialMaxPooling</span>
<span class="sd">    &gt;&gt;&gt; spatialMaxPooling = SpatialMaxPooling(2, 2, 2, 2, -1, -1, True, &quot;NHWC&quot;)</span>
<span class="sd">    creating: createSpatialMaxPooling</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># to_ceil: call floor() when False; call ceil() when True</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="p">,</span>
                 <span class="n">dh</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">to_ceil</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialMaxPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">kw</span><span class="p">,</span>
                                                <span class="n">kh</span><span class="p">,</span>
                                                <span class="n">dw</span><span class="p">,</span>
                                                <span class="n">dh</span><span class="p">,</span>
                                                <span class="n">pad_w</span><span class="p">,</span>
                                                <span class="n">pad_h</span><span class="p">,</span>
                                                <span class="n">to_ceil</span><span class="p">,</span>
                                                <span class="nb">format</span><span class="p">)</span></div>


<div class="viewcode-block" id="Select"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Select">[docs]</a><span class="k">class</span> <span class="nc">Select</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A Simple layer selecting an index of the input tensor in the given dimension</span>


<span class="sd">    :param dimension: the dimension to select</span>
<span class="sd">    :param index: the index of the dimension to be selected</span>


<span class="sd">    &gt;&gt;&gt; select = Select(1, 1)</span>
<span class="sd">    creating: createSelect</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Select</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>

<div class="viewcode-block" id="Recurrent"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Recurrent">[docs]</a><span class="k">class</span> <span class="nc">Recurrent</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Recurrent module is a container of rnn cells</span>
<span class="sd">    Different types of rnn cells can be added using add() function</span>


<span class="sd">    &gt;&gt;&gt; recurrent = Recurrent()</span>
<span class="sd">    creating: createRecurrent</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Recurrent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>

<div class="viewcode-block" id="Recurrent.get_hidden_state"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Recurrent.get_hidden_state">[docs]</a>    <span class="k">def</span> <span class="nf">get_hidden_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        get hidden state and cell at last time step.</span>
<span class="sd">        </span>
<span class="sd">        :return: list of hidden state and cell</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">states</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;getHiddenState&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">states</span></div></div>

<div class="viewcode-block" id="RecurrentDecoder"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RecurrentDecoder">[docs]</a><span class="k">class</span> <span class="nc">RecurrentDecoder</span><span class="p">(</span><span class="n">Recurrent</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    RecurrentDecoder module is a container of rnn cells which used to make</span>
<span class="sd">    a prediction of the next timestep based on the prediction we made from</span>
<span class="sd">    the previous timestep. Input for RecurrentDecoder is dynamically composed</span>
<span class="sd">    during training. input at t(i) is output at t(i-1), input at t(0) is</span>
<span class="sd">    user input, and user input has to be batch x stepShape(shape of the input</span>
<span class="sd">    at a single time step).</span>

<span class="sd">    Different types of rnn cells can be added using add() function.</span>

<span class="sd">    &gt;&gt;&gt; recurrent_decoder = RecurrentDecoder(output_length = 5)</span>
<span class="sd">    creating: createRecurrentDecoder</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_length</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Recurrent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">output_length</span><span class="p">)</span></div>

<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LSTM">[docs]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">|   Long Short Term Memory architecture.</span>
<span class="sd">|   Ref.</span>
<span class="sd">|   A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)</span>
<span class="sd">|   B. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</span>
<span class="sd">|   C. http://arxiv.org/pdf/1503.04069v1.pdf</span>
<span class="sd">|   D. https://github.com/wojzaremba/lstm</span>
<span class="sd">|   E. https://github.com/Element-Research/rnn/blob/master/FastLSTM.lua</span>


<span class="sd">    :param inputSize: the size of each input vector</span>
<span class="sd">    :param hiddenSize: Hidden unit size in the LSTM</span>
<span class="sd">    :param p: is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287.pdf)</span>
<span class="sd">    :param activation: activation function, by default to be Tanh if not specified.</span>
<span class="sd">                        It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param inner_activation: activation function for the inner cells, by default to be Sigmoid if not specified.</span>
<span class="sd">                            It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; lstm = LSTM(4, 3, 0.5, &#39;tanh&#39;, Sigmoid(), L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createSigmoid</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    creating: createLSTM</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inner_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inner_activation</span><span class="p">:</span>
            <span class="n">inner_activation</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">inner_activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">inner_activation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span>
                                   <span class="n">activation</span><span class="p">,</span> <span class="n">inner_activation</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="LSTMPeephole"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LSTMPeephole">[docs]</a><span class="k">class</span> <span class="nc">LSTMPeephole</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">|   Long Short Term Memory architecture with peephole.</span>
<span class="sd">|   Ref. A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)</span>
<span class="sd">|   B. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</span>
<span class="sd">|   C. http://arxiv.org/pdf/1503.04069v1.pdf</span>
<span class="sd">|   D. https://github.com/wojzaremba/lstm</span>
<span class="sd">|   E. https://github.com/Element-Research/rnn/blob/master/LSTM.lua</span>


<span class="sd">    :param input_size: the size of each input vector</span>
<span class="sd">    :param hidden_size: Hidden unit size in the LSTM</span>
<span class="sd">    :param  p: is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287.pdf)</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; lstm = LSTMPeephole(4, 3, 0.5, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLSTMPeephole</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTMPeephole</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="GRU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GRU">[docs]</a><span class="k">class</span> <span class="nc">GRU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Gated Recurrent Units architecture.</span>
<span class="sd">    The first input in sequence uses zero value for cell and hidden state</span>


<span class="sd">|   Ref.</span>
<span class="sd">|   http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</span>
<span class="sd">|   https://github.com/Element-Research/rnn/blob/master/GRU.lua</span>


<span class="sd">    :param input_size: the size of each input vector</span>
<span class="sd">    :param hidden_size: Hidden unit size in GRU</span>
<span class="sd">    :param p: is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287.pdf)</span>
<span class="sd">    :param activation: activation function, by default to be Tanh if not specified.</span>
<span class="sd">                        It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param inner_activation: activation function for the inner cells, by default to be Sigmoid if not specified.</span>
<span class="sd">                            It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>



<span class="sd">    &gt;&gt;&gt; gru = GRU(4, 3, 0.5, Tanh(), Sigmoid(), L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    creating: createSigmoid</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createGRU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>  <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inner_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inner_activation</span><span class="p">:</span>
            <span class="n">inner_activation</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">inner_activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">inner_activation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GRU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">inner_activation</span><span class="p">,</span>
                                  <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="RnnCell"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RnnCell">[docs]</a><span class="k">class</span> <span class="nc">RnnCell</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple RNN. User can pass an activation function to the RNN.</span>


<span class="sd">    :param input_size: the size of each input vector</span>
<span class="sd">    :param hidden_size: Hidden unit size in simple RNN</span>
<span class="sd">    :param activation: activation function. It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param isInputWithBias: boolean</span>
<span class="sd">    :param isHiddenWithBias: boolean</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]](../regularizers.md),applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; rnn = RnnCell(4, 3, Tanh(), True, True, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createRnnCell</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">activation</span><span class="p">,</span>
                 <span class="n">isInputWithBias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">isHiddenWithBias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RnnCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">isInputWithBias</span><span class="p">,</span> <span class="n">isHiddenWithBias</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="TimeDistributed"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TimeDistributed">[docs]</a><span class="k">class</span> <span class="nc">TimeDistributed</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This layer is intended to apply contained layer to each temporal time slice</span>
<span class="sd">    of input tensor.</span>


<span class="sd">    For instance, The TimeDistributed Layer can feed each time slice of input tensor</span>
<span class="sd">    to the Linear layer.</span>
<span class="sd">    </span>
<span class="sd">    The input data format is [Batch, Time, Other dims]. For the contained layer, it must not change</span>
<span class="sd">    the Other dims length.</span>


<span class="sd">    &gt;&gt;&gt; td = TimeDistributed(Linear(2, 3))</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    creating: createTimeDistributed</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span></div>


<div class="viewcode-block" id="Concat"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Concat">[docs]</a><span class="k">class</span> <span class="nc">Concat</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Concat concatenates the output of one layer of &quot;parallel&quot;</span>
<span class="sd">    modules along the provided {@code dimension}: they take the</span>
<span class="sd">    same inputs, and their output is concatenated.</span>
<span class="sd">```</span>
<span class="sd">                    +-----------+</span>
<span class="sd">               +----&gt;  module1  -----+</span>
<span class="sd">               |    |           |    |</span>
<span class="sd">    input -----+----&gt;  module2  -----+----&gt; output</span>
<span class="sd">               |    |           |    |</span>
<span class="sd">               +----&gt;  module3  -----+</span>
<span class="sd">                    +-----------+</span>
<span class="sd">```</span>

<span class="sd">    :param dimension: dimension</span>


<span class="sd">    &gt;&gt;&gt; concat = Concat(2)</span>
<span class="sd">    creating: createConcat</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Concat</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">dimension</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialAveragePooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialAveragePooling">[docs]</a><span class="k">class</span> <span class="nc">SpatialAveragePooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 2D average-pooling operation in kWxkH regions by step size dWxdH steps.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>
<span class="sd">    </span>
<span class="sd">    When padW and padH are both -1, we use a padding algorithm similar to the &quot;SAME&quot;</span>
<span class="sd">    padding of tensorflow. That is</span>
<span class="sd"> </span>
<span class="sd">     outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)</span>
<span class="sd">     outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)</span>
<span class="sd"> </span>
<span class="sd">     padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)</span>
<span class="sd">     padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)</span>
<span class="sd"> </span>
<span class="sd">     padTop = padAlongHeight / 2</span>
<span class="sd">     padLeft = padAlongWidth / 2</span>

<span class="sd">    :param kW: kernel width</span>
<span class="sd">    :param kH: kernel height</span>
<span class="sd">    :param dW: step width</span>
<span class="sd">    :param dH: step height</span>
<span class="sd">    :param padW: padding width</span>
<span class="sd">    :param padH: padding height</span>
<span class="sd">    :param global_pooling: If globalPooling then it will pool over the size of the input by doing</span>
<span class="sd">                         kH = input-&gt;height and kW = input-&gt;width</span>
<span class="sd">    :param ceilMode: whether the output size is to be ceiled or floored</span>
<span class="sd">    :param countIncludePad: whether to include padding when dividing thenumber of elements in pooling region</span>
<span class="sd">    :param divide: whether to do the averaging</span>
<span class="sd">    :param format:          &quot;NCHW&quot; or &quot;NHWC&quot;, indicating the input data format</span>


<span class="sd">    &gt;&gt;&gt; spatialAveragePooling = SpatialAveragePooling(7,7)</span>
<span class="sd">    creating: createSpatialAveragePooling</span>
<span class="sd">    &gt;&gt;&gt; spatialAveragePooling = SpatialAveragePooling(2, 2, 2, 2, -1, -1, True, format=&quot;NHWC&quot;)</span>
<span class="sd">    creating: createSpatialAveragePooling</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">global_pooling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">divide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialAveragePooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">kw</span><span class="p">,</span>
                                                    <span class="n">kh</span><span class="p">,</span>
                                                    <span class="n">dw</span><span class="p">,</span>
                                                    <span class="n">dh</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">,</span>
                                                    <span class="n">global_pooling</span><span class="p">,</span>
                                                    <span class="n">ceil_mode</span><span class="p">,</span>
                                                    <span class="n">count_include_pad</span><span class="p">,</span>
                                                    <span class="n">divide</span><span class="p">,</span>
                                                    <span class="nb">format</span><span class="p">)</span>

<div class="viewcode-block" id="SpatialAveragePooling.set_weights"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialAveragePooling.set_weights">[docs]</a>    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialAveragePooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SpatialBatchNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialBatchNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialBatchNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This file implements Batch Normalization as described in the paper:</span>
<span class="sd">    &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&quot;</span>
<span class="sd">    by Sergey Ioffe, Christian Szegedy</span>
<span class="sd">    This implementation is useful for inputs coming from convolution layers.</span>
<span class="sd">    For non-convolutional layers, see [[BatchNormalization]]</span>
<span class="sd">    The operation implemented is:</span>

<span class="sd">```</span>
<span class="sd">          ( x - mean(x) )</span>
<span class="sd">    y = -------------------- * gamma + beta</span>
<span class="sd">       standard-deviation(x)</span>
<span class="sd">```</span>

<span class="sd">    where gamma and beta are learnable parameters.</span>
<span class="sd">    The learning of gamma and beta is optional.</span>
<span class="sd">    </span>
<span class="sd">    :param n_output: output feature map number</span>
<span class="sd">    :param eps: avoid divide zero</span>
<span class="sd">    :param momentum: momentum for weight update</span>
<span class="sd">    :param affine: affine operation on output or not</span>
<span class="sd">    :param data_format a string value (or DataFormat Object in Scala) of &quot;NHWC&quot; or &quot;NCHW&quot; to specify the input data format of this layer. In &quot;NHWC&quot; format</span>
<span class="sd">                        data is stored in the order of [batch_size, height, width, channels], in &quot;NCHW&quot; format data is stored</span>
<span class="sd">                        in the order of [batch_size, channels, height, width].</span>


<span class="sd">    &gt;&gt;&gt; spatialBatchNormalization = SpatialBatchNormalization(1)</span>
<span class="sd">    creating: createSpatialBatchNormalization</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.array([1.0])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.array([0.0])</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.array([0.0])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.array([0.0])</span>
<span class="sd">    &gt;&gt;&gt; spatialBatchNormalization = SpatialBatchNormalization(1, 1e-5, 0.1, True, init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createSpatialBatchNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialBatchNormalization = SpatialBatchNormalization(1, 1e-5, 0.1, True, init_weight, init_bias, init_grad_weight, init_grad_bias, &quot;NHWC&quot;)</span>
<span class="sd">    creating: createSpatialBatchNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialBatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                        <span class="n">n_output</span><span class="p">,</span>
                                                        <span class="n">eps</span><span class="p">,</span>
                                                        <span class="n">momentum</span><span class="p">,</span>
                                                        <span class="n">affine</span><span class="p">,</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">),</span>
                                                        <span class="n">data_format</span><span class="p">)</span>

<div class="viewcode-block" id="SpatialBatchNormalization.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialBatchNormalization.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpatialCrossMapLRN"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialCrossMapLRN">[docs]</a><span class="k">class</span> <span class="nc">SpatialCrossMapLRN</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies Spatial Local Response Normalization between different feature maps.</span>
<span class="sd">    The operation implemented is:</span>
<span class="sd">```</span>
<span class="sd">                                 x_f</span>
<span class="sd">    y_f =  -------------------------------------------------</span>
<span class="sd">            (k+(alpha/size)* sum_{l=l1 to l2} (x_l^2^))^beta^</span>
<span class="sd">```</span>

<span class="sd">    where x_f is the input at spatial locations h,w (not shown for simplicity) and feature map f,</span>
<span class="sd">    l1 corresponds to max(0,f-ceil(size/2)) and l2 to min(F, f-ceil(size/2) + size).</span>
<span class="sd">    Here, F is the number of feature maps.</span>

<span class="sd">    :param size:  the number of channels to sum over</span>
<span class="sd">    :param alpha:  the scaling parameter</span>
<span class="sd">    :param beta:   the exponent</span>
<span class="sd">    :param k: a constant</span>
<span class="sd">    :param data_format a string value (or DataFormat Object in Scala) of &quot;NHWC&quot; or &quot;NCHW&quot; to specify the input data format of this layer. In &quot;NHWC&quot; format</span>
<span class="sd">                        data is stored in the order of [batch_size, height, width, channels], in &quot;NCHW&quot; format data is stored</span>
<span class="sd">                        in the order of [batch_size, channels, height, width]</span>


<span class="sd">    &gt;&gt;&gt; spatialCrossMapLRN = SpatialCrossMapLRN()</span>
<span class="sd">    creating: createSpatialCrossMapLRN</span>
<span class="sd">    &gt;&gt;&gt; spatialCrossMapLRN = SpatialCrossMapLRN(5, 1.0, 0.75, 1.0, &quot;NHWC&quot;)</span>
<span class="sd">    creating: createSpatialCrossMapLRN</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
                 <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialCrossMapLRN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">size</span><span class="p">,</span>
                                                 <span class="n">alpha</span><span class="p">,</span>
                                                 <span class="n">beta</span><span class="p">,</span>
                                                 <span class="n">k</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>
<div class="viewcode-block" id="SpatialDropout3D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDropout3D">[docs]</a><span class="k">class</span> <span class="nc">SpatialDropout3D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This version performs the same function as Dropout, however it drops</span>
<span class="sd">    entire 3D feature maps instead of individual elements. If adjacent voxels</span>
<span class="sd">    within feature maps are strongly correlated (as is normally the case in</span>
<span class="sd">    early convolution layers) then regular dropout will not regularize the</span>
<span class="sd">    activations and will otherwise just result in an effective learning rate</span>
<span class="sd">    decrease. In this case, SpatialDropout3D will help promote independence</span>
<span class="sd">    between feature maps and should be used instead.</span>

<span class="sd">    :param initP the probability p</span>
<span class="sd">    :param format  &#39;NCHW&#39; or &#39;NHWC&#39;.</span>
<span class="sd">        In &#39;NCHW&#39; mode, the channels dimension (the depth)</span>
<span class="sd">        is at index 1, in &#39;NHWC&#39; mode is it at index 4.</span>

<span class="sd">    &gt;&gt;&gt; dropout = SpatialDropout3D(0.5, &quot;NHWC&quot;)</span>
<span class="sd">    creating: createSpatialDropout3D</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDropout3D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">init_p</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>

<div class="viewcode-block" id="SpatialDropout2D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDropout2D">[docs]</a><span class="k">class</span> <span class="nc">SpatialDropout2D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This version performs the same function as Dropout, however it drops</span>
<span class="sd">    entire 2D feature maps instead of individual elements. If adjacent pixels</span>
<span class="sd">    within feature maps are strongly correlated (as is normally the case in</span>
<span class="sd">    early convolution layers) then regular dropout will not regularize the</span>
<span class="sd">    activations and will otherwise just result in an effective learning rate</span>
<span class="sd">    decrease. In this case, SpatialDropout2D will help promote independence</span>
<span class="sd">    between feature maps and should be used instead.</span>

<span class="sd">    :param initP the probability p</span>
<span class="sd">    :param format  &#39;NCHW&#39; or &#39;NHWC&#39;.</span>
<span class="sd">        In &#39;NCHW&#39; mode, the channels dimension (the depth)</span>
<span class="sd">        is at index 1, in &#39;NHWC&#39; mode is it at index 4.</span>

<span class="sd">    &gt;&gt;&gt; dropout = SpatialDropout2D(0.4, &quot;NHWC&quot;)</span>
<span class="sd">    creating: createSpatialDropout2D</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDropout2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">init_p</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>

<div class="viewcode-block" id="SpatialDropout1D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDropout1D">[docs]</a><span class="k">class</span> <span class="nc">SpatialDropout1D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This version performs the same function as Dropout, however it drops</span>
<span class="sd">    entire 1D feature maps instead of individual elements. If adjacent frames</span>
<span class="sd">    within feature maps are strongly correlated (as is normally the case in</span>
<span class="sd">    early convolution layers) then regular dropout will not regularize the</span>
<span class="sd">    activations and will otherwise just result in an effective learning rate</span>
<span class="sd">    decrease. In this case, SpatialDropout1D will help promote independence</span>
<span class="sd">    between feature maps and should be used instead.</span>

<span class="sd">    :param initP the probability p</span>

<span class="sd">    &gt;&gt;&gt; dropout = SpatialDropout1D(0.4)</span>
<span class="sd">    creating: createSpatialDropout1D</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDropout1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">init_p</span><span class="p">)</span></div>

<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Dropout">[docs]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Dropout masks(set to zero) parts of input using a bernoulli distribution.</span>
<span class="sd">    Each input element has a probability initP of being dropped. If scale is</span>
<span class="sd">    set, the outputs are scaled by a factor of 1/(1-initP) during training.</span>
<span class="sd">    During evaluating, output is the same as input.</span>


<span class="sd">    :param initP: probability to be dropped</span>
<span class="sd">    :param inplace: inplace model</span>
<span class="sd">    :param scale: if scale by a factor of 1/(1-initP)</span>


<span class="sd">    &gt;&gt;&gt; dropout = Dropout(0.4)</span>
<span class="sd">    creating: createDropout</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">init_p</span><span class="p">,</span>
                                      <span class="n">inplace</span><span class="p">,</span>
                                      <span class="n">scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="GaussianDropout"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GaussianDropout">[docs]</a><span class="k">class</span> <span class="nc">GaussianDropout</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply multiplicative 1-centered Gaussian noise.</span>
<span class="sd">    The multiplicative noise will have standard deviation `sqrt(rate / (1 - rate)).</span>

<span class="sd">    As it is a regularization layer, it is only active at training time.</span>

<span class="sd">    :param rate: drop probability (as with `Dropout`).</span>


<span class="sd">    &gt;&gt;&gt; GaussianDropout = GaussianDropout(0.5)</span>
<span class="sd">    creating: createGaussianDropout</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">rate</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                              <span class="n">rate</span><span class="p">)</span></div>


<div class="viewcode-block" id="GaussianNoise"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GaussianNoise">[docs]</a><span class="k">class</span> <span class="nc">GaussianNoise</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply additive zero-centered Gaussian noise.</span>
<span class="sd">    This is useful to mitigate overfitting</span>
<span class="sd">    (you could see it as a form of random data augmentation).</span>
<span class="sd">    Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.</span>

<span class="sd">    As it is a regularization layer, it is only active at training time.</span>

<span class="sd">    :param stdev: standard deviation of the noise distribution</span>

<span class="sd">    &gt;&gt;&gt; GaussianNoise = GaussianNoise(0.5)</span>
<span class="sd">    creating: createGaussianNoise</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stddev</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianNoise</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                            <span class="n">stddev</span><span class="p">)</span></div>

<div class="viewcode-block" id="View"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.View">[docs]</a><span class="k">class</span> <span class="nc">View</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This module creates a new view of the input tensor using the sizes passed to the constructor.</span>
<span class="sd">    The method setNumInputDims() allows to specify the expected number of dimensions of the</span>
<span class="sd">    inputs of the modules. This makes it possible to use minibatch inputs when using a size -1</span>
<span class="sd">    for one of the dimensions.</span>


<span class="sd">    :param size: sizes use for creates a new view</span>


<span class="sd">    &gt;&gt;&gt; view = View([1024,2])</span>
<span class="sd">    creating: createView</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">sizes</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">View</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">sizes</span><span class="p">,</span>
                                   <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Abs"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Abs">[docs]</a><span class="k">class</span> <span class="nc">Abs</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    an element-wise abs operation</span>


<span class="sd">    &gt;&gt;&gt; abs = Abs()</span>
<span class="sd">    creating: createAbs</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Abs</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Add"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Add">[docs]</a><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    adds a bias term to input data ;</span>

<span class="sd">    :param input_size: size of input data</span>

<span class="sd">    &gt;&gt;&gt; add = Add(1)</span>
<span class="sd">    creating: createAdd</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">input_size</span><span class="p">)</span>
<div class="viewcode-block" id="Add.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Add.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="AddConstant"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.AddConstant">[docs]</a><span class="k">class</span> <span class="nc">AddConstant</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    adding a constant</span>


<span class="sd">    :param constant_scalar: constant value</span>
<span class="sd">    :param inplace: Can optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; addConstant = AddConstant(1e-5, True)</span>
<span class="sd">    creating: createAddConstant</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">constant_scalar</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AddConstant</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">constant_scalar</span><span class="p">,</span>
                                          <span class="n">inplace</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BatchNormalization">[docs]</a><span class="k">class</span> <span class="nc">BatchNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This layer implements Batch Normalization as described in the paper:</span>
<span class="sd">             &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">             Covariate Shift&quot;</span>
<span class="sd">    by Sergey Ioffe, Christian Szegedy https://arxiv.org/abs/1502.03167</span>


<span class="sd">    This implementation is useful for inputs NOT coming from convolution layers. For convolution</span>
<span class="sd">    layers, use nn.SpatialBatchNormalization.</span>


<span class="sd">    The operation implemented is:</span>
<span class="sd">```</span>
<span class="sd">                ( x - mean(x) )</span>
<span class="sd">         y = -------------------- * gamma + beta</span>
<span class="sd">             standard-deviation(x)</span>
<span class="sd">```</span>
<span class="sd">    where gamma and beta are learnable parameters.The learning of gamma and beta is optional.</span>


<span class="sd">    :param n_output: output feature map number</span>
<span class="sd">    :param eps: avoid divide zero</span>
<span class="sd">    :param momentum: momentum for weight update</span>
<span class="sd">    :param affine: affine operation on output or not</span>


<span class="sd">    &gt;&gt;&gt; batchNormalization = BatchNormalization(1, 1e-5, 1e-5, True)</span>
<span class="sd">    creating: createBatchNormalization</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(2)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([2])</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.zeros([2])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([2])</span>
<span class="sd">    &gt;&gt;&gt; batchNormalization = BatchNormalization(2, 1e-5, 1e-5, True, init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createBatchNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_output</span><span class="p">,</span>
                                                 <span class="n">eps</span><span class="p">,</span>
                                                 <span class="n">momentum</span><span class="p">,</span>
                                                 <span class="n">affine</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>

<div class="viewcode-block" id="BatchNormalization.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BatchNormalization.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="BifurcateSplitTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BifurcateSplitTable">[docs]</a><span class="k">class</span> <span class="nc">BifurcateSplitTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a Tensor as input and</span>
<span class="sd">    outputs two tables, splitting the Tensor along</span>
<span class="sd">    the specified dimension `dimension`.</span>

<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>

<span class="sd">    :param dimension to be split along this dimension</span>
<span class="sd">    :param T Numeric type. Only support float/double now</span>

<span class="sd">    &gt;&gt;&gt; bifurcateSplitTable = BifurcateSplitTable(1)</span>
<span class="sd">    creating: createBifurcateSplitTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BifurcateSplitTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">dimension</span><span class="p">)</span></div>


<div class="viewcode-block" id="Bilinear"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Bilinear">[docs]</a><span class="k">class</span> <span class="nc">Bilinear</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    a bilinear transformation with sparse inputs,</span>
<span class="sd">    The input tensor given in forward(input) is a table containing both inputs x_1 and x_2,</span>
<span class="sd">    which are tensors of size N x inputDimension1 and N x inputDimension2, respectively.</span>

<span class="sd">    :param input_size1 input dimension of x_1</span>
<span class="sd">    :param input_size2 input dimension of x_2</span>
<span class="sd">    :param output_size output dimension</span>
<span class="sd">    :param bias_res whether use bias</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; bilinear = Bilinear(1, 1, 1, True, L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createBilinear</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size1</span><span class="p">,</span>
                 <span class="n">input_size2</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">bias_res</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bilinear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">input_size1</span><span class="p">,</span>
                                       <span class="n">input_size2</span><span class="p">,</span>
                                       <span class="n">output_size</span><span class="p">,</span>
                                       <span class="n">bias_res</span><span class="p">,</span>
                                       <span class="n">wRegularizer</span><span class="p">,</span>
                                       <span class="n">bRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="Bilinear.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Bilinear.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Bottle"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Bottle">[docs]</a><span class="k">class</span> <span class="nc">Bottle</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Bottle allows varying dimensionality input to be forwarded through any module</span>
<span class="sd">    that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.</span>

<span class="sd">    :param module: transform module</span>
<span class="sd">    :param n_input_dim: nInputDim dimensions of module</span>
<span class="sd">    :param n_output_dim1: output of nOutputDim dimensions</span>


<span class="sd">    &gt;&gt;&gt; bottle = Bottle(Linear(100,10), 1, 1)</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    creating: createBottle</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">module</span><span class="p">,</span>
                 <span class="n">n_input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">n_output_dim1</span><span class="o">=</span><span class="n">INTMAX</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">module</span><span class="p">,</span>
                                     <span class="n">n_input_dim</span><span class="p">,</span>
                                     <span class="n">n_output_dim1</span><span class="p">)</span></div>


<div class="viewcode-block" id="CAdd"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAdd">[docs]</a><span class="k">class</span> <span class="nc">CAdd</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This layer has a bias tensor with given size. The bias will be added element wise to the input</span>
<span class="sd">    tensor. If the element number of the bias tensor match the input tensor, a simply element wise</span>
<span class="sd">    will be done. Or the bias will be expanded to the same size of the input. The expand means</span>
<span class="sd">    repeat on unmatched singleton dimension(if some unmatched dimension isn&#39;t singleton dimension,</span>
<span class="sd">    it will report an error). If the input is a batch, a singleton dimension will be add to the</span>
<span class="sd">    first dimension before the expand.</span>


<span class="sd">    :param size: the size of the bias</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; cAdd = CAdd([1,2])</span>
<span class="sd">    creating: createCAdd</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CAdd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">size</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span>

<div class="viewcode-block" id="CAdd.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAdd.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="CAddTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAddTable">[docs]</a><span class="k">class</span> <span class="nc">CAddTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Merge the input tensors in the input table by element wise adding them together. The input</span>
<span class="sd">    table is actually an array of tensor with same size.</span>


<span class="sd">    :param inplace: reuse the input memory</span>


<span class="sd">    &gt;&gt;&gt; cAddTable = CAddTable(True)</span>
<span class="sd">    creating: createCAddTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CAddTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">inplace</span><span class="p">)</span></div>

<div class="viewcode-block" id="CAveTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAveTable">[docs]</a><span class="k">class</span> <span class="nc">CAveTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Merge the input tensors in the input table by element wise taking the average. The input</span>
<span class="sd">    table is actually an array of tensor with same size.</span>


<span class="sd">    :param inplace: reuse the input memory</span>


<span class="sd">    &gt;&gt;&gt; cAveTable = CAveTable(True)</span>
<span class="sd">    creating: createCAveTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CAveTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="CDivTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CDivTable">[docs]</a><span class="k">class</span> <span class="nc">CDivTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table with two Tensor and returns the component-wise division between them.</span>


<span class="sd">    &gt;&gt;&gt; cDivTable = CDivTable()</span>
<span class="sd">    creating: createCDivTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CDivTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CMaxTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMaxTable">[docs]</a><span class="k">class</span> <span class="nc">CMaxTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table of Tensors and outputs the max of all of them.</span>


<span class="sd">    &gt;&gt;&gt; cMaxTable = CMaxTable()</span>
<span class="sd">    creating: createCMaxTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMaxTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CMinTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMinTable">[docs]</a><span class="k">class</span> <span class="nc">CMinTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table of Tensors and outputs the min of all of them.</span>

<span class="sd">    &gt;&gt;&gt; cMinTable = CMinTable()</span>
<span class="sd">    creating: createCMinTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMinTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CMul"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMul">[docs]</a><span class="k">class</span> <span class="nc">CMul</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a component-wise multiplication to the incoming data</span>


<span class="sd">    :param size: size of the data</span>


<span class="sd">    &gt;&gt;&gt; cMul = CMul([1,2])</span>
<span class="sd">    creating: createCMul</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMul</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">size</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">)</span>

<div class="viewcode-block" id="CMul.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMul.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="CMulTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMulTable">[docs]</a><span class="k">class</span> <span class="nc">CMulTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table of Tensors and outputs the multiplication of all of them.</span>


<span class="sd">    &gt;&gt;&gt; cMulTable = CMulTable()</span>
<span class="sd">    creating: createCMulTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMulTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CSubTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CSubTable">[docs]</a><span class="k">class</span> <span class="nc">CSubTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table with two Tensor and returns the component-wise subtraction between them.</span>


<span class="sd">    &gt;&gt;&gt; cSubTable = CSubTable()</span>
<span class="sd">    creating: createCSubTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CSubTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Clamp"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Clamp">[docs]</a><span class="k">class</span> <span class="nc">Clamp</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Clamps all elements into the range [min_value, max_value].</span>
<span class="sd">    Output is identical to input in the range,</span>
<span class="sd">    otherwise elements less than min_value (or greater than max_value)</span>
<span class="sd">    are saturated to min_value (or max_value).</span>


<span class="sd">    :param min:</span>
<span class="sd">    :param max:</span>


<span class="sd">    &gt;&gt;&gt; clamp = Clamp(1, 3)</span>
<span class="sd">    creating: createClamp</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="nb">min</span><span class="p">,</span>
                 <span class="nb">max</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Clamp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="nb">min</span><span class="p">,</span>
                                    <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="Contiguous"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Contiguous">[docs]</a><span class="k">class</span> <span class="nc">Contiguous</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    used to make input, grad_output both contiguous</span>


<span class="sd">    &gt;&gt;&gt; contiguous = Contiguous()</span>
<span class="sd">    creating: createContiguous</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Contiguous</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Cosine"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Cosine">[docs]</a><span class="k">class</span> <span class="nc">Cosine</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Cosine calculates the cosine similarity of the input to k mean centers. The input given in</span>
<span class="sd">    forward(input) must be either a vector (1D tensor) or matrix (2D tensor). If the input is a</span>
<span class="sd">    vector, it must have the size of inputSize. If it is a matrix, then each row is assumed to be</span>
<span class="sd">    an input sample of given batch (the number of rows means the batch size and the number of</span>
<span class="sd">    columns should be equal to the inputSize).</span>


<span class="sd">    :param input_size: the size of each input sample</span>
<span class="sd">    :param output_size: the size of the module output of each sample</span>


<span class="sd">    &gt;&gt;&gt; cosine = Cosine(2,3)</span>
<span class="sd">    creating: createCosine</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Cosine</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">input_size</span><span class="p">,</span>
                                     <span class="n">output_size</span><span class="p">)</span>
<div class="viewcode-block" id="Cosine.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Cosine.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="CosineDistance"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CosineDistance">[docs]</a><span class="k">class</span> <span class="nc">CosineDistance</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Outputs the cosine distance between inputs</span>


<span class="sd">    &gt;&gt;&gt; cosineDistance = CosineDistance()</span>
<span class="sd">    creating: createCosineDistance</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineDistance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="UpSampling2D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.UpSampling2D">[docs]</a><span class="k">class</span> <span class="nc">UpSampling2D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Upsampling layer for 2D inputs.</span>
<span class="sd">    Repeats the heights and widths of the data by size[0] and size[1] respectively.</span>

<span class="sd">    If input&#39;s dataformat is NCHW, then the size of output is (N, C, H * size[0], W * size[1])</span>

<span class="sd">    :param size tuple of 2 integers. The upsampling factors for heights and widths.</span>
<span class="sd">    :param format DataFormat, NCHW or NHWC</span>

<span class="sd">    &gt;&gt;&gt; upsampled2d = UpSampling2D([2, 3])</span>
<span class="sd">    creating: createUpSampling2D</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;nchw&quot;</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UpSampling2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<div class="viewcode-block" id="UpSampling1D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.UpSampling1D">[docs]</a><span class="k">class</span> <span class="nc">UpSampling1D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Upsampling layer for 1D inputs.</span>
<span class="sd">    Repeats each temporal step length times along the time axis.</span>

<span class="sd">    If input&#39;s size is (batch, steps, features),</span>
<span class="sd">    then the output&#39;s size is (batch, steps * length, features)</span>

<span class="sd">    :param length integer, upsampling factor.</span>
<span class="sd">    &gt;&gt;&gt; upsampled1d = UpSampling1D(2)</span>
<span class="sd">    creating: createUpSampling1D</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UpSampling1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span></div>

<div class="viewcode-block" id="Input"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Input">[docs]</a><span class="k">class</span> <span class="nc">Input</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Input layer do nothing to the input tensors, just passing them through. It is used as input to</span>
<span class="sd">    the Graph container (add a link) when the first layer of the graph container accepts multiple</span>
<span class="sd">    tensors as inputs.</span>

<span class="sd">    Each input node of the graph container should accept one tensor as input. If you want a module</span>
<span class="sd">    accepting multiple tensors as input, you should add some Input module before it and connect</span>
<span class="sd">    the outputs of the Input nodes to it.</span>

<span class="sd">    Please note that the return is not a layer but a Node containing input layer.</span>

<span class="sd">    &gt;&gt;&gt; input = Input()</span>
<span class="sd">    creating: createInput</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="DotProduct"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.DotProduct">[docs]</a><span class="k">class</span> <span class="nc">DotProduct</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This is a simple table layer which takes a table of two tensors as input</span>
<span class="sd">    and calculate the dot product between them as outputs</span>


<span class="sd">    &gt;&gt;&gt; dotProduct = DotProduct()</span>
<span class="sd">    creating: createDotProduct</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DotProduct</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="ELU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ELU">[docs]</a><span class="k">class</span> <span class="nc">ELU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    D-A Clevert, Thomas Unterthiner, Sepp Hochreiter</span>
<span class="sd">    Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</span>
<span class="sd">    [http://arxiv.org/pdf/1511.07289.pdf]</span>


<span class="sd">    &gt;&gt;&gt; eLU = ELU(1e-5, True)</span>
<span class="sd">    creating: createELU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">alpha</span><span class="p">,</span>
                                  <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Euclidean"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Euclidean">[docs]</a><span class="k">class</span> <span class="nc">Euclidean</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Outputs the Euclidean distance of the input to outputSize centers</span>

<span class="sd">    :param inputSize: inputSize</span>
<span class="sd">    :param outputSize: outputSize</span>
<span class="sd">    :param T: Numeric type. Only support float/double now</span>


<span class="sd">    &gt;&gt;&gt; euclidean = Euclidean(1, 1, True)</span>
<span class="sd">    creating: createEuclidean</span>
<span class="sd">    &#39;&#39;&#39;</span>



    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">fast_backward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Euclidean</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">input_size</span><span class="p">,</span>
                                        <span class="n">output_size</span><span class="p">,</span>
                                        <span class="n">fast_backward</span><span class="p">)</span>

<div class="viewcode-block" id="Euclidean.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Euclidean.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Exp"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Exp">[docs]</a><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies element-wise exp to input tensor.</span>

<span class="sd">    &gt;&gt;&gt; exp = Exp()</span>
<span class="sd">    creating: createExp</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Exp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="FlattenTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.FlattenTable">[docs]</a><span class="k">class</span> <span class="nc">FlattenTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This is a table layer which takes an arbitrarily deep table of Tensors</span>
<span class="sd">    (potentially nested) as input and a table of Tensors without any nested</span>
<span class="sd">    table will be produced</span>


<span class="sd">    &gt;&gt;&gt; flattenTable = FlattenTable()</span>
<span class="sd">    creating: createFlattenTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlattenTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="GradientReversal"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GradientReversal">[docs]</a><span class="k">class</span> <span class="nc">GradientReversal</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple module preserves the input, but takes the</span>
<span class="sd">    gradient from the subsequent layer, multiplies it by -lambda</span>
<span class="sd">    and passes it to the preceding layer. This can be used to maximise</span>
<span class="sd">    an objective function whilst using gradient descent, as described in</span>
<span class="sd">     [&quot;Domain-Adversarial Training of Neural Networks&quot;</span>
<span class="sd">     (http://arxiv.org/abs/1505.07818)]</span>


<span class="sd">    :param lambda: hyper-parameter lambda can be set dynamically during training</span>


<span class="sd">    &gt;&gt;&gt; gradientReversal = GradientReversal(1e-5)</span>
<span class="sd">    creating: createGradientReversal</span>
<span class="sd">    &gt;&gt;&gt; gradientReversal = GradientReversal()</span>
<span class="sd">    creating: createGradientReversal</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">the_lambda</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradientReversal</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">the_lambda</span><span class="p">)</span></div>


<div class="viewcode-block" id="HardShrink"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.HardShrink">[docs]</a><span class="k">class</span> <span class="nc">HardShrink</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This is a transfer layer which applies the hard shrinkage function</span>
<span class="sd">    element-wise to the input Tensor. The parameter lambda is set to 0.5</span>
<span class="sd">    by default</span>
<span class="sd">```</span>
<span class="sd">            x, if x &gt;  lambda</span>
<span class="sd">    f(x) =  x, if x &lt; -lambda</span>
<span class="sd">            0, otherwise</span>
<span class="sd">```</span>

<span class="sd">   :param the_lambda: a threshold value whose default value is 0.5</span>


<span class="sd">    &gt;&gt;&gt; hardShrink = HardShrink(1e-5)</span>
<span class="sd">    creating: createHardShrink</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">the_lambda</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HardShrink</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">the_lambda</span><span class="p">)</span></div>


<div class="viewcode-block" id="HardTanh"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.HardTanh">[docs]</a><span class="k">class</span> <span class="nc">HardTanh</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies HardTanh to each element of input, HardTanh is defined:</span>
<span class="sd">```</span>
<span class="sd">             |  maxValue, if x &gt; maxValue</span>
<span class="sd">      f(x) = |  minValue, if x &lt; minValue</span>
<span class="sd">             |  x, otherwise</span>
<span class="sd">```</span>
<span class="sd">    :param min_value: minValue in f(x), default is -1.</span>
<span class="sd">    :param max_value: maxValue in f(x), default is 1.</span>
<span class="sd">    :param inplace: whether enable inplace model.</span>


<span class="sd">    &gt;&gt;&gt; hardTanh = HardTanh(1e-5, 1e5, True)</span>
<span class="sd">    creating: createHardTanh</span>
<span class="sd">    &gt;&gt;&gt; hardTanh = HardTanh()</span>
<span class="sd">    creating: createHardTanh</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">min_value</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">max_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HardTanh</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">min_value</span><span class="p">,</span>
                                       <span class="n">max_value</span><span class="p">,</span>
                                       <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Index"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Index">[docs]</a><span class="k">class</span> <span class="nc">Index</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the Tensor index operation along the given dimension.</span>


<span class="sd">    :param dimension: the dimension to be indexed</span>


<span class="sd">    &gt;&gt;&gt; index = Index(1)</span>
<span class="sd">    creating: createIndex</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Index</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">dimension</span><span class="p">)</span></div>


<div class="viewcode-block" id="InferReshape"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.InferReshape">[docs]</a><span class="k">class</span> <span class="nc">InferReshape</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Reshape the input tensor with automatic size inference support.</span>
<span class="sd">    Positive numbers in the `size` argument are used to reshape the input to the</span>
<span class="sd">    corresponding dimension size.</span>
<span class="sd">    There are also two special values allowed in `size`:</span>
<span class="sd">       a. `0` means keep the corresponding dimension size of the input unchanged.</span>
<span class="sd">          i.e., if the 1st dimension size of the input is 2,</span>
<span class="sd">          the 1st dimension size of output will be set as 2 as well.</span>
<span class="sd">       b. `-1` means infer this dimension size from other dimensions.</span>
<span class="sd">          This dimension size is calculated by keeping the amount of output elements</span>
<span class="sd">          consistent with the input.</span>
<span class="sd">          Only one `-1` is allowable in `size`.</span>

<span class="sd">    For example,</span>
<span class="sd">       Input tensor with size: (4, 5, 6, 7)</span>
<span class="sd">       -&gt; InferReshape(Array(4, 0, 3, -1))</span>
<span class="sd">       Output tensor with size: (4, 5, 3, 14)</span>
<span class="sd">    The 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,</span>
<span class="sd">    and inferred the last dim as 14.</span>

<span class="sd">     :param size:      the target tensor size</span>
<span class="sd">     :param batch_mode: whether in batch mode</span>


<span class="sd">    &gt;&gt;&gt; inferReshape = InferReshape([4, 0, 3, -1], False)</span>
<span class="sd">    creating: createInferReshape</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span>
                 <span class="n">batch_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">InferReshape</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                           <span class="n">size</span><span class="p">,</span>
                                           <span class="n">batch_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="JoinTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.JoinTable">[docs]</a><span class="k">class</span> <span class="nc">JoinTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a table module which takes a table of Tensors as input and</span>
<span class="sd">    outputs a Tensor by joining them together along the dimension `dimension`.</span>


<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user need to specify the number of dimensions of each sample tensor in the</span>
<span class="sd">    batch using `nInputDims`.</span>


<span class="sd">    :param dimension: to be join in this dimension</span>
<span class="sd">    :param nInputDims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>


<span class="sd">    &gt;&gt;&gt; joinTable = JoinTable(1, 1)</span>
<span class="sd">    creating: createJoinTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">JoinTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">dimension</span><span class="p">,</span>
                                        <span class="n">n_input_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="SparseJoinTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SparseJoinTable">[docs]</a><span class="k">class</span> <span class="nc">SparseJoinTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    :: Experimental ::</span>

<span class="sd">    Sparse version of JoinTable. Backward just pass the origin gradOutput back to</span>
<span class="sd">    the next layers without split. So this layer may just works in Wide&amp;Deep like models.</span>


<span class="sd">    :param dimension: to be join in this dimension</span>


<span class="sd">    &gt;&gt;&gt; joinTable = SparseJoinTable(1)</span>
<span class="sd">    creating: createSparseJoinTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SparseJoinTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">dimension</span><span class="p">)</span></div>


<div class="viewcode-block" id="L1Penalty"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.L1Penalty">[docs]</a><span class="k">class</span> <span class="nc">L1Penalty</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    adds an L1 penalty to an input (for sparsity).</span>
<span class="sd">    L1Penalty is an inline module that in its forward propagation copies the input Tensor</span>
<span class="sd">    directly to the output, and computes an L1 loss of the latent state (input) and stores</span>
<span class="sd">    it in the module&#39;s loss field. During backward propagation: gradInput = gradOutput + gradLoss.</span>


<span class="sd">    :param l1weight:</span>
<span class="sd">    :param sizeAverage:</span>
<span class="sd">    :param provideOutput:</span>


<span class="sd">    &gt;&gt;&gt; l1Penalty = L1Penalty(1, True, True)</span>
<span class="sd">    creating: createL1Penalty</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">l1weight</span><span class="p">,</span>
                 <span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">provide_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Penalty</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">l1weight</span><span class="p">,</span>
                                        <span class="n">size_average</span><span class="p">,</span>
                                        <span class="n">provide_output</span><span class="p">)</span></div>

<div class="viewcode-block" id="NegativeEntropyPenalty"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.NegativeEntropyPenalty">[docs]</a><span class="k">class</span> <span class="nc">NegativeEntropyPenalty</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Penalize the input multinomial distribution if it has low entropy.</span>
<span class="sd">    The input to this layer should be a batch of vector each representing a</span>
<span class="sd">    multinomial distribution. The input is typically the output of a softmax layer.</span>
<span class="sd">    </span>
<span class="sd">    For forward, the output is the same as input and a NegativeEntropy loss of</span>
<span class="sd">    the latent state will be calculated each time. For backward,</span>
<span class="sd">    gradInput = gradOutput + gradLoss</span>

<span class="sd">    This can be used in reinforcement learning to discourage the policy from</span>
<span class="sd">    collapsing to a single action for a given state, which improves exploration.</span>
<span class="sd">    See the A3C paper for more detail (https://arxiv.org/pdf/1602.01783.pdf).</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; ne = NegativeEntropyPenalty(0.01)</span>
<span class="sd">    creating: createNegativeEntropyPenalty</span>
<span class="sd">    </span>
<span class="sd">    :param beta penalty coefficient</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NegativeEntropyPenalty</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                                                     <span class="n">bigdl_type</span><span class="p">,</span>
                                                     <span class="n">beta</span><span class="p">)</span></div>


<div class="viewcode-block" id="LeakyReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LeakyReLU">[docs]</a><span class="k">class</span> <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a transfer module that applies LeakyReLU, which parameter negval sets the slope of the</span>
<span class="sd">    negative part: LeakyReLU is defined as: f(x) = max(0, x) + negval * min(0, x)</span>


<span class="sd">    :param negval: sets the slope of the negative partl</span>
<span class="sd">    :param inplace: if it is true, doing the operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; leakyReLU = LeakyReLU(1e-5, True)</span>
<span class="sd">    creating: createLeakyReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">negval</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">negval</span><span class="p">,</span>
                                        <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Log">[docs]</a><span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the log function element-wise to the input Tensor,</span>
<span class="sd">     thus outputting a Tensor of the same dimension.</span>


<span class="sd">    &gt;&gt;&gt; log = Log()</span>
<span class="sd">    creating: createLog</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Log</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSigmoid"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LogSigmoid">[docs]</a><span class="k">class</span> <span class="nc">LogSigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is a transform layer corresponding to the sigmoid function:</span>
<span class="sd">    f(x) = Log(1 / (1 + e ^^ (-x)))</span>


<span class="sd">    &gt;&gt;&gt; logSigmoid = LogSigmoid()</span>
<span class="sd">    creating: createLogSigmoid</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogSigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LookupTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LookupTable">[docs]</a><span class="k">class</span> <span class="nc">LookupTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    a convolution of width 1, commonly used for word embeddings</span>

<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>

<span class="sd">    &gt;&gt;&gt; lookupTable = LookupTable(1, 1, 1e-5, 1e-5, 1e-5, True, L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLookupTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_index</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">max_norm</span><span class="o">=</span><span class="n">DOUBLEMAX</span><span class="p">,</span>
                 <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">should_scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LookupTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">n_index</span><span class="p">,</span>
                                          <span class="n">n_output</span><span class="p">,</span>
                                          <span class="n">padding_value</span><span class="p">,</span>
                                          <span class="n">max_norm</span><span class="p">,</span>
                                          <span class="n">norm_type</span><span class="p">,</span>
                                          <span class="n">should_scale_grad_by_freq</span><span class="p">,</span>
                                          <span class="n">wRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="LookupTable.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LookupTable.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="LookupTableSparse"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LookupTableSparse">[docs]</a><span class="k">class</span> <span class="nc">LookupTableSparse</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    LookupTable for multi-values.</span>
<span class="sd">    Also called embedding_lookup_sparse in TensorFlow.</span>

<span class="sd">    The input of LookupTableSparse should be a 2D SparseTensor or two 2D SparseTensors.</span>
<span class="sd">    If the input is a SparseTensor, the values are positive integer ids,</span>
<span class="sd">    values in each row of this SparseTensor will be turned into a dense vector.</span>
<span class="sd">    If the input is two SparseTensors, the first tensor should be the integer ids, just</span>
<span class="sd">    like the SparseTensor input. And the second tensor is the corresponding</span>
<span class="sd">    weights of the integer ids.</span>

<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>

<span class="sd">    &gt;&gt;&gt; lookupTableSparse = LookupTableSparse(20, 5, &quot;mean&quot;, 2, L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLookupTableSparse</span>
<span class="sd">    &gt;&gt;&gt; indices = np.array([[0, 0, 1, 2], [0, 1, 0, 3]])</span>
<span class="sd">    &gt;&gt;&gt; values = np.array([2, 4, 1, 2])</span>
<span class="sd">    &gt;&gt;&gt; weightValues = np.array([2, 0.5, 1, 3])</span>
<span class="sd">    &gt;&gt;&gt; input = JTensor.sparse(values, indices, np.array([3, 4]))</span>
<span class="sd">    &gt;&gt;&gt; weight = JTensor.sparse(weightValues, indices, np.array([3, 4]))</span>
<span class="sd">    &gt;&gt;&gt; layer1 = LookupTableSparse(10, 4, &quot;mean&quot;)</span>
<span class="sd">    creating: createLookupTableSparse</span>
<span class="sd">    &gt;&gt;&gt; layer1.set_weights(np.arange(1, 41, 1).reshape(10, 4)) # set weight to 1 to 40</span>
<span class="sd">    &gt;&gt;&gt; output = layer1.forward([input, weight])</span>
<span class="sd">    &gt;&gt;&gt; expected_output = np.array([[6.5999999 , 7.60000038, 8.60000038, 9.60000038],[ 1., 2., 3., 4.], [5., 6., 7., 8.]])</span>
<span class="sd">    &gt;&gt;&gt; np.testing.assert_allclose(output, expected_output, rtol=1e-6, atol=1e-6)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_index</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">combiner</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span>
                 <span class="n">max_norm</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LookupTableSparse</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">n_index</span><span class="p">,</span>
                                          <span class="n">n_output</span><span class="p">,</span>
                                          <span class="n">combiner</span><span class="p">,</span>
                                          <span class="n">max_norm</span> <span class="o">+</span> <span class="mf">0.0</span><span class="p">,</span>
                                          <span class="n">wRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="LookupTableSparse.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LookupTableSparse.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="MM"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MM">[docs]</a><span class="k">class</span> <span class="nc">MM</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Module to perform matrix multiplication on two mini-batch inputs, producing a mini-batch.</span>


<span class="sd">    :param trans_a: specifying whether or not transpose the first input matrix</span>
<span class="sd">    :param trans_b: specifying whether or not transpose the second input matrix</span>


<span class="sd">    &gt;&gt;&gt; mM = MM(True, True)</span>
<span class="sd">    creating: createMM</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">trans_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">trans_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                 <span class="n">trans_a</span><span class="p">,</span>
                                 <span class="n">trans_b</span><span class="p">)</span></div>


<div class="viewcode-block" id="MV"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MV">[docs]</a><span class="k">class</span> <span class="nc">MV</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a module to perform matrix vector multiplication on two mini-batch inputs,</span>
<span class="sd">    producing a mini-batch.</span>


<span class="sd">    :param trans: whether make matrix transpose before multiplication</span>


<span class="sd">    &gt;&gt;&gt; mV = MV(True)</span>
<span class="sd">    creating: createMV</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">trans</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                 <span class="n">trans</span><span class="p">)</span></div>


<div class="viewcode-block" id="MapTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MapTable">[docs]</a><span class="k">class</span> <span class="nc">MapTable</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is a container for a single module which will be applied</span>
<span class="sd">    to all input elements. The member module is cloned as necessary to</span>
<span class="sd">    process all input elements.</span>


<span class="sd">    &gt;&gt;&gt; mapTable = MapTable(Linear(100,10))</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    creating: createMapTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">module</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MapTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="MaskedSelect"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MaskedSelect">[docs]</a><span class="k">class</span> <span class="nc">MaskedSelect</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Performs a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument with</span>
<span class="sd">    the input on the forward and backward passes.</span>

<span class="sd">    &gt;&gt;&gt; maskedSelect = MaskedSelect()</span>
<span class="sd">    creating: createMaskedSelect</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaskedSelect</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Max"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Max">[docs]</a><span class="k">class</span> <span class="nc">Max</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a max operation over dimension `dim`</span>


<span class="sd">   :param dim: max along this dimension</span>
<span class="sd">   :param num_input_dims: Optional. If in a batch model, set to the inputDims.</span>


<span class="sd">    &gt;&gt;&gt; max = Max(1)</span>
<span class="sd">    creating: createMax</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Max</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">dim</span><span class="p">,</span>
                                  <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Mean"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Mean">[docs]</a><span class="k">class</span> <span class="nc">Mean</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple layer which applies a mean operation over the given dimension. When nInputDims</span>
<span class="sd">    is provided, the input will be considered as batches. Then the mean operation will be applied</span>
<span class="sd">    in (dimension + 1). The input to this layer is expected to be a tensor, or a batch of</span>
<span class="sd">    tensors; when using mini-batch, a batch of sample tensors will be passed to the layer and the</span>
<span class="sd">    user need to specify the number of dimensions of each sample tensor in the batch using</span>
<span class="sd">    nInputDims.</span>


<span class="sd">    :param dimension: the dimension to be applied mean operation</span>
<span class="sd">    :param n_input_dims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimension would be consideredas batch size</span>
<span class="sd">    :param squeeze: default is true, which will squeeze the sum dimension; set it to false to keep the sum dimension </span>

<span class="sd">    &gt;&gt;&gt; mean = Mean(1, 1, True)</span>
<span class="sd">    creating: createMean</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">squeeze</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mean</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">dimension</span><span class="p">,</span>
                                   <span class="n">n_input_dims</span><span class="p">,</span>
                                   <span class="n">squeeze</span><span class="p">)</span></div>


<div class="viewcode-block" id="Min"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Min">[docs]</a><span class="k">class</span> <span class="nc">Min</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a min operation over dimension `dim`.</span>


<span class="sd">    :param dim: min along this dimension</span>
<span class="sd">    :param num_input_dims: Optional. If in a batch model, set to the input_dim.</span>


<span class="sd">    &gt;&gt;&gt; min = Min(1)</span>
<span class="sd">    creating: createMin</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Min</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">dim</span><span class="p">,</span>
                                  <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="MixtureTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MixtureTable">[docs]</a><span class="k">class</span> <span class="nc">MixtureTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a table {gater, experts} as input and outputs the mixture of experts</span>
<span class="sd">    (a Tensor or table of Tensors) using a gater Tensor. When dim is provided, it specifies the</span>
<span class="sd">    dimension of the experts Tensor that will be interpolated (or mixed). Otherwise, the experts</span>
<span class="sd">    should take the form of a table of Tensors. This Module works for experts of dimension 1D or</span>
<span class="sd">    more, and for a 1D or 2D gater, i.e. for single examples or mini-batches.</span>


<span class="sd">    &gt;&gt;&gt; mixtureTable = MixtureTable()</span>
<span class="sd">    creating: createMixtureTable</span>
<span class="sd">    &gt;&gt;&gt; mixtureTable = MixtureTable(10)</span>
<span class="sd">    creating: createMixtureTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="n">INTMAX</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MixtureTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="Mul"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Mul">[docs]</a><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Multiply a single scalar factor to the incoming data</span>


<span class="sd">    &gt;&gt;&gt; mul = Mul()</span>
<span class="sd">    creating: createMul</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mul</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>

<div class="viewcode-block" id="Mul.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Mul.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="MulConstant"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MulConstant">[docs]</a><span class="k">class</span> <span class="nc">MulConstant</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Multiplies input Tensor by a (non-learnable) scalar constant.</span>
<span class="sd">    This module is sometimes useful for debugging purposes.</span>


<span class="sd">    :param scalar: scalar constant</span>
<span class="sd">    :param inplace: Can optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; mulConstant = MulConstant(2.5)</span>
<span class="sd">    creating: createMulConstant</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">scalar</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MulConstant</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">scalar</span><span class="p">,</span>
                                          <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Narrow"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Narrow">[docs]</a><span class="k">class</span> <span class="nc">Narrow</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Narrow is application of narrow operation in a module.</span>
<span class="sd">    The module further supports a negative length in order to handle inputs with an unknown size.</span>

<span class="sd">    &gt;&gt;&gt; narrow = Narrow(1, 1, 1)</span>
<span class="sd">    creating: createNarrow</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">offset</span><span class="p">,</span>
                 <span class="n">length</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Narrow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">dimension</span><span class="p">,</span>
                                     <span class="n">offset</span><span class="p">,</span>
                                     <span class="n">length</span><span class="p">)</span></div>


<div class="viewcode-block" id="NarrowTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.NarrowTable">[docs]</a><span class="k">class</span> <span class="nc">NarrowTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a table as input and outputs the subtable starting at index</span>
<span class="sd">    offset having length elements (defaults to 1 element). The elements can be either</span>
<span class="sd">    a table or a Tensor. If `length` is negative, it means selecting the elements from the</span>
<span class="sd">    offset to element which located at the abs(`length`) to the last element of the input.</span>


<span class="sd">    :param offset: the start index of table</span>
<span class="sd">    :param length: the length want to select</span>


<span class="sd">    &gt;&gt;&gt; narrowTable = NarrowTable(1, 1)</span>
<span class="sd">    creating: createNarrowTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">offset</span><span class="p">,</span>
                 <span class="n">length</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NarrowTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">offset</span><span class="p">,</span>
                                          <span class="n">length</span><span class="p">)</span></div>


<div class="viewcode-block" id="Normalize"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Normalize">[docs]</a><span class="k">class</span> <span class="nc">Normalize</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Normalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents</span>
<span class="sd">    division by zero when the input contains all zero elements (default = 1e-10).</span>
<span class="sd">    p can be the max value of double</span>


<span class="sd">    &gt;&gt;&gt; normalize = Normalize(1e-5, 1e-5)</span>
<span class="sd">    creating: createNormalize</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">p</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Normalize</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">p</span><span class="p">,</span>
                                        <span class="n">eps</span><span class="p">)</span></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PReLU">[docs]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies parametric ReLU, which parameter varies the slope of the negative part.</span>


<span class="sd">    PReLU: f(x) = max(0, x) + a * min(0, x)</span>


<span class="sd">    nOutputPlane&#39;s default value is 0, that means using PReLU in shared version and has</span>
<span class="sd">    only one parameters.</span>


<span class="sd">    Notice: Please don&#39;t use weight decay on this.</span>


<span class="sd">    :param n_output_plane: input map number. Default is 0.</span>


<span class="sd">    &gt;&gt;&gt; pReLU = PReLU(1)</span>
<span class="sd">    creating: createPReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">n_output_plane</span><span class="p">)</span>

<div class="viewcode-block" id="PReLU.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PReLU.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Padding"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Padding">[docs]</a><span class="k">class</span> <span class="nc">Padding</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This module adds pad units of padding to dimension dim of the input. If pad is negative,</span>
<span class="sd">    padding is added to the left, otherwise, it is added to the right of the dimension.</span>


<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user need to specify the number of dimensions of each sample tensor in the</span>
<span class="sd">    batch using n_input_dim.</span>


<span class="sd">    :param dim: the dimension to be applied padding operation</span>
<span class="sd">    :param pad: num of the pad units</span>
<span class="sd">    :param n_input_dim: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>
<span class="sd">    :param value: padding value</span>


<span class="sd">    &gt;&gt;&gt; padding = Padding(1, 1, 1, 1e-5, 1)</span>
<span class="sd">    creating: createPadding</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">pad</span><span class="p">,</span>
                 <span class="n">n_input_dim</span><span class="p">,</span>
                 <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">n_index</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Padding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">dim</span><span class="p">,</span>
                                      <span class="n">pad</span><span class="p">,</span>
                                      <span class="n">n_input_dim</span><span class="p">,</span>
                                      <span class="n">value</span><span class="p">,</span>
                                      <span class="n">n_index</span><span class="p">)</span></div>


<div class="viewcode-block" id="PairwiseDistance"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PairwiseDistance">[docs]</a><span class="k">class</span> <span class="nc">PairwiseDistance</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a module that takes a table of two vectors as input and outputs</span>
<span class="sd">    the distance between them using the p-norm.</span>
<span class="sd">    The input given in `forward(input)` is a [[Table]] that contains two tensors which</span>
<span class="sd">    must be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,</span>
<span class="sd">    it must have the size of `inputSize`. If it is a matrix, then each row is assumed to be</span>
<span class="sd">    an input sample of the given batch (the number of rows means the batch size and</span>
<span class="sd">    the number of columns should be equal to the `inputSize`).</span>

<span class="sd">    :param norm: the norm of distance</span>


<span class="sd">    &gt;&gt;&gt; pairwiseDistance = PairwiseDistance(2)</span>
<span class="sd">    creating: createPairwiseDistance</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">norm</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PairwiseDistance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">norm</span><span class="p">)</span></div>


<div class="viewcode-block" id="ParallelTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ParallelTable">[docs]</a><span class="k">class</span> <span class="nc">ParallelTable</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a container module that applies the i-th member module to the i-th</span>
<span class="sd">    input, and outputs an output in the form of Table</span>


<span class="sd">    &gt;&gt;&gt; parallelTable = ParallelTable()</span>
<span class="sd">    creating: createParallelTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Power"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Power">[docs]</a><span class="k">class</span> <span class="nc">Power</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply an element-wise power operation with scale and shift.</span>
<span class="sd">    f(x) = (shift + scale * x)^power^</span>

<span class="sd">    :param power: the exponent.</span>
<span class="sd">    :param scale: Default is 1.</span>
<span class="sd">    :param shift: Default is 0.</span>


<span class="sd">    &gt;&gt;&gt; power = Power(1e-5)</span>
<span class="sd">    creating: createPower</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">power</span><span class="p">,</span>
                 <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">shift</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Power</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">power</span><span class="p">,</span>
                                    <span class="n">scale</span><span class="p">,</span>
                                    <span class="n">shift</span><span class="p">)</span></div>


<div class="viewcode-block" id="RReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RReLU">[docs]</a><span class="k">class</span> <span class="nc">RReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,</span>
<span class="sd">    thus outputting a Tensor of the same dimension. Informally the RReLU is also known as</span>
<span class="sd">    &#39;insanity&#39; layer. RReLU is defined as:</span>
<span class="sd">```</span>
<span class="sd">        f(x) = max(0,x) + a * min(0, x) where a ~ U(l, u).</span>
<span class="sd">```</span>

<span class="sd">    In training mode negative inputs are multiplied by a factor drawn from a uniform random</span>
<span class="sd">    distribution U(l, u).</span>


<span class="sd">    In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean factor</span>
<span class="sd">        a = (l + u) / 2.</span>


<span class="sd">    By default, l = 1/8 and u = 1/3. If l == u a RReLU effectively becomes a LeakyReLU.</span>


<span class="sd">    Regardless of operating in in-place mode a RReLU will internally allocate an input-sized</span>
<span class="sd">    noise tensor to store random factors for negative inputs.</span>


<span class="sd">    The backward() operation assumes that forward() has been called before.</span>


<span class="sd">    For reference see [Empirical Evaluation of Rectified Activations in Convolutional Network](</span>
<span class="sd">    http://arxiv.org/abs/1505.00853).</span>


<span class="sd">    :param lower: lower boundary of uniform random distribution</span>
<span class="sd">    :param upper: upper boundary of uniform random distribution</span>
<span class="sd">    :param inplace: optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; rReLU = RReLU(1e-5, 1e5, True)</span>
<span class="sd">    creating: createRReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">lower</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">upper</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">lower</span><span class="p">,</span>
                                    <span class="n">upper</span><span class="p">,</span>
                                    <span class="n">inplace</span><span class="p">)</span></div>

<div class="viewcode-block" id="SpatialSeperableConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialSeperableConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialSeperableConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Separable convolutions consist in first performing a depthwise spatial convolution (which acts</span>
<span class="sd">    on each input channel separately) followed by a pointwise convolution which mixes together the</span>
<span class="sd">    resulting output channels. The  depth_multiplier argument controls how many output channels are</span>
<span class="sd">    generated per input channel in the depthwise step.</span>

<span class="sd">    :param n_input_channel The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param n_output_channel The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param depth_multiplier how many internal channels are generated per input channel</span>
<span class="sd">    :param kernel_w The kernel width of the convolution</span>
<span class="sd">    :param kernel_h The kernel height of the convolution</span>
<span class="sd">    :param stride_w The step of the convolution in the width dimension.</span>
<span class="sd">    :param stride_h The step of the convolution in the height dimension</span>
<span class="sd">    :param pad_w The additional zeros added per width to the input planes.</span>
<span class="sd">    :param pad_h The additional zeros added per height to the input planes.</span>
<span class="sd">    :param with_bias: the optional initial value for if need bias</span>
<span class="sd">    :param data_format: a string value of &quot;NHWC&quot; or &quot;NCHW&quot; to specify the input data format of this layer. In &quot;NHWC&quot; format</span>
<span class="sd">                       data is stored in the order of [batch_size, height, width, channels], in &quot;NCHW&quot; format data is stored</span>
<span class="sd">                       in the order of [batch_size, channels, height, width].</span>
<span class="sd">    :param w_regularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the depth weights matrices.</span>
<span class="sd">    :param b_regularizer: instance of [[Regularizer]]applied to the pointwise bias.</span>
<span class="sd">    :param p_regularizer: instance of [[Regularizer]]applied to the pointwise weights.</span>

<span class="sd">    &gt;&gt;&gt; conv = SpatialSeperableConvolution(6, 12, 1, 5, 5)</span>
<span class="sd">    creating: createSpatialSeperableConvolution</span>
<span class="sd">    &gt;&gt;&gt; conv.setWRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; conv.setBRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; conv = SpatialSeperableConvolution(6, 12, 1, 5, 5, 1, 1, 0, 0, True, &quot;NCHW&quot;, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createSpatialSeperableConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_channel</span><span class="p">,</span>
                 <span class="n">n_output_channel</span><span class="p">,</span>
                 <span class="n">depth_multiplier</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">kernel_h</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">w_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">b_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">p_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialSeperableConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_input_channel</span><span class="p">,</span>
                                                 <span class="n">n_output_channel</span><span class="p">,</span>
                                                 <span class="n">depth_multiplier</span><span class="p">,</span>
                                                 <span class="n">kernel_w</span><span class="p">,</span>
                                                 <span class="n">kernel_h</span><span class="p">,</span>
                                                 <span class="n">stride_w</span><span class="p">,</span>
                                                 <span class="n">stride_h</span><span class="p">,</span>
                                                 <span class="n">pad_w</span><span class="p">,</span>
                                                 <span class="n">pad_h</span><span class="p">,</span>
                                                 <span class="n">with_bias</span><span class="p">,</span>
                                                 <span class="n">data_format</span><span class="p">,</span>
                                                 <span class="n">w_regularizer</span><span class="p">,</span>
                                                 <span class="n">b_regularizer</span><span class="p">,</span>
                                                 <span class="n">p_regularizer</span><span class="p">,</span>
                                                 <span class="p">)</span></div>

<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ReLU6">[docs]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Same as ReLU except that the rectifying function f(x) saturates at x = 6</span>


<span class="sd">    :param inplace: either True = in-place or False = keeping separate state</span>


<span class="sd">    &gt;&gt;&gt; reLU6 = ReLU6(True)</span>
<span class="sd">    creating: createReLU6</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReLU6</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">inplace</span><span class="p">)</span></div>

<div class="viewcode-block" id="SReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SReLU">[docs]</a><span class="k">class</span> <span class="nc">SReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;S-shaped Rectified Linear Unit.</span>

<span class="sd">    It follows:</span>
<span class="sd">    `f(x) = t^r + a^r(x - t^r) for x &gt;= t^r`,</span>
<span class="sd">    `f(x) = x for t^r &gt; x &gt; t^l`,</span>
<span class="sd">    `f(x) = t^l + a^l(x - t^l) for x &lt;= t^l`.</span>

<span class="sd">    # References</span>
<span class="sd">        - [Deep Learning with S-shaped Rectified Linear Activation Units](http://arxiv.org/abs/1512.07030)</span>



<span class="sd">    :param shared_axes: the axes along which to share learnable</span>
<span class="sd">            parameters for the activation function.</span>
<span class="sd">            For example, if the incoming feature maps</span>
<span class="sd">            are from a 2D convolution</span>
<span class="sd">            with output shape `(batch, height, width, channels)`,</span>
<span class="sd">            and you wish to share parameters across space</span>
<span class="sd">            so that each filter only has one set of parameters,</span>
<span class="sd">            set `shared_axes=[1, 2]`.</span>

<span class="sd">    &gt;&gt;&gt; srelu = SReLU()</span>
<span class="sd">    creating: createSReLU</span>
<span class="sd">    &gt;&gt;&gt; srelu = SReLU((1, 2))</span>
<span class="sd">    creating: createSReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">share_axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">share_axes</span><span class="p">)</span>

<div class="viewcode-block" id="SReLU.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SReLU.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tLeftInit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aLeftInit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">tRightInit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aRightInit</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">tLeftInit</span><span class="p">,</span> <span class="n">aLeftInit</span><span class="p">,</span> <span class="n">tRightInit</span><span class="p">,</span> <span class="n">aRightInit</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="ActivityRegularization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ActivityRegularization">[docs]</a><span class="k">class</span> <span class="nc">ActivityRegularization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Layer that applies an update to the cost function based input activity.</span>

<span class="sd">    :param l1: L1 regularization factor (positive float).</span>
<span class="sd">    :param l2: L2 regularization factor (positive float).</span>


<span class="sd">    &gt;&gt;&gt; ar = ActivityRegularization(0.1, 0.02)</span>
<span class="sd">    creating: createActivityRegularization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">l1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ActivityRegularization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span></div>

<div class="viewcode-block" id="Replicate"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Replicate">[docs]</a><span class="k">class</span> <span class="nc">Replicate</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Replicate repeats input `nFeatures` times along its `dim` dimension.</span>
<span class="sd">    Notice: No memory copy, it set the stride along the `dim`-th dimension to zero.</span>


<span class="sd">    :param n_features: replicate times.</span>
<span class="sd">    :param dim: dimension to be replicated.</span>
<span class="sd">    :param n_dim: specify the number of non-batch dimensions.</span>


<span class="sd">    &gt;&gt;&gt; replicate = Replicate(2)</span>
<span class="sd">    creating: createReplicate</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_features</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_dim</span><span class="o">=</span><span class="n">INTMAX</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Replicate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">n_features</span><span class="p">,</span>
                                        <span class="n">dim</span><span class="p">,</span>
                                        <span class="n">n_dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="RoiPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RoiPooling">[docs]</a><span class="k">class</span> <span class="nc">RoiPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Region of interest pooling</span>
<span class="sd">    The RoIPooling uses max pooling to convert the features inside any valid region of interest</span>
<span class="sd">    into a small feature map with a fixed spatial extent of pooledH * pooledW (e.g., 7 * 7)</span>
<span class="sd">    an RoI is a rectangular window into a conv feature map.</span>
<span class="sd">    Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its</span>
<span class="sd">    top-left corner (x1, y1) and its bottom-right corner (x2, y2).</span>
<span class="sd">    RoI max pooling works by dividing the h * w RoI window into an pooledH * pooledW grid of</span>
<span class="sd">    sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window</span>
<span class="sd">    into the corresponding output grid cell.</span>
<span class="sd">    Pooling is applied independently to each feature map channel</span>


<span class="sd">    :param pooled_w:      spatial extent in width</span>
<span class="sd">    :param pooled_h:      spatial extent in height</span>
<span class="sd">    :param spatial_scale: spatial scale</span>


<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(2,2,6,8)</span>
<span class="sd">    &gt;&gt;&gt; input_rois = np.array([0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3],dtype=&#39;float64&#39;).reshape(4,5)</span>
<span class="sd">    &gt;&gt;&gt; m = RoiPooling(3,2,1.0)</span>
<span class="sd">    creating: createRoiPooling</span>
<span class="sd">    &gt;&gt;&gt; out = m.forward([input_data,input_rois])</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pooled_w</span><span class="p">,</span>
                 <span class="n">pooled_h</span><span class="p">,</span>
                 <span class="n">spatial_scale</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RoiPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">pooled_w</span><span class="p">,</span>
                                         <span class="n">pooled_h</span><span class="p">,</span>
                                         <span class="n">spatial_scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="Scale"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Scale">[docs]</a><span class="k">class</span> <span class="nc">Scale</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Scale is the combination of CMul and CAdd</span>
<span class="sd">    Computes the elementwise product of input and weight, with the shape of the weight &quot;expand&quot; to</span>
<span class="sd">    match the shape of the input.</span>
<span class="sd">    Similarly, perform a expand cdd bias and perform an elementwise add</span>


<span class="sd">    :param size: size of weight and bias</span>


<span class="sd">    &gt;&gt;&gt; scale = Scale([1,2])</span>
<span class="sd">    creating: createScale</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Scale</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="SelectTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SelectTable">[docs]</a><span class="k">class</span> <span class="nc">SelectTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a table as input and outputs the element at index `index`</span>
<span class="sd">    (positive or negative). This can be either a table or a Tensor.</span>
<span class="sd">    The gradients of the non-index elements are zeroed Tensors of the same size.</span>
<span class="sd">    This is true regardless of the depth of the encapsulated Tensor as the function used</span>
<span class="sd">    internally to do so is recursive.</span>


<span class="sd">    :param index: the index to be selected</span>


<span class="sd">    &gt;&gt;&gt; selectTable = SelectTable(1)</span>
<span class="sd">    creating: createSelectTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">index</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelectTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftMax"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftMax">[docs]</a><span class="k">class</span> <span class="nc">SoftMax</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the</span>
<span class="sd">    elements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.</span>
<span class="sd">    Softmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)</span>
<span class="sd">    where shift = max_i(x_i).</span>


<span class="sd">    &gt;&gt;&gt; softMax = SoftMax()</span>
<span class="sd">    creating: createSoftMax</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftMin"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftMin">[docs]</a><span class="k">class</span> <span class="nc">SoftMin</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the</span>
<span class="sd">    elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.</span>
<span class="sd">    Softmin is defined as: f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)</span>
<span class="sd">    where shift = max_i(-x_i).</span>


<span class="sd">    &gt;&gt;&gt; softMin = SoftMin()</span>
<span class="sd">    creating: createSoftMin</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMin</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftPlus"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftPlus">[docs]</a><span class="k">class</span> <span class="nc">SoftPlus</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply the SoftPlus function to an n-dimensional input tensor.</span>
<span class="sd">    SoftPlus function: f_i(x) = 1/beta * log(1 + exp(beta * x_i))</span>


<span class="sd">    :param beta: Controls sharpness of transfer function</span>


<span class="sd">    &gt;&gt;&gt; softPlus = SoftPlus(1e-5)</span>
<span class="sd">    creating: createSoftPlus</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftPlus</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">beta</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftShrink"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftShrink">[docs]</a><span class="k">class</span> <span class="nc">SoftShrink</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply the soft shrinkage function element-wise to the input Tensor</span>


<span class="sd">    SoftShrinkage operator:</span>
<span class="sd">```</span>
<span class="sd">           | x - lambda, if x &gt;  lambda</span>
<span class="sd">    f(x) = | x + lambda, if x &lt; -lambda</span>
<span class="sd">           | 0, otherwise</span>
<span class="sd">```</span>

<span class="sd">    :param the_lambda: lambda, default is 0.5</span>


<span class="sd">    &gt;&gt;&gt; softShrink = SoftShrink(1e-5)</span>
<span class="sd">    creating: createSoftShrink</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">the_lambda</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftShrink</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">the_lambda</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftSign"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftSign">[docs]</a><span class="k">class</span> <span class="nc">SoftSign</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply SoftSign function to an n-dimensional input Tensor.</span>


<span class="sd">    SoftSign function: f_i(x) = x_i / (1+|x_i|)</span>


<span class="sd">    &gt;&gt;&gt; softSign = SoftSign()</span>
<span class="sd">    creating: createSoftSign</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftSign</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialDilatedConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDilatedConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialDilatedConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply a 2D dilated convolution over an input image.</span>


<span class="sd">    The input tensor is expected to be a 3D or 4D(with batch) tensor.</span>


<span class="sd">    If input is a 3D tensor nInputPlane x height x width,</span>
<span class="sd">    owidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1</span>
<span class="sd">    oheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1</span>


<span class="sd">    Reference Paper: Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].</span>
<span class="sd">    arXiv preprint arXiv:1511.07122, 2015.</span>


<span class="sd">    :param n_input_plane: The number of expected input planes in the image given into forward().</span>
<span class="sd">    :param n_output_plane: The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kw: The kernel width of the convolution.</span>
<span class="sd">    :param kh: The kernel height of the convolution.</span>
<span class="sd">    :param dw: The step of the convolution in the width dimension. Default is 1.</span>
<span class="sd">    :param dh: The step of the convolution in the height dimension. Default is 1.</span>
<span class="sd">    :param pad_w: The additional zeros added per width to the input planes. Default is 0.</span>
<span class="sd">    :param pad_h: The additional zeros added per height to the input planes. Default is 0.</span>
<span class="sd">    :param dilation_w: The number of pixels to skip. Default is 1.</span>
<span class="sd">    :param dilation_h: The number of pixels to skip. Default is 1.</span>
<span class="sd">    :param init_method: Init method, Default, Xavier.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; spatialDilatedConvolution = SpatialDilatedConvolution(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialDilatedConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDilatedConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                        <span class="n">n_input_plane</span><span class="p">,</span>
                                                        <span class="n">n_output_plane</span><span class="p">,</span>
                                                        <span class="n">kw</span><span class="p">,</span>
                                                        <span class="n">kh</span><span class="p">,</span>
                                                        <span class="n">dw</span><span class="p">,</span>
                                                        <span class="n">dh</span><span class="p">,</span>
                                                        <span class="n">pad_w</span><span class="p">,</span>
                                                        <span class="n">pad_h</span><span class="p">,</span>
                                                        <span class="n">dilation_w</span><span class="p">,</span>
                                                        <span class="n">dilation_h</span><span class="p">,</span>
                                                        <span class="n">wRegularizer</span><span class="p">,</span>
                                                        <span class="n">bRegularizer</span><span class="p">)</span>
                                                        
<div class="viewcode-block" id="SpatialDilatedConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDilatedConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpatialFullConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialFullConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialFullConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply a 2D full convolution over an input image.</span>
<span class="sd">    The input tensor is expected to be a 3D or 4D(with batch) tensor. Note that instead</span>
<span class="sd">    of setting adjW and adjH, SpatialFullConvolution[Table, T] also accepts a table input</span>
<span class="sd">    with two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,</span>
<span class="sd">    and the size of sizeTensor is used to set the size of the output (will ignore the adjW and</span>
<span class="sd">    adjH values used to construct the module). This module can be used without a bias by setting</span>
<span class="sd">    parameter noBias = true while constructing the module.</span>


<span class="sd">    If input is a 3D tensor nInputPlane x height x width,</span>
<span class="sd">    owidth  = (width  - 1) * dW - 2*padW + kW + adjW</span>
<span class="sd">    oheight = (height - 1) * dH - 2*padH + kH + adjH</span>


<span class="sd">    Other frameworks call this operation &quot;In-network Upsampling&quot;, &quot;Fractionally-strided convolution&quot;,</span>
<span class="sd">    &quot;Backwards Convolution,&quot; &quot;Deconvolution&quot;, or &quot;Upconvolution.&quot;</span>


<span class="sd">    Reference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic</span>
<span class="sd">    segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</span>
<span class="sd">    2015: 3431-3440.</span>

<span class="sd">    :param nInputPlane The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param nOutputPlane The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kW The kernel width of the convolution.</span>
<span class="sd">    :param kH The kernel height of the convolution.</span>
<span class="sd">    :param dW The step of the convolution in the width dimension. Default is 1.</span>
<span class="sd">    :param dH The step of the convolution in the height dimension. Default is 1.</span>
<span class="sd">    :param padW The additional zeros added per width to the input planes. Default is 0.</span>
<span class="sd">    :param padH The additional zeros added per height to the input planes. Default is 0.</span>
<span class="sd">    :param adjW Extra width to add to the output image. Default is 0.</span>
<span class="sd">    :param adjH Extra height to add to the output image. Default is 0.</span>
<span class="sd">    :param nGroup Kernel group number.</span>
<span class="sd">    :param noBias If bias is needed.</span>
<span class="sd">    :param initMethod Init method, Default, Xavier, Bilinear.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; spatialFullConvolution = SpatialFullConvolution(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialFullConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialFullConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                     <span class="n">n_input_plane</span><span class="p">,</span>
                                                     <span class="n">n_output_plane</span><span class="p">,</span>
                                                     <span class="n">kw</span><span class="p">,</span>
                                                     <span class="n">kh</span><span class="p">,</span>
                                                     <span class="n">dw</span><span class="p">,</span>
                                                     <span class="n">dh</span><span class="p">,</span>
                                                     <span class="n">pad_w</span><span class="p">,</span>
                                                     <span class="n">pad_h</span><span class="p">,</span>
                                                     <span class="n">adj_w</span><span class="p">,</span>
                                                     <span class="n">adj_h</span><span class="p">,</span>
                                                     <span class="n">n_group</span><span class="p">,</span>
                                                     <span class="n">no_bias</span><span class="p">,</span>
                                                     <span class="n">wRegularizer</span><span class="p">,</span>
                                                     <span class="n">bRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="SpatialFullConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialFullConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="VolumetricFullConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricFullConvolution">[docs]</a><span class="k">class</span> <span class="nc">VolumetricFullConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply a 3D full convolution over an 3D input image, a sequence of images, or a video etc.</span>
<span class="sd">    The input tensor is expected to be a 4D or 5D(with batch) tensor. Note that instead</span>
<span class="sd">    of setting adjT, adjW and adjH, `VolumetricFullConvolution` also accepts a table input</span>
<span class="sd">    with two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,</span>
<span class="sd">    and the size of sizeTensor is used to set the size of the output (will ignore the adjT, adjW and</span>
<span class="sd">    adjH values used to construct the module). This module can be used without a bias by setting</span>
<span class="sd">    parameter noBias = true while constructing the module.</span>


<span class="sd">    If input is a 4D tensor nInputPlane x depth x height x width,</span>
<span class="sd">    odepth = (depth  - 1) * dT - 2*padt + kT + adjT</span>
<span class="sd">    owidth  = (width  - 1) * dW - 2*padW + kW + adjW</span>
<span class="sd">    oheight = (height - 1) * dH - 2*padH + kH + adjH</span>


<span class="sd">    Other frameworks call this operation &quot;In-network Upsampling&quot;, &quot;Fractionally-strided convolution&quot;,</span>
<span class="sd">    &quot;Backwards Convolution,&quot; &quot;Deconvolution&quot;, or &quot;Upconvolution.&quot;</span>


<span class="sd">    Reference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic</span>
<span class="sd">    segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</span>
<span class="sd">    2015: 3431-3440.</span>

<span class="sd">    :param nInputPlane The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param nOutputPlane The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kT The kernel depth of the convolution.</span>
<span class="sd">    :param kW The kernel width of the convolution.</span>
<span class="sd">    :param kH The kernel height of the convolution.</span>
<span class="sd">    :param dT The step of the convolution in the depth dimension. Default is 1.</span>
<span class="sd">    :param dW The step of the convolution in the width dimension. Default is 1.</span>
<span class="sd">    :param dH The step of the convolution in the height dimension. Default is 1.</span>
<span class="sd">    :param padT The additional zeros added per depth to the input planes. Default is 0.</span>
<span class="sd">    :param padW The additional zeros added per width to the input planes. Default is 0.</span>
<span class="sd">    :param padH The additional zeros added per height to the input planes. Default is 0.</span>
<span class="sd">    :param adjT Extra depth to add to the output image. Default is 0.</span>
<span class="sd">    :param adjW Extra width to add to the output image. Default is 0.</span>
<span class="sd">    :param adjH Extra height to add to the output image. Default is 0.</span>
<span class="sd">    :param nGroup Kernel group number.</span>
<span class="sd">    :param noBias If bias is needed.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; volumetricFullConvolution = VolumetricFullConvolution(1, 1, 1, 1, 1, 1)</span>
<span class="sd">    creating: createVolumetricFullConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kt</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VolumetricFullConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                     <span class="n">n_input_plane</span><span class="p">,</span>
                                                     <span class="n">n_output_plane</span><span class="p">,</span>
                                                     <span class="n">kt</span><span class="p">,</span>
                                                     <span class="n">kw</span><span class="p">,</span>
                                                     <span class="n">kh</span><span class="p">,</span>
                                                     <span class="n">dt</span><span class="p">,</span>
                                                     <span class="n">dw</span><span class="p">,</span>
                                                     <span class="n">dh</span><span class="p">,</span>
                                                     <span class="n">pad_t</span><span class="p">,</span>
                                                     <span class="n">pad_w</span><span class="p">,</span>
                                                     <span class="n">pad_h</span><span class="p">,</span>
                                                     <span class="n">adj_t</span><span class="p">,</span>
                                                     <span class="n">adj_w</span><span class="p">,</span>
                                                     <span class="n">adj_h</span><span class="p">,</span>
                                                     <span class="n">n_group</span><span class="p">,</span>
                                                     <span class="n">no_bias</span><span class="p">,</span>
                                                     <span class="n">wRegularizer</span><span class="p">,</span>
                                                     <span class="n">bRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="VolumetricFullConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricFullConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>

<div class="viewcode-block" id="SpatialShareConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialShareConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialShareConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>

<span class="sd">    &gt;&gt;&gt; spatialShareConvolution = SpatialShareConvolution(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialShareConvolution</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(1, 12, 6, 5, 5)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(12)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([1, 12, 6, 5, 5])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([12])</span>
<span class="sd">    &gt;&gt;&gt; conv = SpatialShareConvolution(6, 12, 5, 5, 1, 1, 0, 0, 1, True, L1Regularizer(0.5), L1Regularizer(0.5), init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createSpatialShareConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">kernel_h</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialShareConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                      <span class="n">n_input_plane</span><span class="p">,</span>
                                                      <span class="n">n_output_plane</span><span class="p">,</span>
                                                      <span class="n">kernel_w</span><span class="p">,</span>
                                                      <span class="n">kernel_h</span><span class="p">,</span>
                                                      <span class="n">stride_w</span><span class="p">,</span>
                                                      <span class="n">stride_h</span><span class="p">,</span>
                                                      <span class="n">pad_w</span><span class="p">,</span>
                                                      <span class="n">pad_h</span><span class="p">,</span>
                                                      <span class="n">n_group</span><span class="p">,</span>
                                                      <span class="n">propagate_back</span><span class="p">,</span>
                                                      <span class="n">wRegularizer</span><span class="p">,</span>
                                                      <span class="n">bRegularizer</span><span class="p">,</span>
                                                      <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                      <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                      <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                      <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">),</span>
                                                      <span class="n">with_bias</span><span class="p">)</span>
<div class="viewcode-block" id="SpatialShareConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialShareConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="VolumetricConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricConvolution">[docs]</a><span class="k">class</span> <span class="nc">VolumetricConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a 3D convolution over an input image composed of several input planes. The input tensor</span>
<span class="sd">    in forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).</span>

<span class="sd">    :param n_input_plane: The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param n_output_plane: The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param k_t: The kernel size of the convolution in time</span>
<span class="sd">    :param k_w: The kernel width of the convolution</span>
<span class="sd">    :param k_h: The kernel height of the convolution</span>
<span class="sd">    :param d_t: The step of the convolution in the time dimension. Default is 1</span>
<span class="sd">    :param d_w: The step of the convolution in the width dimension. Default is 1</span>
<span class="sd">    :param d_h: The step of the convolution in the height dimension. Default is 1</span>
<span class="sd">    :param pad_t: Additional zeros added to the input plane data on both sides of time axis.Default is 0. (kT-1)/2 is often used here.</span>
<span class="sd">    :param pad_w: The additional zeros added per width to the input planes.</span>
<span class="sd">    :param pad_h: The additional zeros added per height to the input planes.</span>
<span class="sd">    :param with_bias: whether with bias</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]] (eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]] applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; volumetricConvolution = VolumetricConvolution(6, 12, 5, 5, 5, 1, 1, 1)</span>
<span class="sd">    creating: createVolumetricConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">k_t</span><span class="p">,</span>
                 <span class="n">k_w</span><span class="p">,</span>
                 <span class="n">k_h</span><span class="p">,</span>
                 <span class="n">d_t</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">d_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">d_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VolumetricConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">n_input_plane</span><span class="p">,</span>
                                                    <span class="n">n_output_plane</span><span class="p">,</span>
                                                    <span class="n">k_t</span><span class="p">,</span>
                                                    <span class="n">k_w</span><span class="p">,</span>
                                                    <span class="n">k_h</span><span class="p">,</span>
                                                    <span class="n">d_t</span><span class="p">,</span>
                                                    <span class="n">d_w</span><span class="p">,</span>
                                                    <span class="n">d_h</span><span class="p">,</span>
                                                    <span class="n">pad_t</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">,</span>
                                                    <span class="n">with_bias</span><span class="p">,</span>
                                                    <span class="n">wRegularizer</span><span class="p">,</span>
                                                    <span class="n">bRegularizer</span><span class="p">)</span>

<div class="viewcode-block" id="VolumetricConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="VolumetricMaxPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricMaxPooling">[docs]</a><span class="k">class</span> <span class="nc">VolumetricMaxPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 3D max-pooling operation in kTxkWxkH regions by step size dTxdWxdH.</span>
<span class="sd">    The number of output features is equal to the number of input planes / dT.</span>
<span class="sd">    The input can optionally be padded with zeros. Padding should be smaller than</span>
<span class="sd">    half of kernel size. That is, padT &lt; kT/2, padW &lt; kW/2 and padH &lt; kH/2</span>

<span class="sd">    :param k_t: The kernel size</span>
<span class="sd">    :param k_w: The kernel width</span>
<span class="sd">    :param k_h: The kernel height</span>
<span class="sd">    :param d_t: The step in the time dimension</span>
<span class="sd">    :param d_w: The step in the width dimension</span>
<span class="sd">    :param d_h: The step in the height dimension</span>
<span class="sd">    :param pad_t: The padding in the time dimension</span>
<span class="sd">    :param pad_w: The padding in the width dimension</span>
<span class="sd">    :param pad_h: The padding in the height dimension</span>


<span class="sd">    &gt;&gt;&gt; volumetricMaxPooling = VolumetricMaxPooling(5, 5, 5, 1, 1, 1)</span>
<span class="sd">    creating: createVolumetricMaxPooling</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">k_t</span><span class="p">,</span>
                 <span class="n">k_w</span><span class="p">,</span>
                 <span class="n">k_h</span><span class="p">,</span>
                 <span class="n">d_t</span><span class="p">,</span>
                 <span class="n">d_w</span><span class="p">,</span>
                 <span class="n">d_h</span><span class="p">,</span>
                 <span class="n">pad_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VolumetricMaxPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">k_t</span><span class="p">,</span>
                                                    <span class="n">k_w</span><span class="p">,</span>
                                                    <span class="n">k_h</span><span class="p">,</span>
                                                    <span class="n">d_t</span><span class="p">,</span>
                                                    <span class="n">d_w</span><span class="p">,</span>
                                                    <span class="n">d_h</span><span class="p">,</span>
                                                    <span class="n">pad_t</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">)</span></div>


<div class="viewcode-block" id="VolumetricAveragePooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricAveragePooling">[docs]</a><span class="k">class</span> <span class="nc">VolumetricAveragePooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 3D average-pooling operation in kTxkWxkH regions by step size dTxdWxdH.</span>
<span class="sd">    The number of output features is equal to the number of input planes / dT.</span>
<span class="sd">    The input can optionally be padded with zeros. Padding should be smaller than</span>
<span class="sd">    half of kernel size. That is, padT &lt; kT/2, padW &lt; kW/2 and padH &lt; kH/2</span>

<span class="sd">    :param k_t: The kernel size</span>
<span class="sd">    :param k_w: The kernel width</span>
<span class="sd">    :param k_h: The kernel height</span>
<span class="sd">    :param d_t: The step in the time dimension</span>
<span class="sd">    :param d_w: The step in the width dimension</span>
<span class="sd">    :param d_h: The step in the height dimension</span>
<span class="sd">    :param pad_t: The padding in the time dimension</span>
<span class="sd">    :param pad_w: The padding in the width dimension</span>
<span class="sd">    :param pad_h: The padding in the height dimension</span>
<span class="sd">    :param count_include_pad: whether to include padding when dividing the number of elements in pooling region</span>
<span class="sd">    :param ceil_mode: whether the output size is to be ceiled or floored</span>


<span class="sd">    &gt;&gt;&gt; volumetricAveragePooling = VolumetricAveragePooling(5, 5, 5, 1, 1, 1)</span>
<span class="sd">    creating: createVolumetricAveragePooling</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">k_t</span><span class="p">,</span>
                 <span class="n">k_w</span><span class="p">,</span>
                 <span class="n">k_h</span><span class="p">,</span>
                 <span class="n">d_t</span><span class="p">,</span>
                 <span class="n">d_w</span><span class="p">,</span>
                 <span class="n">d_h</span><span class="p">,</span>
                 <span class="n">pad_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VolumetricAveragePooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                        <span class="n">k_t</span><span class="p">,</span>
                                                        <span class="n">k_w</span><span class="p">,</span>
                                                        <span class="n">k_h</span><span class="p">,</span>
                                                        <span class="n">d_t</span><span class="p">,</span>
                                                        <span class="n">d_w</span><span class="p">,</span>
                                                        <span class="n">d_h</span><span class="p">,</span>
                                                        <span class="n">pad_t</span><span class="p">,</span>
                                                        <span class="n">pad_w</span><span class="p">,</span>
                                                        <span class="n">pad_h</span><span class="p">,</span>
                                                        <span class="n">count_include_pad</span><span class="p">,</span>
                                                        <span class="n">ceil_mode</span><span class="p">)</span></div>

<div class="viewcode-block" id="SpatialZeroPadding"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialZeroPadding">[docs]</a><span class="k">class</span> <span class="nc">SpatialZeroPadding</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Each feature map of a given input is padded with specified number of zeros.</span>
<span class="sd">    If padding values are negative, then input is cropped.</span>

<span class="sd">    :param padLeft: pad left position</span>
<span class="sd">    :param padRight: pad right position</span>
<span class="sd">    :param padTop: pad top position</span>
<span class="sd">    :param padBottom: pad bottom position</span>


<span class="sd">    &gt;&gt;&gt; spatialZeroPadding = SpatialZeroPadding(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialZeroPadding</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pad_left</span><span class="p">,</span>
                 <span class="n">pad_right</span><span class="p">,</span>
                 <span class="n">pad_top</span><span class="p">,</span>
                 <span class="n">pad_bottom</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialZeroPadding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">pad_left</span><span class="p">,</span>
                                                 <span class="n">pad_right</span><span class="p">,</span>
                                                 <span class="n">pad_top</span><span class="p">,</span>
                                                 <span class="n">pad_bottom</span><span class="p">)</span></div>


<div class="viewcode-block" id="SplitTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SplitTable">[docs]</a><span class="k">class</span> <span class="nc">SplitTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a Tensor as input and</span>
<span class="sd">    outputs several tables, splitting the Tensor along</span>
<span class="sd">    the specified dimension `dimension`. Please note the dimension starts from 1.</span>


<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user needs to specify the number of dimensions of each sample tensor in a</span>
<span class="sd">    batch using `nInputDims`.</span>


<span class="sd">    :param dimension: to be split along this dimension</span>
<span class="sd">    :param n_input_dims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>


<span class="sd">    &gt;&gt;&gt; splitTable = SplitTable(1, 1)</span>
<span class="sd">    creating: createSplitTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SplitTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">dimension</span><span class="p">,</span>
                                         <span class="n">n_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sqrt"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sqrt">[docs]</a><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply an element-wise sqrt operation.</span>


<span class="sd">    &gt;&gt;&gt; sqrt = Sqrt()</span>
<span class="sd">    creating: createSqrt</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sqrt</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Square"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Square">[docs]</a><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply an element-wise square operation.</span>

<span class="sd">    &gt;&gt;&gt; square = Square()</span>
<span class="sd">    creating: createSquare</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Square</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Squeeze"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Squeeze">[docs]</a><span class="k">class</span> <span class="nc">Squeeze</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Delete singleton all dimensions or a specific dim.</span>


<span class="sd">    :param dim: Optional. The dimension to be delete. Default: delete all dimensions.</span>
<span class="sd">    :param num_input_dims: Optional. If in a batch model, set to the inputDims.</span>




<span class="sd">    &gt;&gt;&gt; squeeze = Squeeze(1)</span>
<span class="sd">    creating: createSqueeze</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Squeeze</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">dim</span><span class="p">,</span>
                                      <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sum"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sum">[docs]</a><span class="k">class</span> <span class="nc">Sum</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple layer which applies a sum operation over the given dimension.</span>
<span class="sd">    When nInputDims is provided, the input will be considered as a batches.</span>
<span class="sd">    Then the sum operation will be applied in (dimension + 1)</span>
<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user need to specify the number of dimensions of each sample tensor in the</span>
<span class="sd">    batch using `nInputDims`.</span>


<span class="sd">    :param dimension: the dimension to be applied sum operation</span>
<span class="sd">    :param n_input_dims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>
<span class="sd">    :param size_average: default is false, if it is true, it will return the mean instead</span>
<span class="sd">    :param squeeze: default is true, which will squeeze the sum dimension; set it to false to keep the sum dimension</span>


<span class="sd">    &gt;&gt;&gt; sum = Sum(1, 1, True, True)</span>
<span class="sd">    creating: createSum</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">squeeze</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">dimension</span><span class="p">,</span>
                                  <span class="n">n_input_dims</span><span class="p">,</span>
                                  <span class="n">squeeze</span><span class="p">,</span>
                                  <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="TanhShrink"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TanhShrink">[docs]</a><span class="k">class</span> <span class="nc">TanhShrink</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A simple layer for each element of the input tensor, do the following operation</span>
<span class="sd">    during the forward process:</span>
<span class="sd">    [f(x) = tanh(x) - 1]</span>


<span class="sd">    &gt;&gt;&gt; tanhShrink = TanhShrink()</span>
<span class="sd">    creating: createTanhShrink</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TanhShrink</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Threshold"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Threshold">[docs]</a><span class="k">class</span> <span class="nc">Threshold</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Threshold input Tensor.</span>
<span class="sd">    If values in the Tensor smaller than th, then replace it with v</span>


<span class="sd">    :param th: the threshold to compare with</span>
<span class="sd">    :param v: the value to replace with</span>
<span class="sd">    :param ip: inplace mode</span>


<span class="sd">    &gt;&gt;&gt; threshold = Threshold(1e-5, 1e-5, True)</span>
<span class="sd">    creating: createThreshold</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">th</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                 <span class="n">v</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">ip</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Threshold</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">th</span><span class="p">,</span>
                                        <span class="n">v</span><span class="p">,</span>
                                        <span class="n">ip</span><span class="p">)</span></div>

<div class="viewcode-block" id="Negative"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Negative">[docs]</a><span class="k">class</span> <span class="nc">Negative</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Create an Negative layer.  Computing negative value of each element of input tensor</span>

<span class="sd">    :param inplace: if output tensor reuse input tensor storage. Default value is false</span>


<span class="sd">    &gt;&gt;&gt; negative = Negative(False)</span>
<span class="sd">    creating: createNegative</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inplace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Negative</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Unsqueeze"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Unsqueeze">[docs]</a><span class="k">class</span> <span class="nc">Unsqueeze</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Create an Unsqueeze layer.  Insert singleton dim (i.e., dimension 1) at position pos.</span>
<span class="sd">    For an input with dim = input.dim(),</span>
<span class="sd">    there are dim + 1 possible positions to insert the singleton dimension.</span>


<span class="sd">    :param pos: The position will be insert singleton.</span>
<span class="sd">    :param num_input_dims: Optional. If in a batch model, set to the inputDim</span>


<span class="sd">    &gt;&gt;&gt; unsqueeze = Unsqueeze(1, 1)</span>
<span class="sd">    creating: createUnsqueeze</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pos</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Unsqueeze</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">pos</span><span class="p">,</span>
                                        <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Reshape"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Reshape">[docs]</a><span class="k">class</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The forward(input) reshape the input tensor into a size(0) * size(1) * ... tensor, taking the</span>
<span class="sd">    elements row-wise.</span>


<span class="sd">    :param size: the reshape size</span>


<span class="sd">    &gt;&gt;&gt; reshape = Reshape([1, 28, 28])</span>
<span class="sd">    creating: createReshape</span>
<span class="sd">    &gt;&gt;&gt; reshape = Reshape([1, 28, 28], False)</span>
<span class="sd">    creating: createReshape</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">batch_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Reshape</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">batch_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="BiRecurrent"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BiRecurrent">[docs]</a><span class="k">class</span> <span class="nc">BiRecurrent</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Create a Bidirectional recurrent layer</span>


<span class="sd">    :param merge: merge layer</span>


<span class="sd">    &gt;&gt;&gt; biRecurrent = BiRecurrent(CAddTable())</span>
<span class="sd">    creating: createCAddTable</span>
<span class="sd">    creating: createBiRecurrent</span>
<span class="sd">    &gt;&gt;&gt; biRecurrent = BiRecurrent()</span>
<span class="sd">    creating: createBiRecurrent</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">merge</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BiRecurrent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">merge</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConcatTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ConcatTable">[docs]</a><span class="k">class</span> <span class="nc">ConcatTable</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ConcateTable is a container module like Concate. Applies an input</span>
<span class="sd">    to each member module, input can be a tensor or a table.</span>


<span class="sd">    ConcateTable usually works with CAddTable and CMulTable to</span>
<span class="sd">    implement element wise add/multiply on outputs of two modules.</span>


<span class="sd">    &gt;&gt;&gt; concatTable = ConcatTable()</span>
<span class="sd">    creating: createConcatTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConcatTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Identity"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Identity">[docs]</a><span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Identity just return the input to output.</span>
<span class="sd">    It&#39;s useful in same parallel container to get an origin input.</span>


<span class="sd">    &gt;&gt;&gt; identity = Identity()</span>
<span class="sd">    creating: createIdentity</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Identity</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Reverse"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Reverse">[docs]</a><span class="k">class</span> <span class="nc">Reverse</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Reverse the input w.r.t given dimension.</span>
<span class="sd">    The input can be a Tensor or Table.</span>


<span class="sd">    :param dim:</span>


<span class="sd">    &gt;&gt;&gt; reverse = Reverse()</span>
<span class="sd">    creating: createReverse</span>
<span class="sd">    &gt;&gt;&gt; reverse = Reverse(1, False)</span>
<span class="sd">    creating: createReverse</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">is_inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Reverse</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">dimension</span><span class="p">,</span>
                                      <span class="n">is_inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Transpose"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Transpose">[docs]</a><span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Transpose input along specified dimensions</span>


<span class="sd">    :param permutations: dimension pairs that need to swap</span>


<span class="sd">    &gt;&gt;&gt; transpose = Transpose([(1,2)])</span>
<span class="sd">    creating: createTranspose</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">permutations</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transpose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">permutations</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialContrastiveNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialContrastiveNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialContrastiveNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Subtractive + divisive contrast normalization.</span>


<span class="sd">    :param n_input_plane:</span>
<span class="sd">    :param kernel:</span>
<span class="sd">    :param threshold:</span>
<span class="sd">    :param thresval:</span>


<span class="sd">    &gt;&gt;&gt; kernel = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialContrastiveNormalization = SpatialContrastiveNormalization(1, kernel)</span>
<span class="sd">    creating: createSpatialContrastiveNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialContrastiveNormalization = SpatialContrastiveNormalization()</span>
<span class="sd">    creating: createSpatialContrastiveNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">thresval</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialContrastiveNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                              <span class="n">n_input_plane</span><span class="p">,</span>
                                                              <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
                                                              <span class="n">threshold</span><span class="p">,</span>
                                                              <span class="n">thresval</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialConvolutionMap"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialConvolutionMap">[docs]</a><span class="k">class</span> <span class="nc">SpatialConvolutionMap</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is a generalization of SpatialConvolution.</span>
<span class="sd">    It uses a generic connection table between input and output features.</span>
<span class="sd">    The SpatialConvolution is equivalent to using a full connection table.</span>
<span class="sd">    </span>
<span class="sd">    When padW and padH are both -1, we use a padding algorithm similar to the &quot;SAME&quot;</span>
<span class="sd">    padding of tensorflow. That is</span>
<span class="sd"> </span>
<span class="sd">     outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)</span>
<span class="sd">     outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)</span>
<span class="sd"> </span>
<span class="sd">     padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)</span>
<span class="sd">     padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)</span>
<span class="sd"> </span>
<span class="sd">     padTop = padAlongHeight / 2</span>
<span class="sd">     padLeft = padAlongWidth / 2</span>

<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; ct = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolutionMap = SpatialConvolutionMap(ct, 9, 9)</span>
<span class="sd">    creating: createSpatialConvolutionMap</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">conn_table</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialConvolutionMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">conn_table</span><span class="p">),</span>
                                                    <span class="n">kw</span><span class="p">,</span>
                                                    <span class="n">kh</span><span class="p">,</span>
                                                    <span class="n">dw</span><span class="p">,</span>
                                                    <span class="n">dh</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">,</span>
                                                    <span class="n">wRegularizer</span><span class="p">,</span>
                                                    <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialDivisiveNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDivisiveNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialDivisiveNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a spatial division operation on a series of 2D inputs using kernel for</span>
<span class="sd">    computing the weighted average in a neighborhood. The neighborhood is defined for</span>
<span class="sd">    a local spatial region that is the size as kernel and across all features. For</span>
<span class="sd">    an input image, since there is only one feature, the region is only spatial. For</span>
<span class="sd">    an RGB image, the weighted average is taken over RGB channels and a spatial region.</span>


<span class="sd">    If the kernel is 1D, then it will be used for constructing and separable 2D kernel.</span>
<span class="sd">    The operations will be much more efficient in this case.</span>


<span class="sd">    The kernel is generally chosen as a gaussian when it is believed that the correlation</span>
<span class="sd">    of two pixel locations decrease with increasing distance. On the feature dimension,</span>
<span class="sd">    a uniform average is used since the weighting across features is not known.</span>




<span class="sd">    :param nInputPlane: number of input plane, default is 1.</span>
<span class="sd">    :param kernel: kernel tensor, default is a 9 x 9 tensor.</span>
<span class="sd">    :param threshold: threshold</span>
<span class="sd">    :param thresval: threshhold value to replace withif data is smaller than theshold</span>


<span class="sd">    &gt;&gt;&gt; kernel = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialDivisiveNormalization = SpatialDivisiveNormalization(2,kernel)</span>
<span class="sd">    creating: createSpatialDivisiveNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialDivisiveNormalization = SpatialDivisiveNormalization()</span>
<span class="sd">    creating: createSpatialDivisiveNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">thresval</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDivisiveNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                           <span class="n">n_input_plane</span><span class="p">,</span>
                                                           <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
                                                           <span class="n">threshold</span><span class="p">,</span>
                                                           <span class="n">thresval</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialSubtractiveNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialSubtractiveNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialSubtractiveNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a spatial subtraction operation on a series of 2D inputs using kernel for</span>
<span class="sd">    computing the weighted average in a neighborhood. The neighborhood is defined for</span>
<span class="sd">    a local spatial region that is the size as kernel and across all features. For a</span>
<span class="sd">    an input image, since there is only one feature, the region is only spatial. For</span>
<span class="sd">    an RGB image, the weighted average is taken over RGB channels and a spatial region.</span>


<span class="sd">    If the kernel is 1D, then it will be used for constructing and separable 2D kernel.</span>
<span class="sd">    The operations will be much more efficient in this case.</span>


<span class="sd">    The kernel is generally chosen as a gaussian when it is believed that the correlation</span>
<span class="sd">    of two pixel locations decrease with increasing distance. On the feature dimension,</span>
<span class="sd">    a uniform average is used since the weighting across features is not known.</span>


<span class="sd">    :param n_input_plane: number of input plane, default is 1.</span>
<span class="sd">    :param kernel: kernel tensor, default is a 9 x 9 tensor.</span>


<span class="sd">    &gt;&gt;&gt; kernel = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialSubtractiveNormalization = SpatialSubtractiveNormalization(2,kernel)</span>
<span class="sd">    creating: createSpatialSubtractiveNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialSubtractiveNormalization = SpatialSubtractiveNormalization()</span>
<span class="sd">    creating: createSpatialSubtractiveNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialSubtractiveNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                              <span class="n">n_input_plane</span><span class="p">,</span>
                                                              <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">))</span></div>


<div class="viewcode-block" id="SpatialWithinChannelLRN"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialWithinChannelLRN">[docs]</a><span class="k">class</span> <span class="nc">SpatialWithinChannelLRN</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The local response normalization layer performs a kind of lateral inhibition</span>
<span class="sd">    by normalizing over local input regions. the local regions extend spatially,</span>
<span class="sd">    in separate channels (i.e., they have shape 1 x local_size x local_size).</span>

<span class="sd">    :param size  the side length of the square region to sum over</span>
<span class="sd">    :param alpha the scaling parameter</span>
<span class="sd">    :param beta the exponent</span>


<span class="sd">    &gt;&gt;&gt; layer = SpatialWithinChannelLRN()</span>
<span class="sd">    creating: createSpatialWithinChannelLRN</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialWithinChannelLRN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                      <span class="n">size</span><span class="p">,</span>
                                                      <span class="n">alpha</span><span class="p">,</span>
                                                      <span class="n">beta</span><span class="p">)</span></div>

<div class="viewcode-block" id="Pack"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Pack">[docs]</a><span class="k">class</span> <span class="nc">Pack</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Stacks a list of n-dimensional tensors into one (n+1)-dimensional tensor.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; layer = Pack(1)</span>
<span class="sd">    creating: createPack</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimension</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Pack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span></div>

<div class="viewcode-block" id="ConvLSTMPeephole"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ConvLSTMPeephole">[docs]</a><span class="k">class</span> <span class="nc">ConvLSTMPeephole</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    </span>
<span class="sd">|   Convolution Long Short Term Memory architecture with peephole.</span>
<span class="sd">|   Ref. A.: https://arxiv.org/abs/1506.04214 (blueprint for this module)</span>
<span class="sd">|   B. https://github.com/viorik/ConvLSTM</span>

<span class="sd">    :param input_size: number of input planes in the image given into forward()</span>
<span class="sd">    :param output_size: number of output planes the convolution layer will produce</span>
<span class="sd">    :param kernel_i: Convolutional filter size to convolve input</span>
<span class="sd">    :param kernel_c: Convolutional filter size to convolve cell</span>
<span class="sd">    :param stride: The step of the convolution, default is 1</span>
<span class="sd">    :param padding: The additional zeros added, default is -1</span>
<span class="sd">    :param activation: activation function, by default to be Tanh if not specified.</span>
<span class="sd">                        It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param inner_activation: activation function for the inner cells, by default to be Sigmoid if not specified.</span>
<span class="sd">                            It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param cRegularizer: instance of [[Regularizer]]applied to peephole.</span>
<span class="sd">    :param with_peephole: whether use last cell status control a gate.</span>

<span class="sd">    &gt;&gt;&gt; convlstm = ConvLSTMPeephole(4, 3, 3, 3, 1, -1, Tanh(), HardSigmoid(), L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    creating: createHardSigmoid</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createConvLSTMPeephole</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_i</span><span class="p">,</span> <span class="n">kernel_c</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inner_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">with_peephole</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inner_activation</span><span class="p">:</span>
            <span class="n">inner_activation</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">inner_activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">inner_activation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvLSTMPeephole</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_i</span><span class="p">,</span> <span class="n">kernel_c</span><span class="p">,</span>
                                               <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">inner_activation</span><span class="p">,</span>
                                               <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">,</span> <span class="n">cRegularizer</span><span class="p">,</span> <span class="n">with_peephole</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tile"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Tile">[docs]</a><span class="k">class</span> <span class="nc">Tile</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Replicate &#39;copies&#39; copy along &#39;dim&#39; dimension</span>

<span class="sd">    &gt;&gt;&gt; layer = Tile(1, 2)</span>
<span class="sd">    creating: createTile</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">copies</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Tile</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">copies</span><span class="p">)</span></div>

<div class="viewcode-block" id="BinaryThreshold"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BinaryThreshold">[docs]</a><span class="k">class</span> <span class="nc">BinaryThreshold</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Binary threshold, 1 if value &gt; th, 0 otherwise</span>
<span class="sd">    &gt;&gt;&gt; layer = BinaryThreshold(0.1, False)</span>
<span class="sd">    creating: createBinaryThreshold</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">th</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">ip</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BinaryThreshold</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">th</span><span class="p">,</span> <span class="n">ip</span><span class="p">)</span></div>

<div class="viewcode-block" id="ConvLSTMPeephole3D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ConvLSTMPeephole3D">[docs]</a><span class="k">class</span> <span class="nc">ConvLSTMPeephole3D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>

<span class="sd">    :param input_size: number of input planes in the image given into forward()</span>
<span class="sd">    :param output_size: number of output planes the convolution layer will produce</span>
<span class="sd">    :param kernel_i Convolutional filter size to convolve input</span>
<span class="sd">    :param kernel_c Convolutional filter size to convolve cell</span>
<span class="sd">    :param stride The step of the convolution</span>
<span class="sd">    :param padding The additional zeros added</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param cRegularizer: instance of [[Regularizer]]applied to peephole.</span>
<span class="sd">    :param with_peephole: whether use last cell status control a gate.</span>

<span class="sd">    &gt;&gt;&gt; convlstm = ConvLSTMPeephole3D(4, 3, 3, 3, 1, -1, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createConvLSTMPeephole3D</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_i</span><span class="p">,</span> <span class="n">kernel_c</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_peephole</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvLSTMPeephole3D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_i</span><span class="p">,</span> <span class="n">kernel_c</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                                                 <span class="n">padding</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">,</span> <span class="n">cRegularizer</span><span class="p">,</span> <span class="n">with_peephole</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiRNNCell"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MultiRNNCell">[docs]</a><span class="k">class</span> <span class="nc">MultiRNNCell</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A cell that enables stack multiple simple rnn cells</span>

<span class="sd">    &gt;&gt;&gt; cells = []</span>
<span class="sd">    &gt;&gt;&gt; cells.append(ConvLSTMPeephole3D(4, 3, 3, 3, 1))</span>
<span class="sd">    creating: createConvLSTMPeephole3D</span>
<span class="sd">    &gt;&gt;&gt; cells.append(ConvLSTMPeephole3D(4, 3, 3, 3, 1))</span>
<span class="sd">    creating: createConvLSTMPeephole3D</span>
<span class="sd">    &gt;&gt;&gt; stacked_convlstm = MultiRNNCell(cells)</span>
<span class="sd">    creating: createMultiRNNCell</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cells</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiRNNCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">cells</span><span class="p">)</span></div>

<div class="viewcode-block" id="ResizeBilinear"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ResizeBilinear">[docs]</a><span class="k">class</span> <span class="nc">ResizeBilinear</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resize the input image with bilinear interpolation. The input image must be a float tensor with</span>
<span class="sd">    NHWC or NCHW layout</span>

<span class="sd">    :param output_height: output height</span>
<span class="sd">    :param output_width: output width</span>
<span class="sd">    :param align_corner: align corner or not</span>
<span class="sd">    :param data_format: the data format of the input image, NHWC or NCHW</span>

<span class="sd">    &gt;&gt;&gt; resizeBilinear = ResizeBilinear(10, 20, False, &quot;NCHW&quot;)</span>
<span class="sd">    creating: createResizeBilinear</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">,</span> <span class="n">align_corner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResizeBilinear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span>
                                             <span class="n">output_width</span><span class="p">,</span> <span class="n">align_corner</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>

<div class="viewcode-block" id="GaussianSampler"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GaussianSampler">[docs]</a><span class="k">class</span> <span class="nc">GaussianSampler</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes {mean, log_variance} as input and samples from the Gaussian distribution</span>
<span class="sd">    &gt;&gt;&gt; sampler = GaussianSampler()</span>
<span class="sd">    creating: createGaussianSampler</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianSampler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="Masking"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Masking">[docs]</a><span class="k">class</span> <span class="nc">Masking</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Use a mask value to skip timesteps for a sequence</span>
<span class="sd">    ```</span>
<span class="sd">   :param mask_value: mask value</span>

<span class="sd">    &gt;&gt;&gt; masking = Masking(0.0)</span>
<span class="sd">    creating: createMasking</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">mask_value</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Masking</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">mask_value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Maxout"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Maxout">[docs]</a><span class="k">class</span> <span class="nc">Maxout</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A linear maxout layer Maxout layer select the element-wise maximum value of</span>
<span class="sd">    maxoutNumber Linear(inputSize, outputSize) layers</span>
<span class="sd">    ```    </span>
<span class="sd">    :param input_size: the size the each input sample</span>
<span class="sd">    :param output_size: the size of the module output of each sample</span>
<span class="sd">    :param maxout_number: number of Linear layers to use</span>
<span class="sd">    :param with_bias: whether use bias in Linear</span>
<span class="sd">    :param w_regularizer: instance of [[Regularizer]]</span>
<span class="sd">          (eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param b_regularizer: instance of [[Regularizer]]</span>
<span class="sd">           applied to the bias.</span>
<span class="sd">    :param init_weight: initial weight</span>
<span class="sd">    :param init_bias: initial bias</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; maxout = Maxout(2, 5, 3)</span>
<span class="sd">    creating: createMaxout</span>
<span class="sd">    &#39;&#39;&#39;</span>    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">maxout_number</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">w_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">b_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Maxout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">maxout_number</span><span class="p">,</span> <span class="n">with_bias</span><span class="p">,</span>
                                      <span class="n">w_regularizer</span><span class="p">,</span> <span class="n">b_regularizer</span><span class="p">,</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">)</span></div>

<div class="viewcode-block" id="HardSigmoid"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.HardSigmoid">[docs]</a><span class="k">class</span> <span class="nc">HardSigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply Hard-sigmoid function</span>
<span class="sd">```   </span>
<span class="sd">               |  0, if x &lt; -2.5</span>
<span class="sd">        f(x) = |  1, if x &gt; 2.5</span>
<span class="sd">               |  0.2 * x + 0.5, otherwise</span>
<span class="sd">```</span>
<span class="sd">    &gt;&gt;&gt; hardSigmoid = HardSigmoid()</span>
<span class="sd">    creating: createHardSigmoid</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HardSigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="Highway"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Highway">[docs]</a><span class="k">class</span> <span class="nc">Highway</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Densely connected highway network.</span>
<span class="sd">    Highway layers are a natural extension of LSTMs to feedforward networks.</span>

<span class="sd">    :param size input size</span>
<span class="sd">    :param with_bias whether to include a bias</span>
<span class="sd">    :param activation activation function. It can also be the name of an existing activation as a string.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]], applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; highway = Highway(2)</span>
<span class="sd">    creating: createHighway</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation_by_name</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Highway</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">with_bias</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>

<div class="viewcode-block" id="UpSampling3D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.UpSampling3D">[docs]</a><span class="k">class</span> <span class="nc">UpSampling3D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Upsampling layer for 3D inputs.</span>
<span class="sd">    Repeats the 1st, 2nd and 3rd dimensions</span>
<span class="sd">    of the data by size[0], size[1] and size[2] respectively.</span>
<span class="sd">    The input data is assumed to be of the form `minibatch x channels x depth x height x width`.</span>

<span class="sd">    :param size Repeats the depth, height, width dimensions of the data by</span>
<span class="sd">    &gt;&gt;&gt; upsample3d = UpSampling3D([1, 2, 3])</span>
<span class="sd">    creating: createUpSampling3D</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UpSampling3D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>

<div class="viewcode-block" id="PriorBox"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PriorBox">[docs]</a><span class="k">class</span> <span class="nc">PriorBox</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate the prior boxes of designated sizes and aspect ratios across</span>
<span class="sd">    all dimensions (H * W)</span>
<span class="sd">    Intended for use with MultiBox detection method to generate prior</span>
<span class="sd">    :param min_sizes minimum box size in pixels. can be multiple. required!</span>
<span class="sd">    :param max_sizes maximum box size in pixels. can be ignored or same as the # of min_size.</span>
<span class="sd">    :param aspect_ratios optional aspect ratios of the boxes. can be multiple</span>
<span class="sd">    :param is_flip optional bool, default true. if set, flip the aspect ratio.</span>
<span class="sd">    :param is_clip whether to clip the prior&#39;s coordidate such that it is within [0, 1]</span>
<span class="sd">    &gt;&gt;&gt; layer = PriorBox([0.1])</span>
<span class="sd">    creating: createPriorBox</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_sizes</span><span class="p">,</span>
                 <span class="n">max_sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">aspect_ratios</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">is_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">is_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">variances</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">offset</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">img_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">img_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">img_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">step_h</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">step_w</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">step</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PriorBox</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">min_sizes</span><span class="p">,</span>
                                       <span class="n">max_sizes</span><span class="p">,</span>
                                       <span class="n">aspect_ratios</span><span class="p">,</span>
                                       <span class="n">is_flip</span><span class="p">,</span>
                                       <span class="n">is_clip</span><span class="p">,</span>
                                       <span class="n">variances</span><span class="p">,</span>
                                       <span class="n">offset</span><span class="p">,</span>
                                       <span class="n">img_h</span><span class="p">,</span>
                                       <span class="n">img_w</span><span class="p">,</span>
                                       <span class="n">img_size</span><span class="p">,</span>
                                       <span class="n">step_h</span><span class="p">,</span>
                                       <span class="n">step_w</span><span class="p">,</span>
                                       <span class="n">step</span><span class="p">)</span></div>

<div class="viewcode-block" id="NormalizeScale"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.NormalizeScale">[docs]</a><span class="k">class</span> <span class="nc">NormalizeScale</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NormalizeScale is conposed of normalize and scale, this is equal to caffe Normalize layer</span>
<span class="sd">    :param p L_p norm</span>
<span class="sd">    :param eps smoothing parameter</span>
<span class="sd">    :param scale scale parameter</span>
<span class="sd">    :param size size of scale input</span>
<span class="sd">    :param w_regularizer weight regularizer</span>
<span class="sd">    &gt;&gt;&gt; layer = NormalizeScale(2.0, scale = 20.0, size = [1, 5, 1, 1])</span>
<span class="sd">    creating: createNormalizeScale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">w_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NormalizeScale</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">w_regularizer</span><span class="p">)</span></div>

<div class="viewcode-block" id="Proposal"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Proposal">[docs]</a><span class="k">class</span> <span class="nc">Proposal</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs object detection proposals by applying estimated bounding-box</span>
<span class="sd">    transformations to a set of regular boxes (called &quot;anchors&quot;).</span>
<span class="sd">    rois: holds R regions of interest, each is a 5-tuple</span>
<span class="sd">    (n, x1, y1, x2, y2) specifying an image batch index n and a rectangle (x1, y1, x2, y2)</span>
<span class="sd">    scores: holds scores for R regions of interest</span>
<span class="sd">    &gt;&gt;&gt; layer = Proposal(1000, 200, [0.1, 0.2], [2.0, 3.0])</span>
<span class="sd">    creating: createProposal</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_nms_topn</span><span class="p">,</span> <span class="n">post_nms_topn</span><span class="p">,</span> <span class="n">ratios</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span>
                 <span class="n">rpn_pre_nms_topn_train</span><span class="o">=</span><span class="mi">12000</span><span class="p">,</span> <span class="n">rpn_post_nms_topn_train</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Proposal</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">pre_nms_topn</span><span class="p">,</span>
                                       <span class="n">post_nms_topn</span><span class="p">,</span>
                                       <span class="n">ratios</span><span class="p">,</span>
                                       <span class="n">scales</span><span class="p">,</span>
                                       <span class="n">rpn_pre_nms_topn_train</span><span class="p">,</span>
                                       <span class="n">rpn_post_nms_topn_train</span><span class="p">)</span></div>

<div class="viewcode-block" id="DetectionOutputSSD"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.DetectionOutputSSD">[docs]</a><span class="k">class</span> <span class="nc">DetectionOutputSSD</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Layer to Post-process SSD output</span>
<span class="sd">    :param n_classes number of classes</span>
<span class="sd">    :param share_location whether to share location, default is true</span>
<span class="sd">    :param bg_label background label</span>
<span class="sd">    :param nms_thresh nms threshold</span>
<span class="sd">    :param nms_topk nms topk</span>
<span class="sd">    :param keep_top_k result topk</span>
<span class="sd">    :param conf_thresh confidence threshold</span>
<span class="sd">    :param variance_encoded_in_target if variance is encoded in target,</span>
<span class="sd">    we simply need to retore the offset predictions,</span>
<span class="sd">    else if variance is encoded in bbox,</span>
<span class="sd">    we need to scale the offset accordingly.</span>
<span class="sd">    :param conf_post_process whether add some additional post process to confidence prediction</span>
<span class="sd">    &gt;&gt;&gt; layer = DetectionOutputSSD()</span>
<span class="sd">    creating: createDetectionOutputSSD</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">21</span><span class="p">,</span>
                 <span class="n">share_location</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bg_label</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">nms_thresh</span><span class="o">=</span><span class="mf">0.45</span><span class="p">,</span>
                 <span class="n">nms_topk</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                 <span class="n">keep_top_k</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">conf_thresh</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">variance_encoded_in_target</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">conf_post_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DetectionOutputSSD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                                                 <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_classes</span><span class="p">,</span>
                                                 <span class="n">share_location</span><span class="p">,</span>
                                                 <span class="n">bg_label</span><span class="p">,</span>
                                                 <span class="n">nms_thresh</span><span class="p">,</span>
                                                 <span class="n">nms_topk</span><span class="p">,</span>
                                                 <span class="n">keep_top_k</span><span class="p">,</span>
                                                 <span class="n">conf_thresh</span><span class="p">,</span>
                                                 <span class="n">variance_encoded_in_target</span><span class="p">,</span>
                                                 <span class="n">conf_post_process</span><span class="p">)</span></div>

<div class="viewcode-block" id="DetectionOutputFrcnn"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.DetectionOutputFrcnn">[docs]</a><span class="k">class</span> <span class="nc">DetectionOutputFrcnn</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Post process Faster-RCNN models</span>
<span class="sd">    :param nms_thresh nms threshold</span>
<span class="sd">    :param n_classes number of classes</span>
<span class="sd">    :param bbox_vote whether to vote for detections</span>
<span class="sd">    :param max_per_image limit max number of detections per image</span>
<span class="sd">    :param thresh score threshold</span>
<span class="sd">    &gt;&gt;&gt; layer = DetectionOutputFrcnn(21, True)</span>
<span class="sd">    creating: createDetectionOutputFrcnn</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">bbox_vote</span><span class="p">,</span> <span class="n">nms_thresh</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                 <span class="n">max_per_image</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DetectionOutputFrcnn</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">nms_thresh</span><span class="p">,</span>
                                                   <span class="n">n_classes</span><span class="p">,</span>
                                                   <span class="n">bbox_vote</span><span class="p">,</span>
                                                   <span class="n">max_per_image</span><span class="p">,</span>
                                                   <span class="n">thresh</span><span class="p">)</span></div>

<div class="viewcode-block" id="Cropping2D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Cropping2D">[docs]</a><span class="k">class</span> <span class="nc">Cropping2D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cropping layer for 2D input (e.g. picture).</span>
<span class="sd">    It crops along spatial dimensions, i.e. width and height.</span>

<span class="sd">    # Input shape</span>
<span class="sd">        4D tensor with shape:</span>
<span class="sd">        `(batchSize, channels, first_axis_to_crop, second_axis_to_crop)`</span>

<span class="sd">    # Output shape</span>
<span class="sd">        4D tensor with shape:</span>
<span class="sd">        `(batchSize, channels, first_cropped_axis, second_cropped_axis)`</span>

<span class="sd">    :param heightCrop Array of length 2. How many units should be trimmed off at the beginning</span>
<span class="sd">                      and end of the height dimension.</span>
<span class="sd">    :param widthCrop Array of length 2. How many units should be trimmed off at the beginning</span>
<span class="sd">                      and end of the width dimension.</span>
<span class="sd">    :param data_format a string value (or DataFormat Object in Scala) of &quot;NHWC&quot; or &quot;NCHW&quot; to specify the input data format of this layer. In &quot;NHWC&quot; format</span>
<span class="sd">                        data is stored in the order of [batch_size, height, width, channels], in &quot;NCHW&quot; format data is stored</span>
<span class="sd">                        in the order of [batch_size, channels, height, width].</span>
<span class="sd">    &gt;&gt;&gt; cropping2D = Cropping2D([1, 1], [2, 2])</span>
<span class="sd">    creating: createCropping2D</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heightCrop</span><span class="p">,</span> <span class="n">widthCrop</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Cropping2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">heightCrop</span><span class="p">,</span> <span class="n">widthCrop</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>

<div class="viewcode-block" id="Cropping3D"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Cropping3D">[docs]</a><span class="k">class</span> <span class="nc">Cropping3D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cropping layer for 3D data (e.g. spatial or spatio-temporal).</span>

<span class="sd">    # Input shape</span>
<span class="sd">        5D tensor with shape:</span>
<span class="sd">        `(batchSize, channels, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)`</span>

<span class="sd">    # Output shape</span>
<span class="sd">        5D tensor with shape:</span>
<span class="sd">        `(batchSize, channels, first_cropped_axis, second_cropped_axis, third_cropped_axis)`</span>

<span class="sd">    :param dim1Crop Array of length 2. How many units should be trimmed off at the beginning</span>
<span class="sd">                      and end of the first dimension.</span>
<span class="sd">    :param dim2Crop Array of length 2. How many units should be trimmed off at the beginning</span>
<span class="sd">                      and end of the second dimension.</span>
<span class="sd">    :param dim3Crop Array of length 2. How many units should be trimmed off at the beginning</span>
<span class="sd">                      and end of the third dimension.</span>
<span class="sd">    :param data_format a string value. &quot;channel_first&quot; or &quot;channel_last&quot;</span>
<span class="sd">    &gt;&gt;&gt; cropping3D = Cropping3D([1, 1], [2, 2], [1, 1])</span>
<span class="sd">    creating: createCropping3D</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim1Crop</span><span class="p">,</span> <span class="n">dim2Crop</span><span class="p">,</span> <span class="n">dim3Crop</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;channel_first&quot;</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Cropping3D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dim1Crop</span><span class="p">,</span> <span class="n">dim2Crop</span><span class="p">,</span> <span class="n">dim3Crop</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">SparkContext</span>
    <span class="kn">from</span> <span class="nn">bigdl.nn</span> <span class="k">import</span> <span class="n">layer</span>
    <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">init_engine</span>
    <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">create_spark_conf</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="s2">&quot;local[4]&quot;</span><span class="p">,</span> <span class="n">appName</span><span class="o">=</span><span class="s2">&quot;test layer&quot;</span><span class="p">,</span>
                      <span class="n">conf</span><span class="o">=</span><span class="n">create_spark_conf</span><span class="p">())</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span>
    <span class="n">init_engine</span><span class="p">()</span>

    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span><span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
                                                  <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">BigDL  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, Intel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.6.
    </div>
  </body>
</html>