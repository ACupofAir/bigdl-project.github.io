
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>bigdl.optim.optimizer &#8212; BigDL  documentation</title>
    <link rel="stylesheet" href="../../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">BigDL  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for bigdl.optim.optimizer</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Copyright 2016 The BigDL Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>


<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">distutils.dir_util</span> <span class="k">import</span> <span class="n">mkpath</span>

<span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="k">import</span> <span class="n">JavaObject</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="k">import</span> <span class="n">RDD</span>

<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">DOUBLEMAX</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">JTensor</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">JavaValue</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">callBigDlFunc</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">callJavaFunc</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">get_spark_context</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">to_list</span>
<span class="kn">from</span> <span class="nn">bigdl.dataset.dataset</span> <span class="k">import</span> <span class="o">*</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">long</span> <span class="o">=</span> <span class="nb">int</span>
    <span class="n">unicode</span> <span class="o">=</span> <span class="nb">str</span>


<div class="viewcode-block" id="Top1Accuracy"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Top1Accuracy">[docs]</a><span class="k">class</span> <span class="nc">Top1Accuracy</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Caculate the percentage that output&#39;s max probability index equals target.</span>

<span class="sd">    &gt;&gt;&gt; top1 = Top1Accuracy()</span>
<span class="sd">    creating: createTop1Accuracy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="TreeNNAccuracy"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.TreeNNAccuracy">[docs]</a><span class="k">class</span> <span class="nc">TreeNNAccuracy</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Caculate the percentage that output&#39;s max probability index equals target.</span>

<span class="sd">    &gt;&gt;&gt; top1 = TreeNNAccuracy()</span>
<span class="sd">    creating: createTreeNNAccuracy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="Top5Accuracy"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Top5Accuracy">[docs]</a><span class="k">class</span> <span class="nc">Top5Accuracy</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Caculate the percentage that output&#39;s max probability index equals target.</span>

<span class="sd">    &gt;&gt;&gt; top5 = Top5Accuracy()</span>
<span class="sd">    creating: createTop5Accuracy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Loss"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Loss">[docs]</a><span class="k">class</span> <span class="nc">Loss</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This evaluation method is calculate loss of output with respect to target</span>
<span class="sd">    &gt;&gt;&gt; from bigdl.nn.criterion import ClassNLLCriterion</span>
<span class="sd">    &gt;&gt;&gt; loss = Loss()</span>
<span class="sd">    creating: createClassNLLCriterion</span>
<span class="sd">    creating: createLoss</span>

<span class="sd">    &gt;&gt;&gt; loss = Loss(ClassNLLCriterion())</span>
<span class="sd">    creating: createClassNLLCriterion</span>
<span class="sd">    creating: createLoss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cri</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.nn.criterion</span> <span class="k">import</span> <span class="n">ClassNLLCriterion</span>
        <span class="k">if</span> <span class="n">cri</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cri</span> <span class="o">=</span> <span class="n">ClassNLLCriterion</span><span class="p">()</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">cri</span><span class="p">)</span></div>

<div class="viewcode-block" id="MAE"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.MAE">[docs]</a><span class="k">class</span> <span class="nc">MAE</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This evaluation method calculates the mean absolute error of output with respect to target.</span>

<span class="sd">    &gt;&gt;&gt; mae = MAE()</span>
<span class="sd">    creating: createMAE</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="MaxIteration"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.MaxIteration">[docs]</a><span class="k">class</span> <span class="nc">MaxIteration</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    MaxIteration is a trigger that triggers an action when training reaches</span>
<span class="sd">    the number of iterations specified by &quot;max&quot;.</span>
<span class="sd">    Usually used as end_trigger when creating an Optimizer.</span>


<span class="sd">    &gt;&gt;&gt; maxIteration = MaxIteration(20)</span>
<span class="sd">    creating: createMaxIteration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a MaxIteration trigger.</span>


<span class="sd">        :param max: max</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxEpoch"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.MaxEpoch">[docs]</a><span class="k">class</span> <span class="nc">MaxEpoch</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    MaxEpoch is a trigger that triggers an action when training reaches</span>
<span class="sd">    the number of epochs specified by &quot;max_epoch&quot;.</span>
<span class="sd">    Usually used as end_trigger when creating an Optimizer.</span>


<span class="sd">    &gt;&gt;&gt; maxEpoch = MaxEpoch(2)</span>
<span class="sd">    creating: createMaxEpoch</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a MaxEpoch trigger.</span>


<span class="sd">        :param max_epoch: max_epoch</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">)</span></div>


<div class="viewcode-block" id="EveryEpoch"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.EveryEpoch">[docs]</a><span class="k">class</span> <span class="nc">EveryEpoch</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    EveryEpoch is a trigger that triggers an action when each epoch finishs.</span>
<span class="sd">    Could be used as trigger in setvalidation and setcheckpoint in Optimizer,</span>
<span class="sd">    and also in TrainSummary.set_summary_trigger.</span>


<span class="sd">    &gt;&gt;&gt; everyEpoch = EveryEpoch()</span>
<span class="sd">    creating: createEveryEpoch</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a EveryEpoch trigger.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SeveralIteration"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.SeveralIteration">[docs]</a><span class="k">class</span> <span class="nc">SeveralIteration</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    SeveralIteration is a trigger that triggers an action every &quot;n&quot;</span>
<span class="sd">    iterations.</span>
<span class="sd">    Could be used as trigger in setvalidation and setcheckpoint in Optimizer,</span>
<span class="sd">    and also in TrainSummary.set_summary_trigger.</span>


<span class="sd">    &gt;&gt;&gt; serveralIteration = SeveralIteration(2)</span>
<span class="sd">    creating: createSeveralIteration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interval</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a SeveralIteration trigger.</span>


<span class="sd">        :param interval: interval is the &quot;n&quot; where an action is triggeredevery &quot;n&quot; iterations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">interval</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxScore"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.MaxScore">[docs]</a><span class="k">class</span> <span class="nc">MaxScore</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger that triggers an action when validation score larger than &quot;max&quot; score</span>


<span class="sd">    &gt;&gt;&gt; maxScore = MaxScore(0.4)</span>
<span class="sd">    creating: createMaxScore</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a MaxScore trigger.</span>


<span class="sd">        :param max: max score</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="MinLoss"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.MinLoss">[docs]</a><span class="k">class</span> <span class="nc">MinLoss</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger that triggers an action when training loss less than &quot;min&quot; loss</span>


<span class="sd">    &gt;&gt;&gt; minLoss = MinLoss(0.1)</span>
<span class="sd">    creating: createMinLoss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a MinLoss trigger.</span>


<span class="sd">        :param min: min loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="nb">min</span><span class="p">)</span></div>


<div class="viewcode-block" id="Poly"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Poly">[docs]</a><span class="k">class</span> <span class="nc">Poly</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A learning rate decay policy, where the effective learning rate</span>
<span class="sd">    follows a polynomial decay, to be zero by the max_iteration.</span>
<span class="sd">    Calculation: base_lr (1 - iter/max_iteration) ^ (power)</span>


<span class="sd">    :param power: coeffient of decay, refer to calculation formula</span>
<span class="sd">    :param max_iteration: max iteration when lr becomes zero</span>

<span class="sd">    &gt;&gt;&gt; poly = Poly(0.5, 2)</span>
<span class="sd">    creating: createPoly</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">power</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
            <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">power</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">)</span></div>


<div class="viewcode-block" id="Exponential"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Exponential">[docs]</a><span class="k">class</span> <span class="nc">Exponential</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    [[Exponential]] is a learning rate schedule, which rescale the learning rate by</span>
<span class="sd">    lr_{n + 1} = lr * decayRate `^` (iter / decayStep)</span>
<span class="sd">    :param decay_step the inteval for lr decay</span>
<span class="sd">    :param decay_rate decay rate</span>
<span class="sd">    :param stair_case if true, iter / decayStep is an integer division</span>
<span class="sd">                     and the decayed learning rate follows a staircase function.</span>

<span class="sd">    &gt;&gt;&gt; exponential = Exponential(100, 0.1)</span>
<span class="sd">    creating: createExponential</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decay_step</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">stair_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">decay_step</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">stair_case</span><span class="p">)</span></div>


<div class="viewcode-block" id="Step"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Step">[docs]</a><span class="k">class</span> <span class="nc">Step</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A learning rate decay policy, where the effective learning rate is</span>
<span class="sd">    calculated as base_lr * gamma ^ (floor(iter / step_size))</span>


<span class="sd">    :param step_size:</span>
<span class="sd">    :param gamma:</span>


<span class="sd">    &gt;&gt;&gt; step = Step(2, 0.3)</span>
<span class="sd">    creating: createStep</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
            <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span></div>

<div class="viewcode-block" id="Default"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Default">[docs]</a><span class="k">class</span> <span class="nc">Default</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A learning rate decay policy, where the effective learning rate is</span>
<span class="sd">    calculated as base_lr * gamma ^ (floor(iter / step_size))</span>

<span class="sd">    :param step_size</span>
<span class="sd">    :param gamma</span>

<span class="sd">    &gt;&gt;&gt; step = Default()</span>
<span class="sd">    creating: createDefault</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Plateau"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Plateau">[docs]</a><span class="k">class</span> <span class="nc">Plateau</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plateau is the learning rate schedule when a metric has stopped improving.</span>
<span class="sd">    Models often benefit from reducing the learning rate by a factor of 2-10</span>
<span class="sd">    once learning stagnates. It monitors a quantity and if no improvement</span>
<span class="sd">    is seen for a &#39;patience&#39; number of epochs, the learning rate is reduced.</span>

<span class="sd">    :param monitor quantity to be monitored, can be Loss or score</span>
<span class="sd">    :param factor factor by which the learning rate will be reduced. new_lr = lr * factor</span>
<span class="sd">    :param patience number of epochs with no improvement after which learning rate will be reduced.</span>
<span class="sd">    :param mode one of {min, max}.</span>
<span class="sd">                In min mode, lr will be reduced when the quantity monitored has stopped decreasing;</span>
<span class="sd">                in max mode it will be reduced when the quantity monitored has stopped increasing</span>
<span class="sd">    :param epsilon threshold for measuring the new optimum, to only focus on significant changes.</span>
<span class="sd">    :param cooldown number of epochs to wait before resuming normal operation</span>
<span class="sd">                    after lr has been reduced.</span>
<span class="sd">    :param min_lr lower bound on the learning rate.</span>

<span class="sd">    &gt;&gt;&gt; plateau = Plateau(&quot;score&quot;)</span>
<span class="sd">    creating: createPlateau</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">monitor</span><span class="p">,</span>
                 <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">cooldown</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">monitor</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="n">patience</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span>
                           <span class="n">cooldown</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">)</span></div>

<div class="viewcode-block" id="Warmup"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Warmup">[docs]</a><span class="k">class</span> <span class="nc">Warmup</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A learning rate gradual increase policy, where the effective learning rate</span>
<span class="sd">    increase delta after each iteration.</span>
<span class="sd">    Calculation: base_lr + delta * iteration</span>

<span class="sd">    :param delta: increase amount after each iteration</span>

<span class="sd">    &gt;&gt;&gt; warmup = Warmup(0.05)</span>
<span class="sd">    creating: createWarmup</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span></div>

<div class="viewcode-block" id="SequentialSchedule"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.SequentialSchedule">[docs]</a><span class="k">class</span> <span class="nc">SequentialSchedule</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stack several learning rate schedulers.</span>

<span class="sd">    :param iterationPerEpoch: iteration numbers per epoch</span>

<span class="sd">    &gt;&gt;&gt; sequentialSchedule = SequentialSchedule(5)</span>
<span class="sd">    creating: createSequentialSchedule</span>
<span class="sd">    &gt;&gt;&gt; poly = Poly(0.5, 2)</span>
<span class="sd">    creating: createPoly</span>
<span class="sd">    &gt;&gt;&gt; test = sequentialSchedule.add(poly, 5)</span>



<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iteration_per_epoch</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">iteration_per_epoch</span><span class="p">)</span>

<div class="viewcode-block" id="SequentialSchedule.add"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.SequentialSchedule.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a learning rate scheduler to the contained `schedules`</span>

<span class="sd">        :param scheduler: learning rate scheduler to be add</span>
<span class="sd">        :param max_iteration: iteration numbers this scheduler will run</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;addScheduler&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">)</span></div></div>

<div class="viewcode-block" id="OptimMethod"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.OptimMethod">[docs]</a><span class="k">class</span> <span class="nc">OptimMethod</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">jvalue</span><span class="p">):</span>
            <span class="k">assert</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">jvalue</span><span class="p">)</span> <span class="o">==</span> <span class="n">JavaObject</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span>
                <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">JavaValue</span><span class="o">.</span><span class="n">jvm_class_constructor</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>

<div class="viewcode-block" id="OptimMethod.load"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.OptimMethod.load">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        load optim method</span>
<span class="sd">        :param path: file path</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadOptimMethod&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span></div>

<div class="viewcode-block" id="OptimMethod.save"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.OptimMethod.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">overWrite</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        save OptimMethod</span>
<span class="sd">        :param path      path</span>
<span class="sd">        :param overWrite whether to overwrite</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;saveOptimMethod&quot;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">overWrite</span><span class="p">)</span></div></div>

<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.SGD">[docs]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A plain implementation of SGD</span>

<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param weightdecay weight decay</span>
<span class="sd">    :param momentum momentum</span>
<span class="sd">    :param dampening dampening for momentum</span>
<span class="sd">    :param nesterov enables Nesterov momentum</span>
<span class="sd">    :param learningrates 1D tensor of individual learning rates</span>
<span class="sd">    :param weightdecays 1D tensor of individual weight decays</span>
<span class="sd">    &gt;&gt;&gt; sgd = SGD()</span>
<span class="sd">    creating: createDefault</span>
<span class="sd">    creating: createSGD</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">weightdecay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">dampening</span><span class="o">=</span><span class="n">DOUBLEMAX</span><span class="p">,</span>
                 <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">leaningrate_schedule</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">learningrates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">weightdecays</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SGD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">learningrate_decay</span><span class="p">,</span> <span class="n">weightdecay</span><span class="p">,</span>
                           <span class="n">momentum</span><span class="p">,</span> <span class="n">dampening</span><span class="p">,</span> <span class="n">nesterov</span><span class="p">,</span>
                           <span class="n">leaningrate_schedule</span> <span class="k">if</span> <span class="p">(</span><span class="n">leaningrate_schedule</span><span class="p">)</span> <span class="k">else</span> <span class="n">Default</span><span class="p">(),</span>
                           <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">learningrates</span><span class="p">),</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">weightdecays</span><span class="p">))</span></div>

<div class="viewcode-block" id="Adagrad"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Adagrad">[docs]</a><span class="k">class</span> <span class="nc">Adagrad</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adagrad. See the original paper:</span>
<span class="sd">    http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</span>

<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param weightdecay weight decay</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adagrad()</span>
<span class="sd">    creating: createAdagrad</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">weightdecay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Adagrad</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">learningrate_decay</span><span class="p">,</span> <span class="n">weightdecay</span><span class="p">)</span></div>

<div class="viewcode-block" id="LBFGS"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.LBFGS">[docs]</a><span class="k">class</span> <span class="nc">LBFGS</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This implementation of L-BFGS relies on a user-provided line</span>
<span class="sd">    search function (state.lineSearch). If this function is not</span>
<span class="sd">    provided, then a simple learningRate is used to produce fixed</span>
<span class="sd">    size steps. Fixed size steps are much less costly than line</span>
<span class="sd">    searches, and can be useful for stochastic problems.</span>
<span class="sd">    The learning rate is used even when a line search is provided.</span>
<span class="sd">    This is also useful for large-scale stochastic problems, where</span>
<span class="sd">    opfunc is a noisy approximation of f(x). In that case, the learning</span>
<span class="sd">    rate allows a reduction of confidence in the step size.</span>

<span class="sd">    :param max_iter Maximum number of iterations allowed</span>
<span class="sd">    :param max_eval Maximum number of function evaluations</span>
<span class="sd">    :param tolfun Termination tolerance on the first-order optimality</span>
<span class="sd">    :param tolx Termination tol on progress in terms of func/param changes</span>
<span class="sd">    :param ncorrection</span>
<span class="sd">    :param learningrate</span>
<span class="sd">    :param verbose</span>
<span class="sd">    :param linesearch A line search function</span>
<span class="sd">    :param linesearch_options If no line search provided, then a fixed step size is used</span>
<span class="sd">    &gt;&gt;&gt; lbfgs = LBFGS()</span>
<span class="sd">    creating: createLBFGS</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">max_eval</span><span class="o">=</span><span class="n">DOUBLEMAX</span><span class="p">,</span>
                 <span class="n">tolfun</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">tolx</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span>
                 <span class="n">ncorrection</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">linesearch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">linesearch_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">linesearch</span> <span class="ow">or</span> <span class="n">linesearch_options</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;linesearch and linesearch_options must be None in LBFGS&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LBFGS</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">max_eval</span><span class="p">,</span> <span class="n">tolfun</span><span class="p">,</span> <span class="n">tolx</span><span class="p">,</span>
                       <span class="n">ncorrection</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">linesearch</span><span class="p">,</span> <span class="n">linesearch_options</span><span class="p">)</span></div>

<div class="viewcode-block" id="Adadelta"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Adadelta">[docs]</a><span class="k">class</span> <span class="nc">Adadelta</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adadelta implementation for SGD: http://arxiv.org/abs/1212.5701</span>

<span class="sd">    :param decayrate interpolation parameter rho</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adadelta()</span>
<span class="sd">    creating: createAdadelta</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">decayrate</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Adadelta</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">decayrate</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span></div>

<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adam http://arxiv.org/pdf/1412.6980.pdf</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param beta1 first moment coefficient</span>
<span class="sd">    :param beta2 second moment coefficient</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adam()</span>
<span class="sd">    creating: createAdam</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
                 <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">learningrate_decay</span><span class="p">,</span>
                           <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span></div>

<div class="viewcode-block" id="Ftrl"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Ftrl">[docs]</a><span class="k">class</span> <span class="nc">Ftrl</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Ftrl https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf.</span>
<span class="sd">    Support L1 penalty, L2 penalty and shrinkage-type L2 penalty.</span>

<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_power double, must be less or equal to zero. Default is -0.5.</span>
<span class="sd">    :param initial_accumulator_value double, the starting value for accumulators,</span>
<span class="sd">        require zero or positive values.</span>
<span class="sd">    :param l1_regularization_strength double, must be greater or equal to zero. Default is zero.</span>
<span class="sd">    :param l2_regularization_strength double, must be greater or equal to zero. Default is zero.</span>
<span class="sd">    :param l2_shrinkage_regularization_strength double, must be greater or equal to zero.</span>
<span class="sd">        Default is zero. This differs from l2RegularizationStrength above. L2 above is a</span>
<span class="sd">        stabilization penalty, whereas this one is a magnitude penalty.</span>
<span class="sd">    &gt;&gt;&gt; ftrl = Ftrl()</span>
<span class="sd">    creating: createFtrl</span>
<span class="sd">    &gt;&gt;&gt; ftrl2 = Ftrl(1e-2, -0.1, 0.2, 0.3, 0.4, 0.5)</span>
<span class="sd">    creating: createFtrl</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_power</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">initial_accumulator_value</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">l1_regularization_strength</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">l2_regularization_strength</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">l2_shrinkage_regularization_strength</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Ftrl</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">learningrate_power</span><span class="p">,</span>
                                   <span class="n">initial_accumulator_value</span><span class="p">,</span>
                                   <span class="n">l1_regularization_strength</span><span class="p">,</span>
                                   <span class="n">l2_regularization_strength</span><span class="p">,</span>
                                   <span class="n">l2_shrinkage_regularization_strength</span><span class="p">)</span></div>

<div class="viewcode-block" id="Adamax"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Adamax">[docs]</a><span class="k">class</span> <span class="nc">Adamax</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param beta1 first moment coefficient</span>
<span class="sd">    :param beta2 second moment coefficient</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adamax()</span>
<span class="sd">    creating: createAdamax</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span> <span class="o">=</span> <span class="mf">0.002</span><span class="p">,</span>
                 <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
                 <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-38</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Adamax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span></div>

<div class="viewcode-block" id="RMSprop"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.RMSprop">[docs]</a><span class="k">class</span> <span class="nc">RMSprop</span><span class="p">(</span><span class="n">OptimMethod</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of RMSprop</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param decayrate decay rate, also called rho</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = RMSprop()</span>
<span class="sd">    creating: createRMSprop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">decayrate</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
                 <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RMSprop</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">learningrate</span><span class="p">,</span> <span class="n">learningrate_decay</span><span class="p">,</span> <span class="n">decayrate</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiStep"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.MultiStep">[docs]</a><span class="k">class</span> <span class="nc">MultiStep</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    similar to step but it allows non uniform steps defined by stepSizes</span>


<span class="sd">    :param step_size: the series of step sizes used for lr decay</span>
<span class="sd">    :param gamma: coefficient of decay</span>


<span class="sd">    &gt;&gt;&gt; step = MultiStep([2, 5], 0.3)</span>
<span class="sd">    creating: createMultiStep</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_sizes</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">step_sizes</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span></div>


<div class="viewcode-block" id="BaseOptimizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer">[docs]</a><span class="k">class</span> <span class="nc">BaseOptimizer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>

<div class="viewcode-block" id="BaseOptimizer.set_model"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_model">[docs]</a>    <span class="k">def</span> <span class="nf">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set model.</span>


<span class="sd">        :param model: new model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">setModel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_criterion"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_criterion">[docs]</a>    <span class="k">def</span> <span class="nf">set_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        set new criterion, for optimizer reuse</span>

<span class="sd">        :param criterion: new criterion</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setCriterion&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">criterion</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_checkpoint"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_checkpoint">[docs]</a>    <span class="k">def</span> <span class="nf">set_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_trigger</span><span class="p">,</span>
                       <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">isOverWrite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure checkpoint settings.</span>


<span class="sd">        :param checkpoint_trigger: the interval to write snapshots</span>
<span class="sd">        :param checkpoint_path: the path to write snapshots into</span>
<span class="sd">        :param isOverWrite: whether to overwrite existing snapshots in path.default is True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">):</span>
            <span class="n">mkpath</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setCheckPoint&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">checkpoint_trigger</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">isOverWrite</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_gradclip_const"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_gradclip_const">[docs]</a>    <span class="k">def</span> <span class="nf">set_gradclip_const</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure constant clipping settings.</span>


<span class="sd">        :param min_value: the minimum value to clip by</span>
<span class="sd">        :param max_value: the maxmimum value to clip by</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setConstantClip&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_gradclip_l2norm"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_gradclip_l2norm">[docs]</a>    <span class="k">def</span> <span class="nf">set_gradclip_l2norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure L2 norm clipping settings.</span>


<span class="sd">        :param clip_norm: gradient L2-Norm threshold</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setL2NormClip&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseOptimizer.disable_gradclip"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.disable_gradclip">[docs]</a>    <span class="k">def</span> <span class="nf">disable_gradclip</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        disable clipping.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;disableClip&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span></div>

    <span class="c1"># return a module</span>
<div class="viewcode-block" id="BaseOptimizer.optimize"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Do an optimization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callJavaFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">optimize</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">bigdl.nn.layer</span> <span class="k">import</span> <span class="n">Layer</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_train_summary"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_train_summary">[docs]</a>    <span class="k">def</span> <span class="nf">set_train_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set train summary. A TrainSummary object contains information</span>
<span class="sd">        necessary for the optimizer to know how often the logs are recorded,</span>
<span class="sd">        where to store the logs and how to retrieve them, etc. For details,</span>
<span class="sd">        refer to the docs of TrainSummary.</span>


<span class="sd">        :param summary: a TrainSummary object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setTrainSummary&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">summary</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_val_summary"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_val_summary">[docs]</a>    <span class="k">def</span> <span class="nf">set_val_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set validation summary. A ValidationSummary object contains information</span>
<span class="sd">        necessary for the optimizer to know how often the logs are recorded,</span>
<span class="sd">        where to store the logs and how to retrieve them, etc. For details,</span>
<span class="sd">        refer to the docs of ValidationSummary.</span>


<span class="sd">        :param summary: a ValidationSummary object</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setValSummary&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">summary</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="BaseOptimizer.prepare_input"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.prepare_input">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load input. Notebook user can call this method to seprate load data and</span>
<span class="sd">        create optimizer time</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading input ...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">prepareInput</span><span class="p">()</span></div>

<div class="viewcode-block" id="BaseOptimizer.set_end_when"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.BaseOptimizer.set_end_when">[docs]</a>    <span class="k">def</span> <span class="nf">set_end_when</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">end_when</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When to stop, passed in a [[Trigger]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">setEndWhen</span><span class="p">(</span><span class="n">end_when</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Optimizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Optimizer">[docs]</a><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="n">BaseOptimizer</span><span class="p">):</span>

    <span class="c1"># NOTE: This is a deprecated method, you should use `create` method instead.</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">,</span>
                 <span class="n">training_rdd</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">end_trigger</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">optim_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a distributed optimizer.</span>


<span class="sd">        :param model: the neural net model</span>
<span class="sd">        :param training_rdd: the training dataset</span>
<span class="sd">        :param criterion: the loss function</span>
<span class="sd">        :param optim_method: the algorithm to use for optimization,</span>
<span class="sd">           e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.</span>
<span class="sd">        :param end_trigger: when to end the optimization</span>
<span class="sd">        :param batch_size: training batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pvalue</span> <span class="o">=</span> <span class="n">DistriOptimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                      <span class="n">training_rdd</span><span class="p">,</span>
                                      <span class="n">criterion</span><span class="p">,</span>
                                      <span class="n">end_trigger</span><span class="p">,</span>
                                      <span class="n">batch_size</span><span class="p">,</span>
                                      <span class="n">optim_method</span><span class="p">,</span>
                                      <span class="n">bigdl_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pvalue</span><span class="o">.</span><span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pvalue</span><span class="o">.</span><span class="n">bigdl_type</span>

<div class="viewcode-block" id="Optimizer.create"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Optimizer.create">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
               <span class="n">training_set</span><span class="p">,</span>
               <span class="n">criterion</span><span class="p">,</span>
               <span class="n">end_trigger</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
               <span class="n">optim_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">cores</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create an optimizer.</span>
<span class="sd">        Depend on the input type, the returning optimizer can be a local optimizer \</span>
<span class="sd">        or a distributed optimizer.</span>

<span class="sd">        :param model: the neural net model</span>
<span class="sd">        :param training_set: (features, label) for local mode. RDD[Sample] for distributed mode.</span>
<span class="sd">        :param criterion: the loss function</span>
<span class="sd">        :param optim_method: the algorithm to use for optimization,</span>
<span class="sd">           e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.</span>
<span class="sd">        :param end_trigger: when to end the optimization. default value is MapEpoch(1)</span>
<span class="sd">        :param batch_size: training batch size</span>
<span class="sd">        :param cores: This is for local optimizer only and use total physical cores as the default value</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">end_trigger</span><span class="p">:</span>
            <span class="n">end_trigger</span> <span class="o">=</span> <span class="n">MaxEpoch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">optim_method</span><span class="p">:</span>
            <span class="n">optim_method</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="n">RDD</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="n">DataSet</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">DistriOptimizer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                   <span class="n">training_rdd</span><span class="o">=</span><span class="n">training_set</span><span class="p">,</span>
                                   <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
                                   <span class="n">end_trigger</span><span class="o">=</span><span class="n">end_trigger</span><span class="p">,</span>
                                   <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                   <span class="n">optim_method</span><span class="o">=</span><span class="n">optim_method</span><span class="p">,</span>
                                   <span class="n">bigdl_type</span><span class="o">=</span><span class="n">bigdl_type</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">training_set</span>
            <span class="k">return</span> <span class="n">LocalOptimizer</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                                  <span class="n">Y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                  <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                  <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
                                  <span class="n">end_trigger</span><span class="o">=</span><span class="n">end_trigger</span><span class="p">,</span>
                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                  <span class="n">optim_method</span><span class="o">=</span><span class="n">optim_method</span><span class="p">,</span>
                                  <span class="n">cores</span><span class="o">=</span><span class="n">cores</span><span class="p">,</span>
                                  <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Not supported training set: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">training_set</span><span class="p">))</span></div>

<div class="viewcode-block" id="Optimizer.set_validation"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Optimizer.set_validation">[docs]</a>    <span class="k">def</span> <span class="nf">set_validation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">val_rdd</span><span class="p">,</span> <span class="n">trigger</span><span class="p">,</span> <span class="n">val_method</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure validation settings.</span>


<span class="sd">        :param batch_size: validation batch size</span>
<span class="sd">        :param val_rdd: validation dataset</span>
<span class="sd">        :param trigger: validation interval</span>
<span class="sd">        :param val_method: the ValidationMethod to use,e.g. &quot;Top1Accuracy&quot;, &quot;Top5Accuracy&quot;, &quot;Loss&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">val_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">val_method</span> <span class="o">=</span> <span class="p">[</span><span class="n">Top1Accuracy</span><span class="p">()]</span>
        <span class="n">func_name</span> <span class="o">=</span> <span class="s2">&quot;setValidation&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_rdd</span><span class="p">,</span> <span class="n">DataSet</span><span class="p">):</span>
            <span class="n">func_name</span> <span class="o">=</span> <span class="s2">&quot;setValidationFromDataSet&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                      <span class="n">trigger</span><span class="p">,</span> <span class="n">val_rdd</span><span class="p">,</span> <span class="n">to_list</span><span class="p">(</span><span class="n">val_method</span><span class="p">))</span></div>

<div class="viewcode-block" id="Optimizer.set_traindata"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.Optimizer.set_traindata">[docs]</a>    <span class="k">def</span> <span class="nf">set_traindata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_rdd</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set new training dataset, for optimizer reuse</span>

<span class="sd">        :param training_rdd: the training dataset</span>
<span class="sd">        :param batch_size: training batch size</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setTrainData&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                     <span class="n">training_rdd</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span></div></div>



<div class="viewcode-block" id="DistriOptimizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.DistriOptimizer">[docs]</a><span class="k">class</span> <span class="nc">DistriOptimizer</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">,</span>
                 <span class="n">training_rdd</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">end_trigger</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">optim_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create an optimizer.</span>


<span class="sd">        :param model: the neural net model</span>
<span class="sd">        :param training_data: the training dataset</span>
<span class="sd">        :param criterion: the loss function</span>
<span class="sd">        :param optim_method: the algorithm to use for optimization,</span>
<span class="sd">           e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.</span>
<span class="sd">        :param end_trigger: when to end the optimization</span>
<span class="sd">        :param batch_size: training batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">optim_method</span><span class="p">:</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">():</span> <span class="n">SGD</span><span class="p">()}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_method</span><span class="p">,</span> <span class="n">OptimMethod</span><span class="p">):</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">():</span> <span class="n">optim_method</span><span class="p">}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_method</span><span class="p">,</span> <span class="n">JavaObject</span><span class="p">):</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">():</span> <span class="n">OptimMethod</span><span class="p">(</span><span class="n">optim_method</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="n">optim_method</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_rdd</span><span class="p">,</span> <span class="n">RDD</span><span class="p">):</span>
            <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="n">training_rdd</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                               <span class="n">optim_methods</span><span class="p">,</span> <span class="n">end_trigger</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_rdd</span><span class="p">,</span> <span class="n">DataSet</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;createDistriOptimizerFromDataSet&quot;</span><span class="p">,</span>
                                       <span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">training_rdd</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                                       <span class="n">optim_methods</span><span class="p">,</span> <span class="n">end_trigger</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="LocalOptimizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.LocalOptimizer">[docs]</a><span class="k">class</span> <span class="nc">LocalOptimizer</span><span class="p">(</span><span class="n">BaseOptimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create an optimizer.</span>


<span class="sd">    :param model: the neural net model</span>
<span class="sd">    :param X: the training features which is an ndarray or list of ndarray</span>
<span class="sd">    :param Y: the training label which is an ndarray</span>
<span class="sd">    :param criterion: the loss function</span>
<span class="sd">    :param optim_method: the algorithm to use for optimization,</span>
<span class="sd">       e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.</span>
<span class="sd">    :param end_trigger: when to end the optimization</span>
<span class="sd">    :param batch_size: training batch size</span>
<span class="sd">    :param cores: by default is the total physical cores.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">X</span><span class="p">,</span>
                 <span class="n">Y</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">end_trigger</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">optim_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">cores</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">optim_method</span><span class="p">:</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">():</span> <span class="n">SGD</span><span class="p">()}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_method</span><span class="p">,</span> <span class="n">OptimMethod</span><span class="p">):</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">():</span> <span class="n">optim_method</span><span class="p">}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_method</span><span class="p">,</span> <span class="n">JavaObject</span><span class="p">):</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="p">():</span> <span class="n">OptimMethod</span><span class="p">(</span><span class="n">optim_method</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optim_methods</span> <span class="o">=</span> <span class="n">optim_method</span>
        <span class="k">if</span> <span class="n">cores</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cores</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                           <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">to_list</span><span class="p">(</span><span class="n">X</span><span class="p">)],</span>
                           <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span>
                           <span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                           <span class="n">criterion</span><span class="p">,</span>
                           <span class="n">optim_methods</span><span class="p">,</span> <span class="n">end_trigger</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">cores</span><span class="p">)</span>

<div class="viewcode-block" id="LocalOptimizer.set_validation"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.LocalOptimizer.set_validation">[docs]</a>    <span class="k">def</span> <span class="nf">set_validation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">trigger</span><span class="p">,</span> <span class="n">val_method</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure validation settings.</span>

<span class="sd">        :param batch_size: validation batch size</span>
<span class="sd">        :param X_val: features of validation dataset</span>
<span class="sd">        :param Y_val: label of validation dataset</span>
<span class="sd">        :param trigger: validation interval</span>
<span class="sd">        :param val_method: the ValidationMethod to use,e.g. &quot;Top1Accuracy&quot;, &quot;Top5Accuracy&quot;, &quot;Loss&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">val_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">val_method</span> <span class="o">=</span> <span class="p">[</span><span class="n">Top1Accuracy</span><span class="p">()]</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setValidation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                      <span class="n">trigger</span><span class="p">,</span> <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">to_list</span><span class="p">(</span><span class="n">X_val</span><span class="p">)],</span>
                      <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">Y_val</span><span class="p">),</span> <span class="n">to_list</span><span class="p">(</span><span class="n">val_method</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="TrainSummary"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.TrainSummary">[docs]</a><span class="k">class</span> <span class="nc">TrainSummary</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">,</span> <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A logging facility which allows user to trace how indicators (e.g.</span>
<span class="sd">    learning rate, training loss, throughput, etc.) change with iterations/time</span>
<span class="sd">    in an optimization process. TrainSummary is for training indicators only</span>
<span class="sd">    (check ValidationSummary for validation indicators).  It contains necessary</span>
<span class="sd">    information for the optimizer to know where to store the logs, how to</span>
<span class="sd">    retrieve the logs, and so on. - The logs are written in tensorflow-compatible</span>
<span class="sd">    format so that they can be visualized directly using tensorboard. Also the</span>
<span class="sd">    logs can be retrieved as ndarrays and visualized using python libraries</span>
<span class="sd">    such as matplotlib (in notebook, etc.).</span>


<span class="sd">    Use optimizer.setTrainSummary to enable train logger.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a TrainSummary. Logs will be saved to log_dir/app_name/train.</span>


<span class="sd">        :param log_dir: the root dir to store the logs</span>
<span class="sd">        :param app_name: the application name</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">)</span>

<div class="viewcode-block" id="TrainSummary.read_scalar"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.TrainSummary.read_scalar">[docs]</a>    <span class="k">def</span> <span class="nf">read_scalar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tag</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve train logs by type. Return an array of records in the format</span>
<span class="sd">        (step,value,wallClockTime). - &quot;Step&quot; is the iteration count by default.</span>


<span class="sd">        :param tag: the type of the logs, Supported tags are: &quot;LearningRate&quot;,&quot;Loss&quot;, &quot;Throughput&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;summaryReadScalar&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="TrainSummary.set_summary_trigger"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.TrainSummary.set_summary_trigger">[docs]</a>    <span class="k">def</span> <span class="nf">set_summary_trigger</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trigger</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the interval of recording for each indicator.</span>


<span class="sd">        :param tag: tag name. Supported tag names are &quot;LearningRate&quot;, &quot;Loss&quot;,&quot;Throughput&quot;, &quot;Parameters&quot;. &quot;Parameters&quot; is an umbrella tag thatincludes weight, bias, gradWeight, gradBias, and some running status(eg. runningMean and runningVar in BatchNormalization). If youdidn&#39;t set any triggers, we will by default record Loss and Throughputin each iteration, while *NOT* recording LearningRate and Parameters,as recording parameters may introduce substantial overhead when themodel is very big, LearningRate is not a public attribute for allOptimMethod.</span>
<span class="sd">        :param trigger: trigger</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;summarySetTrigger&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">name</span><span class="p">,</span> <span class="n">trigger</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ValidationSummary"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.ValidationSummary">[docs]</a><span class="k">class</span> <span class="nc">ValidationSummary</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     A logging facility which allows user to trace how indicators (e.g.</span>
<span class="sd">     validation loss, top1 accuray, top5 accuracy etc.) change with</span>
<span class="sd">     iterations/time in an optimization process. ValidationSummary is for</span>
<span class="sd">     validation indicators only (check TrainSummary for train indicators).</span>
<span class="sd">     It contains necessary information for the optimizer to know where to</span>
<span class="sd">     store the logs, how to retrieve the logs, and so on. - The logs are</span>
<span class="sd">     written in tensorflow-compatible format so that they can be visualized</span>
<span class="sd">     directly using tensorboard. Also the logs can be retrieved as ndarrays</span>
<span class="sd">     and visualized using python libraries such as matplotlib</span>
<span class="sd">     (in notebook, etc.).</span>


<span class="sd">     Use optimizer.setValidationSummary to enable validation logger.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a ValidationSummary. Logs will be saved to</span>
<span class="sd">        log_dir/app_name/train. By default, all ValidationMethod set into</span>
<span class="sd">        optimizer will be recorded and the recording interval is the same</span>
<span class="sd">        as trigger of ValidationMethod in the optimizer.</span>


<span class="sd">        :param log_dir: the root dir to store the logs</span>
<span class="sd">        :param app_name: the application name</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">)</span>

<div class="viewcode-block" id="ValidationSummary.read_scalar"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.ValidationSummary.read_scalar">[docs]</a>    <span class="k">def</span> <span class="nf">read_scalar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tag</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve validation logs by type. Return an array of records in the</span>
<span class="sd">        format (step,value,wallClockTime). - &quot;Step&quot; is the iteration count</span>
<span class="sd">        by default.</span>


<span class="sd">        :param tag: the type of the logs. The tag should match the name ofthe ValidationMethod set into the optimizer. e.g.&quot;Top1AccuracyLoss&quot;,&quot;Top1Accuracy&quot; or &quot;Top5Accuracy&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;summaryReadScalar&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">tag</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="L1L2Regularizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.L1L2Regularizer">[docs]</a><span class="k">class</span> <span class="nc">L1L2Regularizer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply both L1 and L2 regularization</span>

<span class="sd">    :param l1 l1 regularization rate</span>
<span class="sd">    :param l2 l2 regularization rate</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span></div>

<div class="viewcode-block" id="ActivityRegularization"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.ActivityRegularization">[docs]</a><span class="k">class</span> <span class="nc">ActivityRegularization</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply both L1 and L2 regularization</span>

<span class="sd">    :param l1 l1 regularization rate</span>
<span class="sd">    :param l2 l2 regularization rate</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span></div>

<div class="viewcode-block" id="L1Regularizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.L1Regularizer">[docs]</a><span class="k">class</span> <span class="nc">L1Regularizer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply L1 regularization</span>

<span class="sd">    :param l1 l1 regularization rate</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Regularizer"><a class="viewcode-back" href="../../../bigdl.optim.html#bigdl.optim.optimizer.L2Regularizer">[docs]</a><span class="k">class</span> <span class="nc">L2Regularizer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply L2 regularization</span>

<span class="sd">    :param l2 l2 regularization rate</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">SparkContext</span>
    <span class="kn">from</span> <span class="nn">bigdl.optim</span> <span class="k">import</span> <span class="n">optimizer</span>
    <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">init_engine</span>
    <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">create_spark_conf</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="s2">&quot;local[4]&quot;</span><span class="p">,</span> <span class="n">appName</span><span class="o">=</span><span class="s2">&quot;test optimizer&quot;</span><span class="p">,</span>
                      <span class="n">conf</span><span class="o">=</span><span class="n">create_spark_conf</span><span class="p">())</span>
    <span class="n">init_engine</span><span class="p">()</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span><span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
                                                  <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">BigDL  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, Intel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.5.
    </div>
  </body>
</html>