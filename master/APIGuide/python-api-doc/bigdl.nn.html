
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>bigdl.nn package &#8212; BigDL  documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="bigdl.optim package" href="bigdl.optim.html" />
    <link rel="prev" title="bigdl.models.lenet package" href="bigdl.models.lenet.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="bigdl.optim.html" title="bigdl.optim package"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="bigdl.models.lenet.html" title="bigdl.models.lenet package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">BigDL  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="bigdl.html" accesskey="U">bigdl package</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">bigdl.nn package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-bigdl.nn.criterion">bigdl.nn.criterion module</a></li>
<li><a class="reference internal" href="#module-bigdl.nn.initialization_method">bigdl.nn.initialization_method module</a></li>
<li><a class="reference internal" href="#module-bigdl.nn.layer">bigdl.nn.layer module</a></li>
<li><a class="reference internal" href="#module-bigdl.nn">Module contents</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="bigdl.models.lenet.html"
                        title="previous chapter">bigdl.models.lenet package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="bigdl.optim.html"
                        title="next chapter">bigdl.optim package</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/bigdl.nn.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="bigdl-nn-package">
<h1>bigdl.nn package<a class="headerlink" href="#bigdl-nn-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-bigdl.nn.criterion">
<span id="bigdl-nn-criterion-module"></span><h2>bigdl.nn.criterion module<a class="headerlink" href="#module-bigdl.nn.criterion" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="bigdl.nn.criterion.AbsCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">AbsCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#AbsCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.AbsCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>measures the mean absolute value of the element-wise difference between input</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">absCriterion</span> <span class="o">=</span> <span class="n">AbsCriterion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createAbsCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.BCECriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">BCECriterion</code><span class="sig-paren">(</span><em>weights=None</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#BCECriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.BCECriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weights</strong> – weights for each class</li>
<li><strong>sizeAverage</strong> – whether to average the loss or not</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bCECriterion</span> <span class="o">=</span> <span class="n">BCECriterion</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="go">creating: createBCECriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bCECriterion</span> <span class="o">=</span> <span class="n">BCECriterion</span><span class="p">()</span>
<span class="go">creating: createBCECriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.CategoricalCrossEntropy">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">CategoricalCrossEntropy</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#CategoricalCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.CategoricalCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>This criterion is same with cross entropy criterion, except it takes a one-hot format target
tensor
&gt;&gt;&gt; cce = CategoricalCrossEntropy()
creating: createCategoricalCrossEntropy</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.ClassNLLCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">ClassNLLCriterion</code><span class="sig-paren">(</span><em>weights=None</em>, <em>size_average=True</em>, <em>logProbAsInput=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#ClassNLLCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.ClassNLLCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>The negative log likelihood criterion. It is useful to train a classification problem with n
classes. If provided, the optional argument weights should be a 1D Tensor assigning weight to
each of the classes. This is particularly useful when you have an unbalanced training set.</p>
<p>The input given through a forward() is expected to contain log-probabilities/probabilities of
each class: input has to be a 1D Tensor of size n. Obtaining log-probabilities/probabilities
in a neural network is easily achieved by adding a LogSoftMax/SoftMax layer in the last layer
of your neural network. You may use CrossEntropyCriterion instead, if you prefer not to add an
extra layer to your network. This criterion expects a class index (1 to the number of class) as
target when calling forward(input, target) and backward(input, target).</p>
<p>In the log-probabilities case,
The loss can be described as:
loss(x, class) = -x[class]
or in the case of the weights argument it is specified as follows:
loss(x, class) = -weights[class] * x[class]
Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when
calculating losses in non-batch mode.</p>
<p>Note that if the target is <cite>-1</cite>, the training process will skip this sample.
In other will, the forward process will return zero output and the backward process
will also return zero <cite>gradInput</cite>.</p>
<p>By default, the losses are averaged over observations for each minibatch. However, if the field
sizeAverage is set to false, the losses are instead summed for each minibatch.</p>
<p>In particular, when weights=None, size_average=True and logProbAsInput=False, this is same as
<cite>sparse_categorical_crossentropy</cite> loss in keras.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weights</strong> – weights of each class</li>
<li><strong>size_average</strong> – whether to average or not</li>
<li><strong>logProbAsInput</strong> – indicating whether to accept log-probabilities or probabilities as input.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classNLLCriterion</span> <span class="o">=</span> <span class="n">ClassNLLCriterion</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createClassNLLCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classNLLCriterion</span> <span class="o">=</span> <span class="n">ClassNLLCriterion</span><span class="p">()</span>
<span class="go">creating: createClassNLLCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.ClassSimplexCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">ClassSimplexCriterion</code><span class="sig-paren">(</span><em>n_classes</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#ClassSimplexCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.ClassSimplexCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>ClassSimplexCriterion implements a criterion for classification.
It learns an embedding per class, where each class’ embedding is a
point on an (N-1)-dimensional simplex, where N is the number of classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>nClasses</strong> – the number of classes.</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classSimplexCriterion</span> <span class="o">=</span> <span class="n">ClassSimplexCriterion</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createClassSimplexCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.CosineDistanceCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">CosineDistanceCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#CosineDistanceCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.CosineDistanceCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the loss given an input and target,
Loss = 1 - cos(x, y)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cosineDistanceCriterion</span> <span class="o">=</span> <span class="n">CosineDistanceCriterion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createCosineDistanceCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cosineDistanceCriterion</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]),</span>
<span class="gp">... </span>                                  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]))</span>
<span class="go">0.07272728</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.CosineEmbeddingCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">CosineEmbeddingCriterion</code><span class="sig-paren">(</span><em>margin=0.0</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#CosineEmbeddingCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.CosineEmbeddingCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the loss given an input x = {x1, x2},
a table of two Tensors, and a Tensor label y with values 1 or -1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>margin</strong> – a number from -1 to 1, 0 to 0.5 is suggested</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cosineEmbeddingCriterion</span> <span class="o">=</span> <span class="n">CosineEmbeddingCriterion</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createCosineEmbeddingCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cosineEmbeddingCriterion</span><span class="o">.</span><span class="n">forward</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]),</span>
<span class="gp">... </span>                                  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])],</span>
<span class="gp">... </span>                                <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="go">0.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.CosineProximityCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">CosineProximityCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#CosineProximityCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.CosineProximityCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>compute the negative of the mean cosine proximity between predictions and targets.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="s1">&#39;(i) = x(i) / sqrt(max(sum(x(i)^2), 1e-12))</span>
<span class="n">y</span><span class="s1">&#39;(i) = y(i) / sqrt(max(sum(x(i)^2), 1e-12))</span>
<span class="n">cosine_proximity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_i</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">x</span><span class="s1">&#39;(i) * y&#39;</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cosineProximityCriterion</span> <span class="o">=</span> <span class="n">CosineProximityCriterion</span><span class="p">()</span>
<span class="go">creating: createCosineProximityCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.Criterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">Criterion</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#Criterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.Criterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="bigdl.util.html#bigdl.util.common.JavaValue" title="bigdl.util.common.JavaValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.util.common.JavaValue</span></code></a></p>
<p>Criterion is helpful to train a neural network.
Given an input and a target, they compute a gradient according to a given loss function.</p>
<dl class="method">
<dt id="bigdl.nn.criterion.Criterion.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>input</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#Criterion.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.Criterion.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>NB: It’s for debug only, please use optimizer.optimize() in production.
Performs a back-propagation step through the criterion, with respect to the given input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> – ndarray or list of ndarray</li>
<li><strong>target</strong> – ndarray or list of ndarray</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.criterion.Criterion.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#Criterion.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.Criterion.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>NB: It’s for debug only, please use optimizer.optimize() in production.
Takes an input object, and computes the corresponding loss of the criterion,
compared with <cite>target</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> – ndarray or list of ndarray</li>
<li><strong>target</strong> – ndarray or list of ndarray</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">value of loss</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="bigdl.nn.criterion.Criterion.of">
<em class="property">classmethod </em><code class="descname">of</code><span class="sig-paren">(</span><em>jcriterion</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#Criterion.of"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.Criterion.of" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a python Criterion by a java criterion object</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>jcriterion</strong> – A java criterion object which created by Py4j</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a criterion.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.CrossEntropyCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">CrossEntropyCriterion</code><span class="sig-paren">(</span><em>weights=None</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#CrossEntropyCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.CrossEntropyCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>This criterion combines LogSoftMax and ClassNLLCriterion in one single class.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>weights</strong> – A tensor assigning weight to each of the classes</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cec</span> <span class="o">=</span> <span class="n">CrossEntropyCriterion</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="go">creating: createCrossEntropyCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cec</span> <span class="o">=</span> <span class="n">CrossEntropyCriterion</span><span class="p">()</span>
<span class="go">creating: createCrossEntropyCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.DiceCoefficientCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">DiceCoefficientCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>epsilon=1.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#DiceCoefficientCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.DiceCoefficientCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>The Dice-Coefficient criterion
input: Tensor,target: Tensor</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">return</span><span class="p">:</span>      <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="nb">input</span> <span class="n">intersection</span> <span class="n">target</span><span class="p">)</span>
        <span class="mi">1</span> <span class="o">-</span> <span class="o">----------------------------------</span>
                <span class="nb">input</span> <span class="n">union</span> <span class="n">target</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">diceCoefficientCriterion</span> <span class="o">=</span> <span class="n">DiceCoefficientCriterion</span><span class="p">(</span><span class="n">size_average</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="go">creating: createDiceCoefficientCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diceCoefficientCriterion</span> <span class="o">=</span> <span class="n">DiceCoefficientCriterion</span><span class="p">()</span>
<span class="go">creating: createDiceCoefficientCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.DistKLDivCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">DistKLDivCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#DistKLDivCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.DistKLDivCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>The Kullback-Leibler divergence criterion</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sizeAverage</strong> – </td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">distKLDivCriterion</span> <span class="o">=</span> <span class="n">DistKLDivCriterion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createDistKLDivCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.DotProductCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">DotProductCriterion</code><span class="sig-paren">(</span><em>size_average=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#DotProductCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.DotProductCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Compute the dot product of input and target tensor.
Input and target are required to have the same size.
:param size_average: whether to average over each observations in the same batch</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dp</span> <span class="o">=</span><span class="n">DotProductCriterion</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createDotProductCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.GaussianCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">GaussianCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#GaussianCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.GaussianCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Computes the log-likelihood of a sample x given a Gaussian distribution p.
&gt;&gt;&gt; GaussianCriterion = GaussianCriterion()
creating: createGaussianCriterion</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.HingeEmbeddingCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">HingeEmbeddingCriterion</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#HingeEmbeddingCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.HingeEmbeddingCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the loss given an
input x which is a 1-dimensional vector and a label y (1 or -1).
This is usually used for measuring whether two inputs are similar
or dissimilar,
e.g. using the L1 pairwise distance, and is typically used for
learning nonlinear embeddings or semi-supervised learning.</p>
<p>If x and y are n-dimensional Tensors, the sum operation still operates
over all the elements, and divides by n (this can be avoided if one sets
the internal variable sizeAverage to false). The margin has a default
value of 1, or can be set in the constructor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hingeEmbeddingCriterion</span> <span class="o">=</span> <span class="n">HingeEmbeddingCriterion</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createHingeEmbeddingCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.KLDCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">KLDCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#KLDCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.KLDCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Computes the KL-divergence of the input normal distribution to a standard normal distribution.
The input has to be a table. The first element of input is the mean of the distribution,
the second element of input is the log_variance of the distribution. The input distribution is
assumed to be diagonal.
&gt;&gt;&gt; KLDCriterion = KLDCriterion(True)
creating: createKLDCriterion</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.KullbackLeiblerDivergenceCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">KullbackLeiblerDivergenceCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#KullbackLeiblerDivergenceCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.KullbackLeiblerDivergenceCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>compute Kullback Leibler DivergenceCriterion error for intput and target
This method is same as <cite>kullback_leibler_divergence</cite> loss in keras. Loss calculated as:
y_true = K.clip(input, K.epsilon(), 1)
y_pred = K.clip(target, K.epsilon(), 1)
and output K.sum(y_true * K.log(y_true / y_pred), axis=-1)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">error</span> <span class="o">=</span> <span class="n">KullbackLeiblerDivergenceCriterion</span><span class="p">()</span>
<span class="go">creating: createKullbackLeiblerDivergenceCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.L1Cost">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">L1Cost</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#L1Cost"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.L1Cost" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>compute L1 norm for input, and sign of input</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l1Cost</span> <span class="o">=</span> <span class="n">L1Cost</span><span class="p">()</span>
<span class="go">creating: createL1Cost</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.L1HingeEmbeddingCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">L1HingeEmbeddingCriterion</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#L1HingeEmbeddingCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.L1HingeEmbeddingCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the loss given an input x = {x1, x2},
a table of two Tensors, and a label y (1 or -1):</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>margin</strong> – </td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l1HingeEmbeddingCriterion</span> <span class="o">=</span> <span class="n">L1HingeEmbeddingCriterion</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createL1HingeEmbeddingCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l1HingeEmbeddingCriterion</span> <span class="o">=</span> <span class="n">L1HingeEmbeddingCriterion</span><span class="p">()</span>
<span class="go">creating: createL1HingeEmbeddingCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.298</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">l1HingeEmbeddingCriterion</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">result</span> <span class="o">==</span> <span class="mf">5.148</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MSECriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MSECriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MSECriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MSECriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the mean squared error between n elements
in the input x and output y:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> \<span class="nb">sum</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|^</span><span class="mi">2</span>
</pre></div>
</div>
<p>If x and y are d-dimensional Tensors with a total of n elements,
the sum operation still operates over all the elements, and divides by n.
The two Tensors must have the same number of elements (but their sizes might be different).
The division by n can be avoided if one sets the internal variable sizeAverage to false.
By default, the losses are averaged over observations for each minibatch. However,
if the field sizeAverage is set to false, the losses are instead summed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mSECriterion</span> <span class="o">=</span> <span class="n">MSECriterion</span><span class="p">()</span>
<span class="go">creating: createMSECriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MarginCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MarginCriterion</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=True</em>, <em>squared=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MarginCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MarginCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that optimizes a two-class classification hinge loss (margin-based loss)
between input x (a Tensor of dimension 1) and output y.</p>
<p>When margin = 1, size_average = True and squared = False, this is the same as hinge loss in keras;
When margin = 1, size_average = False and squared = True, this is the same as squared_hinge loss in keras.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> – if unspecified, is by default 1.</li>
<li><strong>size_average</strong> – size average in a mini-batch</li>
<li><strong>squared</strong> – whether to calculate the squared hinge loss</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">marginCriterion</span> <span class="o">=</span> <span class="n">MarginCriterion</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createMarginCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MarginRankingCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MarginRankingCriterion</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MarginRankingCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MarginRankingCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that measures the loss given an input x = {x1, x2},
a table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).
In batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size
batchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.
If y == 1 then it assumed the first input should be ranked higher (have a larger value) than
the second input, and vice-versa for y == -1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>margin</strong> – </td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">marginRankingCriterion</span> <span class="o">=</span> <span class="n">MarginRankingCriterion</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createMarginRankingCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MeanAbsolutePercentageCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MeanAbsolutePercentageCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MeanAbsolutePercentageCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MeanAbsolutePercentageCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>This method is same as <cite>mean_absolute_percentage_error</cite> loss in keras.
It caculates diff = K.abs((y - x) / K.clip(K.abs(y), K.epsilon(), Double.MaxValue))
and return 100 * K.mean(diff) as output. Here, the x and y can have or not have a batch.
&gt;&gt;&gt; error = MeanAbsolutePercentageCriterion()
creating: createMeanAbsolutePercentageCriterion</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MeanSquaredLogarithmicCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MeanSquaredLogarithmicCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MeanSquaredLogarithmicCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MeanSquaredLogarithmicCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>This method is same as <cite>mean_squared_logarithmic_error</cite> loss in keras.
It calculates: first_log = K.log(K.clip(y, K.epsilon(),  Double.MaxValue) + 1.)
second_log = K.log(K.clip(x, K.epsilon(),  Double.MaxValue) + 1.)
and output K.mean(K.square(first_log - second_log)). Here, the x and y can have or not have a batch.
&gt;&gt;&gt; error = MeanSquaredLogarithmicCriterion()
creating: createMeanSquaredLogarithmicCriterion</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MultiCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MultiCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MultiCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MultiCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>a weighted sum of other criterions each applied to the same input and target</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">multiCriterion</span> <span class="o">=</span> <span class="n">MultiCriterion</span><span class="p">()</span>
<span class="go">creating: createMultiCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mSECriterion</span> <span class="o">=</span> <span class="n">MSECriterion</span><span class="p">()</span>
<span class="go">creating: createMSECriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiCriterion</span> <span class="o">=</span> <span class="n">multiCriterion</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mSECriterion</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiCriterion</span> <span class="o">=</span> <span class="n">multiCriterion</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mSECriterion</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.criterion.MultiCriterion.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>criterion</em>, <em>weight=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MultiCriterion.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MultiCriterion.add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MultiLabelMarginCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MultiLabelMarginCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MultiLabelMarginCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MultiLabelMarginCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that optimizes a multi-class multi-classification hinge loss (
margin-based loss) between input x and output y (which is a Tensor of target class indices)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size_average</strong> – size average in a mini-batch</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">multiLabelMarginCriterion</span> <span class="o">=</span> <span class="n">MultiLabelMarginCriterion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createMultiLabelMarginCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MultiLabelSoftMarginCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MultiLabelSoftMarginCriterion</code><span class="sig-paren">(</span><em>weights=None</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MultiLabelSoftMarginCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MultiLabelSoftMarginCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>A MultiLabel multiclass criterion based on sigmoid:
the loss is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_i</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">log</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
<p>where p[i] = exp(x[i]) / (1 + exp(x[i]))
and with weights:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_i</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">log</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiLabelSoftMarginCriterion</span> <span class="o">=</span> <span class="n">MultiLabelSoftMarginCriterion</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="go">creating: createMultiLabelSoftMarginCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiLabelSoftMarginCriterion</span> <span class="o">=</span> <span class="n">MultiLabelSoftMarginCriterion</span><span class="p">()</span>
<span class="go">creating: createMultiLabelSoftMarginCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.MultiMarginCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">MultiMarginCriterion</code><span class="sig-paren">(</span><em>p=1</em>, <em>weights=None</em>, <em>margin=1.0</em>, <em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#MultiMarginCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.MultiMarginCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss)
between input x and output y (which is a target class index).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> – </li>
<li><strong>weights</strong> – </li>
<li><strong>margin</strong> – </li>
<li><strong>size_average</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiMarginCriterion</span> <span class="o">=</span> <span class="n">MultiMarginCriterion</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">weights</span><span class="p">)</span>
<span class="go">creating: createMultiMarginCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiMarginCriterion</span> <span class="o">=</span> <span class="n">MultiMarginCriterion</span><span class="p">()</span>
<span class="go">creating: createMultiMarginCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.PGCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">PGCriterion</code><span class="sig-paren">(</span><em>sizeAverage=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#PGCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.PGCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>The Criterion to compute the negative policy gradient given a
multinomial distribution and the sampled action and reward.</p>
<p>The input to this criterion should be a 2-D tensor representing
a batch of multinomial distribution, the target should also be
a 2-D tensor with the same size of input, representing the sampled
action and reward/advantage with the index of non-zero element in the vector
represents the sampled action and the non-zero element itself represents
the reward. If the action is space is large, you should consider using
SparseTensor for target.</p>
<p>The loss computed is simple the standard policy gradient,</p>
<p>loss = - 1/n * sum(R_{n} dot_product log(P_{n}))</p>
<p>where R_{n} is the reward vector, and P_{n} is the input distribution.</p>
<p>:param sizeAverage whether to average over each observations in the same batch</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pg</span> <span class="o">=</span> <span class="n">PGCriterion</span><span class="p">()</span>
<span class="go">creating: createPGCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.ParallelCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">ParallelCriterion</code><span class="sig-paren">(</span><em>repeat_target=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#ParallelCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.ParallelCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>ParallelCriterion is a weighted sum of other criterions each applied to a different input
and target. Set repeatTarget = true to share the target for criterions.</p>
<p>Use add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>repeat_target</strong> – Whether to share the target for all criterions.</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">parallelCriterion</span> <span class="o">=</span> <span class="n">ParallelCriterion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createParallelCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mSECriterion</span> <span class="o">=</span> <span class="n">MSECriterion</span><span class="p">()</span>
<span class="go">creating: createMSECriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallelCriterion</span> <span class="o">=</span> <span class="n">parallelCriterion</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mSECriterion</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallelCriterion</span> <span class="o">=</span> <span class="n">parallelCriterion</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mSECriterion</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.criterion.ParallelCriterion.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>criterion</em>, <em>weight=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#ParallelCriterion.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.ParallelCriterion.add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.PoissonCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">PoissonCriterion</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#PoissonCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.PoissonCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>compute Poisson error for input and target, loss calculated as:
mean(input - target * K.log(input + K.epsilon()), axis=-1)
&gt;&gt;&gt; error = PoissonCriterion()
creating: createPoissonCriterion</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.SmoothL1Criterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">SmoothL1Criterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#SmoothL1Criterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.SmoothL1Criterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that can be thought of as a smooth version of the AbsCriterion.
It uses a squared term if the absolute element-wise error falls below 1.
It is less sensitive to outliers than the MSECriterion and in some
cases prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                      <span class="o">|</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="o">^</span><span class="p">,</span> <span class="k">if</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> \<span class="nb">sum</span> <span class="o">|</span>
                      <span class="o">|</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>   <span class="n">otherwise</span>
</pre></div>
</div>
<p>If x and y are d-dimensional Tensors with a total of n elements,
the sum operation still operates over all the elements, and divides by n.
The division by n can be avoided if one sets the internal variable sizeAverage to false</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size_average</strong> – whether to average the loss</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">smoothL1Criterion</span> <span class="o">=</span> <span class="n">SmoothL1Criterion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createSmoothL1Criterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.SmoothL1CriterionWithWeights">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">SmoothL1CriterionWithWeights</code><span class="sig-paren">(</span><em>sigma</em>, <em>num=0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#SmoothL1CriterionWithWeights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.SmoothL1CriterionWithWeights" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>a smooth version of the AbsCriterion
It uses a squared term if the absolute element-wise error falls below 1.
It is less sensitive to outliers than the MSECriterion and in some cases
prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">w_in</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">w_out</span><span class="p">)</span>
           <span class="o">|</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">d_i</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">w_out</span>          <span class="k">if</span> <span class="o">|</span><span class="n">d_i</span><span class="o">|</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">sigma</span> <span class="o">/</span> <span class="n">sigma</span>
<span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> \<span class="nb">sum</span> <span class="o">|</span>
           <span class="o">|</span> <span class="p">(</span><span class="o">|</span><span class="n">d_i</span><span class="o">|</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="n">sigma</span> <span class="o">/</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">*</span> <span class="n">w_out</span>   <span class="n">otherwise</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">smoothL1CriterionWithWeights</span> <span class="o">=</span> <span class="n">SmoothL1CriterionWithWeights</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSmoothL1CriterionWithWeights</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.SoftMarginCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">SoftMarginCriterion</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#SoftMarginCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.SoftMarginCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Creates a criterion that optimizes a two-class classification logistic loss
between input x (a Tensor of dimension 1) and output y (which is a tensor
containing either 1s or -1s).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_i</span> <span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span> <span class="o">/</span> <span class="n">x</span><span class="p">:</span><span class="n">nElement</span><span class="p">()</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sizeaverage</strong> – The normalization by the number of elements in the inputcan be disabled by setting</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softMarginCriterion</span> <span class="o">=</span> <span class="n">SoftMarginCriterion</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createSoftMarginCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softMarginCriterion</span> <span class="o">=</span> <span class="n">SoftMarginCriterion</span><span class="p">()</span>
<span class="go">creating: createSoftMarginCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.SoftmaxWithCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">SoftmaxWithCriterion</code><span class="sig-paren">(</span><em>ignore_label=None</em>, <em>normalize_mode='VALID'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#SoftmaxWithCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.SoftmaxWithCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>Computes the multinomial logistic loss for a one-of-many classification task,
passing real-valued predictions through a softmax to get a probability distribution over classes.
It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer
as its gradient computation is more numerically stable.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ignoreLabel</strong> – (optional) Specify a label value thatshould be ignored when computing the loss.</li>
<li><strong>normalizeMode</strong> – How to normalize the output loss.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softmaxWithCriterion</span> <span class="o">=</span> <span class="n">SoftmaxWithCriterion</span><span class="p">()</span>
<span class="go">creating: createSoftmaxWithCriterion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmaxWithCriterion</span> <span class="o">=</span> <span class="n">SoftmaxWithCriterion</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;FULL&quot;</span><span class="p">)</span>
<span class="go">creating: createSoftmaxWithCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.TimeDistributedCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">TimeDistributedCriterion</code><span class="sig-paren">(</span><em>criterion</em>, <em>size_average=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#TimeDistributedCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.TimeDistributedCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>This class is intended to support inputs with 3 or more dimensions.
Apply Any Provided Criterion to every temporal slice of an input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>criterion</strong> – embedded criterion</li>
<li><strong>size_average</strong> – whether to divide the sequence length</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">td</span> <span class="o">=</span> <span class="n">TimeDistributedCriterion</span><span class="p">(</span><span class="n">ClassNLLCriterion</span><span class="p">())</span>
<span class="go">creating: createClassNLLCriterion</span>
<span class="go">creating: createTimeDistributedCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.TimeDistributedMaskCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">TimeDistributedMaskCriterion</code><span class="sig-paren">(</span><em>criterion</em>, <em>padding_value=0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#TimeDistributedMaskCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.TimeDistributedMaskCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>This class is intended to support inputs with 3 or more dimensions.
Apply Any Provided Criterion to every temporal slice of an input.
In addition, it supports padding mask.</p>
<p>eg. if the target is [ [-1, 1, 2, 3, -1], [5, 4, 3, -1, -1] ],
and set the paddingValue property to -1, then the loss of -1 would not
be accumulated and the loss is only divided by 6 (ont including the amount of
-1, in this case, we are only interested in 1, 2, 3, 5, 4, 3)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>criterion</strong> – embedded criterion</li>
<li><strong>padding_value</strong> – padding value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">td</span> <span class="o">=</span> <span class="n">TimeDistributedMaskCriterion</span><span class="p">(</span><span class="n">ClassNLLCriterion</span><span class="p">())</span>
<span class="go">creating: createClassNLLCriterion</span>
<span class="go">creating: createTimeDistributedMaskCriterion</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.criterion.TransformerCriterion">
<em class="property">class </em><code class="descclassname">bigdl.nn.criterion.</code><code class="descname">TransformerCriterion</code><span class="sig-paren">(</span><em>criterion</em>, <em>input_transformer=None</em>, <em>target_transformer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/criterion.html#TransformerCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.criterion.TransformerCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.criterion.Criterion" title="bigdl.nn.criterion.Criterion"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.criterion.Criterion</span></code></a></p>
<p>The criterion that takes two modules to transform input and target, and take
one criterion to compute the loss with the transformed input and target.</p>
<p>This criterion can be used to construct complex criterion. For example, the
<cite>inputTransformer</cite> and <cite>targetTransformer</cite> can be pre-trained CNN networks,
and we can use the networks’ output to compute the high-level feature
reconstruction loss, which is commonly used in areas like neural style transfer
(<a class="reference external" href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>), texture synthesis (<a class="reference external" href="https://arxiv.org/abs/1505.07376">https://arxiv.org/abs/1505.07376</a>),
.etc.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trans</span> <span class="o">=</span> <span class="n">TransformerCriterion</span><span class="p">(</span><span class="n">MSECriterion</span><span class="p">())</span>
<span class="go">creating: createMSECriterion</span>
<span class="go">creating: createTransformerCriterion</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-bigdl.nn.initialization_method">
<span id="bigdl-nn-initialization-method-module"></span><h2>bigdl.nn.initialization_method module<a class="headerlink" href="#module-bigdl.nn.initialization_method" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="bigdl.nn.initialization_method.BilinearFiller">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">BilinearFiller</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#BilinearFiller"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.BilinearFiller" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Initialize the weight with coefficients for bilinear interpolation.</p>
<p>A common use case is with the DeconvolutionLayer acting as upsampling.
The variable tensor passed in the init function should have 5 dimensions
of format [nGroup, nInput, nOutput, kH, kW], and kH should be equal to kW</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.ConstInitMethod">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">ConstInitMethod</code><span class="sig-paren">(</span><em>value</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#ConstInitMethod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.ConstInitMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Initializer that generates tensors with certain constant double.</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.InitializationMethod">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">InitializationMethod</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#InitializationMethod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.InitializationMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="bigdl.util.html#bigdl.util.common.JavaValue" title="bigdl.util.common.JavaValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.util.common.JavaValue</span></code></a></p>
<p>Initialization method to initialize bias and weight.
The init method will be called in Module.reset()</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.MsraFiller">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">MsraFiller</code><span class="sig-paren">(</span><em>varianceNormAverage=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#MsraFiller"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.MsraFiller" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>MsraFiller Initializer.
See <a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf</a></p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.Ones">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">Ones</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#Ones"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.Ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Initializer that generates tensors with ones.</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.RandomNormal">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">RandomNormal</code><span class="sig-paren">(</span><em>mean</em>, <em>stdv</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#RandomNormal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.RandomNormal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Initializer that generates tensors with a normal distribution.</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.RandomUniform">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">RandomUniform</code><span class="sig-paren">(</span><em>upper=None</em>, <em>lower=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#RandomUniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.RandomUniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Initializer that generates tensors with a uniform distribution.
It draws samples from a uniform distribution within [lower, upper]
If lower and upper is not specified, it draws samples form a
uniform distribution within [-limit, limit] where “limit” is “1/sqrt(fan_in)”</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.Xavier">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">Xavier</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#Xavier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.Xavier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Xavier Initializer. See <a class="reference external" href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a></p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.initialization_method.Zeros">
<em class="property">class </em><code class="descclassname">bigdl.nn.initialization_method.</code><code class="descname">Zeros</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/initialization_method.html#Zeros"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.initialization_method.Zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.initialization_method.InitializationMethod" title="bigdl.nn.initialization_method.InitializationMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.initialization_method.InitializationMethod</span></code></a></p>
<p>Initializer that generates tensors with zeros.</p>
</dd></dl>

</div>
<div class="section" id="module-bigdl.nn.layer">
<span id="bigdl-nn-layer-module"></span><h2>bigdl.nn.layer module<a class="headerlink" href="#module-bigdl.nn.layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="bigdl.nn.layer.Abs">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Abs</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Abs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>an element-wise abs operation</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">abs</span> <span class="o">=</span> <span class="n">Abs</span><span class="p">()</span>
<span class="go">creating: createAbs</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ActivityRegularization">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ActivityRegularization</code><span class="sig-paren">(</span><em>l1=0.0</em>, <em>l2=0.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ActivityRegularization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ActivityRegularization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Layer that applies an update to the cost function based input activity.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>l1</strong> – L1 regularization factor (positive float).</li>
<li><strong>l2</strong> – L2 regularization factor (positive float).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ar</span> <span class="o">=</span> <span class="n">ActivityRegularization</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="go">creating: createActivityRegularization</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Add">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Add</code><span class="sig-paren">(</span><em>input_size</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Add" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>adds a bias term to input data ;</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input_size</strong> – size of input data</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createAdd</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Add.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Add.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Add.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.AddConstant">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">AddConstant</code><span class="sig-paren">(</span><em>constant_scalar</em>, <em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#AddConstant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.AddConstant" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>adding a constant</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>constant_scalar</strong> – constant value</li>
<li><strong>inplace</strong> – Can optionally do its operation in-place without using extra state memory</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">addConstant</span> <span class="o">=</span> <span class="n">AddConstant</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createAddConstant</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.BatchNormalization">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">BatchNormalization</code><span class="sig-paren">(</span><em>n_output</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#BatchNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.BatchNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This layer implements Batch Normalization as described in the paper:
“Batch Normalization: Accelerating Deep Network Training by Reducing Internal
Covariate Shift”
by Sergey Ioffe, Christian Szegedy <a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></p>
<p>This implementation is useful for inputs NOT coming from convolution layers. For convolution
layers, use nn.SpatialBatchNormalization.</p>
<p>The operation implemented is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="p">(</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">--------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
    <span class="n">standard</span><span class="o">-</span><span class="n">deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>where gamma and beta are learnable parameters.The learning of gamma and beta is optional.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_output</strong> – output feature map number</li>
<li><strong>eps</strong> – avoid divide zero</li>
<li><strong>momentum</strong> – momentum for weight update</li>
<li><strong>affine</strong> – affine operation on output or not</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batchNormalization</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createBatchNormalization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batchNormalization</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">)</span>
<span class="go">creating: createBatchNormalization</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.BatchNormalization.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#BatchNormalization.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.BatchNormalization.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.BiRecurrent">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">BiRecurrent</code><span class="sig-paren">(</span><em>merge=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#BiRecurrent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.BiRecurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>Create a Bidirectional recurrent layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>merge</strong> – merge layer</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">biRecurrent</span> <span class="o">=</span> <span class="n">BiRecurrent</span><span class="p">(</span><span class="n">CAddTable</span><span class="p">())</span>
<span class="go">creating: createCAddTable</span>
<span class="go">creating: createBiRecurrent</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">biRecurrent</span> <span class="o">=</span> <span class="n">BiRecurrent</span><span class="p">()</span>
<span class="go">creating: createBiRecurrent</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.BifurcateSplitTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">BifurcateSplitTable</code><span class="sig-paren">(</span><em>dimension</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#BifurcateSplitTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.BifurcateSplitTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Creates a module that takes a Tensor as input and
outputs two tables, splitting the Tensor along
the specified dimension <cite>dimension</cite>.</p>
<p>The input to this layer is expected to be a tensor, or a batch of tensors;</p>
<p>:param dimension to be split along this dimension
:param T Numeric type. Only support float/double now</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bifurcateSplitTable</span> <span class="o">=</span> <span class="n">BifurcateSplitTable</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createBifurcateSplitTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Bilinear">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Bilinear</code><span class="sig-paren">(</span><em>input_size1</em>, <em>input_size2</em>, <em>output_size</em>, <em>bias_res=True</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>a bilinear transformation with sparse inputs,
The input tensor given in forward(input) is a table containing both inputs x_1 and x_2,
which are tensors of size N x inputDimension1 and N x inputDimension2, respectively.</p>
<p>:param input_size1 input dimension of x_1
:param input_size2 input dimension of x_2
:param output_size output dimension
:param bias_res whether use bias
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bilinear</span> <span class="o">=</span> <span class="n">Bilinear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createBilinear</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Bilinear.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Bilinear.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Bilinear.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.BinaryThreshold">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">BinaryThreshold</code><span class="sig-paren">(</span><em>th=1e-06</em>, <em>ip=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#BinaryThreshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.BinaryThreshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Binary threshold, 1 if value &gt; th, 0 otherwise
&gt;&gt;&gt; layer = BinaryThreshold(0.1, False)
creating: createBinaryThreshold</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.BinaryTreeLSTM">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">BinaryTreeLSTM</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>gate_output=True</em>, <em>with_graph=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#BinaryTreeLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.BinaryTreeLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This class is an implementation of Binary TreeLSTM (Constituency Tree LSTM).
:param inputSize input units size
:param hiddenSize hidden units size
:param gateOutput whether gate output
:param withGraph whether create lstms with [[Graph]], the default value is true.
&gt;&gt;&gt; treeLSTM = BinaryTreeLSTM(100, 200)
creating: createBinaryTreeLSTM</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Bottle">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Bottle</code><span class="sig-paren">(</span><em>module</em>, <em>n_input_dim=2</em>, <em>n_output_dim1=2147483647</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Bottle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Bottle" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>Bottle allows varying dimensionality input to be forwarded through any module
that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> – transform module</li>
<li><strong>n_input_dim</strong> – nInputDim dimensions of module</li>
<li><strong>n_output_dim1</strong> – output of nOutputDim dimensions</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bottle</span> <span class="o">=</span> <span class="n">Bottle</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createLinear</span>
<span class="go">creating: createBottle</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CAdd">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CAdd</code><span class="sig-paren">(</span><em>size</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CAdd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CAdd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This layer has a bias tensor with given size. The bias will be added element wise to the input
tensor. If the element number of the bias tensor match the input tensor, a simply element wise
will be done. Or the bias will be expanded to the same size of the input. The expand means
repeat on unmatched singleton dimension(if some unmatched dimension isn’t singleton dimension,
it will report an error). If the input is a batch, a singleton dimension will be add to the
first dimension before the expand.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> – the size of the bias</li>
<li><strong>bRegularizer</strong> – instance of [[Regularizer]]applied to the bias.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cAdd</span> <span class="o">=</span> <span class="n">CAdd</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="go">creating: createCAdd</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.CAdd.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CAdd.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CAdd.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CAddTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CAddTable</code><span class="sig-paren">(</span><em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CAddTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CAddTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Merge the input tensors in the input table by element wise adding them together. The input
table is actually an array of tensor with same size.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – reuse the input memory</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cAddTable</span> <span class="o">=</span> <span class="n">CAddTable</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createCAddTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CAveTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CAveTable</code><span class="sig-paren">(</span><em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CAveTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CAveTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Merge the input tensors in the input table by element wise taking the average. The input
table is actually an array of tensor with same size.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – reuse the input memory</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cAveTable</span> <span class="o">=</span> <span class="n">CAveTable</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createCAveTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CDivTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CDivTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CDivTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CDivTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Takes a table with two Tensor and returns the component-wise division between them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cDivTable</span> <span class="o">=</span> <span class="n">CDivTable</span><span class="p">()</span>
<span class="go">creating: createCDivTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CMaxTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CMaxTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CMaxTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CMaxTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Takes a table of Tensors and outputs the max of all of them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cMaxTable</span> <span class="o">=</span> <span class="n">CMaxTable</span><span class="p">()</span>
<span class="go">creating: createCMaxTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CMinTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CMinTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CMinTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CMinTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Takes a table of Tensors and outputs the min of all of them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cMinTable</span> <span class="o">=</span> <span class="n">CMinTable</span><span class="p">()</span>
<span class="go">creating: createCMinTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CMul">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CMul</code><span class="sig-paren">(</span><em>size</em>, <em>wRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CMul"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CMul" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a component-wise multiplication to the incoming data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size</strong> – size of the data</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cMul</span> <span class="o">=</span> <span class="n">CMul</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="go">creating: createCMul</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.CMul.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CMul.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CMul.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CMulTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CMulTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CMulTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CMulTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Takes a table of Tensors and outputs the multiplication of all of them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cMulTable</span> <span class="o">=</span> <span class="n">CMulTable</span><span class="p">()</span>
<span class="go">creating: createCMulTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CSubTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CSubTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CSubTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CSubTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Takes a table with two Tensor and returns the component-wise subtraction between them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cSubTable</span> <span class="o">=</span> <span class="n">CSubTable</span><span class="p">()</span>
<span class="go">creating: createCSubTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Clamp">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Clamp</code><span class="sig-paren">(</span><em>min</em>, <em>max</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Clamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Clamps all elements into the range [min_value, max_value].
Output is identical to input in the range,
otherwise elements less than min_value (or greater than max_value)
are saturated to min_value (or max_value).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>min</strong> – </li>
<li><strong>max</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clamp</span> <span class="o">=</span> <span class="n">Clamp</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">creating: createClamp</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Concat">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Concat</code><span class="sig-paren">(</span><em>dimension</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>Concat concatenates the output of one layer of “parallel”
modules along the provided <a class="reference external" href="mailto:{&#37;&#52;&#48;code">{<span>&#64;</span>code</a> dimension}: they take the
same inputs, and their output is concatenated.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                <span class="o">+-----------+</span>
           <span class="o">+----&gt;</span>  <span class="n">module1</span>  <span class="o">-----+</span>
           <span class="o">|</span>    <span class="o">|</span>           <span class="o">|</span>    <span class="o">|</span>
<span class="nb">input</span> <span class="o">-----+----&gt;</span>  <span class="n">module2</span>  <span class="o">-----+----&gt;</span> <span class="n">output</span>
           <span class="o">|</span>    <span class="o">|</span>           <span class="o">|</span>    <span class="o">|</span>
           <span class="o">+----&gt;</span>  <span class="n">module3</span>  <span class="o">-----+</span>
                <span class="o">+-----------+</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dimension</strong> – dimension</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">concat</span> <span class="o">=</span> <span class="n">Concat</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createConcat</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ConcatTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ConcatTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ConcatTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ConcatTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>ConcateTable is a container module like Concate. Applies an input
to each member module, input can be a tensor or a table.</p>
<p>ConcateTable usually works with CAddTable and CMulTable to
implement element wise add/multiply on outputs of two modules.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">concatTable</span> <span class="o">=</span> <span class="n">ConcatTable</span><span class="p">()</span>
<span class="go">creating: createConcatTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Container">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Container</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Container"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Container" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>[[Container]] is a sub-class of Model that declares methods defined in all containers.
A container usually contain some other modules which can be added through the “add” method</p>
<dl class="method">
<dt id="bigdl.nn.layer.Container.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>model</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Container.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Container.add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Container.flattened_layers">
<code class="descname">flattened_layers</code><span class="sig-paren">(</span><em>include_container=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Container.flattened_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Container.flattened_layers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="bigdl.nn.layer.Container.layers">
<code class="descname">layers</code><a class="headerlink" href="#bigdl.nn.layer.Container.layers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Contiguous">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Contiguous</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Contiguous"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>used to make input, grad_output both contiguous</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">contiguous</span> <span class="o">=</span> <span class="n">Contiguous</span><span class="p">()</span>
<span class="go">creating: createContiguous</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ConvLSTMPeephole">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ConvLSTMPeephole</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>kernel_i</em>, <em>kernel_c</em>, <em>stride=1</em>, <em>padding=-1</em>, <em>activation=None</em>, <em>inner_activation=None</em>, <em>wRegularizer=None</em>, <em>uRegularizer=None</em>, <em>bRegularizer=None</em>, <em>cRegularizer=None</em>, <em>with_peephole=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ConvLSTMPeephole"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ConvLSTMPeephole" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<div class="line-block">
<div class="line">Convolution Long Short Term Memory architecture with peephole.</div>
<div class="line">Ref. A.: <a class="reference external" href="https://arxiv.org/abs/1506.04214">https://arxiv.org/abs/1506.04214</a> (blueprint for this module)</div>
<div class="line">B. <a class="reference external" href="https://github.com/viorik/ConvLSTM">https://github.com/viorik/ConvLSTM</a></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – number of input planes in the image given into forward()</li>
<li><strong>output_size</strong> – number of output planes the convolution layer will produce</li>
<li><strong>kernel_i</strong> – Convolutional filter size to convolve input</li>
<li><strong>kernel_c</strong> – Convolutional filter size to convolve cell</li>
<li><strong>stride</strong> – The step of the convolution, default is 1</li>
<li><strong>padding</strong> – The additional zeros added, default is -1</li>
<li><strong>activation</strong> – activation function, by default to be Tanh if not specified.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>It can also be the name of an existing activation as a string.
:param inner_activation: activation function for the inner cells, by default to be Sigmoid if not specified.
It can also be the name of an existing activation as a string.
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices
:param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices
:param bRegularizer: instance of [[Regularizer]]applied to the bias.
:param cRegularizer: instance of [[Regularizer]]applied to peephole.
:param with_peephole: whether use last cell status control a gate.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">convlstm</span> <span class="o">=</span> <span class="n">ConvLSTMPeephole</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">(),</span> <span class="n">HardSigmoid</span><span class="p">(),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createTanh</span>
<span class="go">creating: createHardSigmoid</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createConvLSTMPeephole</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ConvLSTMPeephole3D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ConvLSTMPeephole3D</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>kernel_i</em>, <em>kernel_c</em>, <em>stride=1</em>, <em>padding=-1</em>, <em>wRegularizer=None</em>, <em>uRegularizer=None</em>, <em>bRegularizer=None</em>, <em>cRegularizer=None</em>, <em>with_peephole=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ConvLSTMPeephole3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ConvLSTMPeephole3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – number of input planes in the image given into forward()</li>
<li><strong>output_size</strong> – number of output planes the convolution layer will produce</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param kernel_i Convolutional filter size to convolve input
:param kernel_c Convolutional filter size to convolve cell
:param stride The step of the convolution
:param padding The additional zeros added
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices
:param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices
:param bRegularizer: instance of [[Regularizer]]applied to the bias.
:param cRegularizer: instance of [[Regularizer]]applied to peephole.
:param with_peephole: whether use last cell status control a gate.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">convlstm</span> <span class="o">=</span> <span class="n">ConvLSTMPeephole3D</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createConvLSTMPeephole3D</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Cosine">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Cosine</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Cosine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Cosine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Cosine calculates the cosine similarity of the input to k mean centers. The input given in
forward(input) must be either a vector (1D tensor) or matrix (2D tensor). If the input is a
vector, it must have the size of inputSize. If it is a matrix, then each row is assumed to be
an input sample of given batch (the number of rows means the batch size and the number of
columns should be equal to the inputSize).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – the size of each input sample</li>
<li><strong>output_size</strong> – the size of the module output of each sample</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cosine</span> <span class="o">=</span> <span class="n">Cosine</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="go">creating: createCosine</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Cosine.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Cosine.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Cosine.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CosineDistance">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CosineDistance</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CosineDistance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CosineDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Outputs the cosine distance between inputs</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cosineDistance</span> <span class="o">=</span> <span class="n">CosineDistance</span><span class="p">()</span>
<span class="go">creating: createCosineDistance</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Cropping2D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Cropping2D</code><span class="sig-paren">(</span><em>heightCrop</em>, <em>widthCrop</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Cropping2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Cropping2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Cropping layer for 2D input (e.g. picture).
It crops along spatial dimensions, i.e. width and height.</p>
<p># Input shape
4D tensor with shape:
<cite>(batchSize, channels, first_axis_to_crop, second_axis_to_crop)</cite></p>
<p># Output shape
4D tensor with shape:
<cite>(batchSize, channels, first_cropped_axis, second_cropped_axis)</cite></p>
<p>:param heightCrop Array of length 2. How many units should be trimmed off at the beginning
and end of the height dimension.
:param widthCrop Array of length 2. How many units should be trimmed off at the beginning
and end of the width dimension.
:param data_format a string value (or DataFormat Object in Scala) of “NHWC” or “NCHW” to specify the input data format of this layer. In “NHWC” format
data is stored in the order of [batch_size, height, width, channels], in “NCHW” format data is stored
in the order of [batch_size, channels, height, width].
&gt;&gt;&gt; cropping2D = Cropping2D([1, 1], [2, 2])
creating: createCropping2D</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Cropping3D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Cropping3D</code><span class="sig-paren">(</span><em>dim1Crop</em>, <em>dim2Crop</em>, <em>dim3Crop</em>, <em>data_format='channel_first'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Cropping3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Cropping3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Cropping layer for 3D data (e.g. spatial or spatio-temporal).</p>
<p># Input shape
5D tensor with shape:
<cite>(batchSize, channels, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)</cite></p>
<p># Output shape
5D tensor with shape:
<cite>(batchSize, channels, first_cropped_axis, second_cropped_axis, third_cropped_axis)</cite></p>
<p>:param dim1Crop Array of length 2. How many units should be trimmed off at the beginning
and end of the first dimension.
:param dim2Crop Array of length 2. How many units should be trimmed off at the beginning
and end of the second dimension.
:param dim3Crop Array of length 2. How many units should be trimmed off at the beginning
and end of the third dimension.
:param data_format a string value. “channel_first” or “channel_last”
&gt;&gt;&gt; cropping3D = Cropping3D([1, 1], [2, 2], [1, 1])
creating: createCropping3D</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.CrossProduct">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">CrossProduct</code><span class="sig-paren">(</span><em>numTensor=0</em>, <em>embeddingSize=0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#CrossProduct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.CrossProduct" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>A layer which takes a table of multiple tensors(n &gt;= 2) as input
and calculate to dot product for <cite>all combinations of pairs</cite> among input tensors.</p>
<p>Dot-product outputs are ordered according to orders of pairs in input Table.
For instance, input (Table) is T(A, B, C), output (Tensor) will be [A.*B, A.*C, B.*C].</p>
<p>Dimensions of input’ Tensors could be one or two, if two, first dimension is <cite>batchSize</cite>.
For convenience, output is 2-dim Tensor regardless of input’ dims.</p>
<p>Table size checking and Tensor size checking will be execute before each forward,
when [[numTensor]] and [[embeddingSize]] are set values greater than zero.</p>
<p>:param numTensor (for checking)number of Tensor input Table contains, :default 0(won’t check)
:param embeddingSize (for checking)vector length of dot product, :default 0(won’t check)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crossProduct</span> <span class="o">=</span> <span class="n">CrossProduct</span><span class="p">()</span>
<span class="go">creating: createCrossProduct</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.DenseToSparse">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">DenseToSparse</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#DenseToSparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.DenseToSparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Convert DenseTensor to SparseTensor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">DenseToSparse</span> <span class="o">=</span> <span class="n">DenseToSparse</span><span class="p">()</span>
<span class="go">creating: createDenseToSparse</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.DetectionOutputFrcnn">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">DetectionOutputFrcnn</code><span class="sig-paren">(</span><em>n_classes</em>, <em>bbox_vote</em>, <em>nms_thresh=0.3</em>, <em>max_per_image=100</em>, <em>thresh=0.05</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#DetectionOutputFrcnn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.DetectionOutputFrcnn" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Post process Faster-RCNN models
:param nms_thresh nms threshold
:param n_classes number of classes
:param bbox_vote whether to vote for detections
:param max_per_image limit max number of detections per image
:param thresh score threshold
&gt;&gt;&gt; layer = DetectionOutputFrcnn(21, True)
creating: createDetectionOutputFrcnn</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.DetectionOutputSSD">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">DetectionOutputSSD</code><span class="sig-paren">(</span><em>n_classes=21</em>, <em>share_location=True</em>, <em>bg_label=0</em>, <em>nms_thresh=0.45</em>, <em>nms_topk=400</em>, <em>keep_top_k=200</em>, <em>conf_thresh=0.01</em>, <em>variance_encoded_in_target=False</em>, <em>conf_post_process=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#DetectionOutputSSD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.DetectionOutputSSD" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Layer to Post-process SSD output
:param n_classes number of classes
:param share_location whether to share location, default is true
:param bg_label background label
:param nms_thresh nms threshold
:param nms_topk nms topk
:param keep_top_k result topk
:param conf_thresh confidence threshold
:param variance_encoded_in_target if variance is encoded in target,
we simply need to retore the offset predictions,
else if variance is encoded in bbox,
we need to scale the offset accordingly.
:param conf_post_process whether add some additional post process to confidence prediction
&gt;&gt;&gt; layer = DetectionOutputSSD()
creating: createDetectionOutputSSD</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.DotProduct">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">DotProduct</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#DotProduct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.DotProduct" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This is a simple table layer which takes a table of two tensors as input
and calculate the dot product between them as outputs</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dotProduct</span> <span class="o">=</span> <span class="n">DotProduct</span><span class="p">()</span>
<span class="go">creating: createDotProduct</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Dropout">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>init_p=0.5</em>, <em>inplace=False</em>, <em>scale=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Dropout masks(set to zero) parts of input using a bernoulli distribution.
Each input element has a probability initP of being dropped. If scale is
set, the outputs are scaled by a factor of 1/(1-initP) during training.
During evaluating, output is the same as input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>initP</strong> – probability to be dropped</li>
<li><strong>inplace</strong> – inplace model</li>
<li><strong>scale</strong> – if scale by a factor of 1/(1-initP)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
<span class="go">creating: createDropout</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ELU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>D-A Clevert, Thomas Unterthiner, Sepp Hochreiter
Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
[<a class="reference external" href="http://arxiv.org/pdf/1511.07289.pdf">http://arxiv.org/pdf/1511.07289.pdf</a>]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eLU</span> <span class="o">=</span> <span class="n">ELU</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createELU</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Echo">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Echo</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Echo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Echo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This module is for debug purpose, which can print activation and gradient in your model
topology</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">echo</span> <span class="o">=</span> <span class="n">Echo</span><span class="p">()</span>
<span class="go">creating: createEcho</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Euclidean">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Euclidean</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>fast_backward=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Euclidean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Euclidean" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Outputs the Euclidean distance of the input to outputSize centers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>inputSize</strong> – inputSize</li>
<li><strong>outputSize</strong> – outputSize</li>
<li><strong>T</strong> – Numeric type. Only support float/double now</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">euclidean</span> <span class="o">=</span> <span class="n">Euclidean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createEuclidean</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Euclidean.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Euclidean.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Euclidean.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Exp">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Exp</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Exp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies element-wise exp to input tensor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span> <span class="o">=</span> <span class="n">Exp</span><span class="p">()</span>
<span class="go">creating: createExp</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.FlattenTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">FlattenTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#FlattenTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.FlattenTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This is a table layer which takes an arbitrarily deep table of Tensors
(potentially nested) as input and a table of Tensors without any nested
table will be produced</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">flattenTable</span> <span class="o">=</span> <span class="n">FlattenTable</span><span class="p">()</span>
<span class="go">creating: createFlattenTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.GRU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>p=0.0</em>, <em>activation=None</em>, <em>inner_activation=None</em>, <em>wRegularizer=None</em>, <em>uRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Gated Recurrent Units architecture.
The first input in sequence uses zero value for cell and hidden state</p>
<div class="line-block">
<div class="line">Ref.</div>
<div class="line"><a class="reference external" href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</a></div>
<div class="line"><a class="reference external" href="https://github.com/Element-Research/rnn/blob/master/GRU.lua">https://github.com/Element-Research/rnn/blob/master/GRU.lua</a></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – the size of each input vector</li>
<li><strong>hidden_size</strong> – Hidden unit size in GRU</li>
<li><strong>p</strong> – is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](<a class="reference external" href="http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A">http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A</a> Theoretically Grounded Application of Dropout in Recurrent Neural Networks](<a class="reference external" href="https://arxiv.org/pdf/1512.05287.pdf">https://arxiv.org/pdf/1512.05287.pdf</a>)</li>
<li><strong>activation</strong> – activation function, by default to be Tanh if not specified.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>It can also be the name of an existing activation as a string.
:param inner_activation: activation function for the inner cells, by default to be Sigmoid if not specified.
It can also be the name of an existing activation as a string.
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">(),</span> <span class="n">Sigmoid</span><span class="p">(),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createTanh</span>
<span class="go">creating: createSigmoid</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createGRU</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.GaussianDropout">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">GaussianDropout</code><span class="sig-paren">(</span><em>rate</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#GaussianDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.GaussianDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply multiplicative 1-centered Gaussian noise.
The multiplicative noise will have standard deviation <a href="#id1"><span class="problematic" id="id2">`</span></a>sqrt(rate / (1 - rate)).</p>
<p>As it is a regularization layer, it is only active at training time.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rate</strong> – drop probability (as with <cite>Dropout</cite>).</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">GaussianDropout</span> <span class="o">=</span> <span class="n">GaussianDropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">creating: createGaussianDropout</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.GaussianNoise">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">GaussianNoise</code><span class="sig-paren">(</span><em>stddev</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#GaussianNoise"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.GaussianNoise" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply additive zero-centered Gaussian noise.
This is useful to mitigate overfitting
(you could see it as a form of random data augmentation).
Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.</p>
<p>As it is a regularization layer, it is only active at training time.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stdev</strong> – standard deviation of the noise distribution</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">GaussianNoise</span> <span class="o">=</span> <span class="n">GaussianNoise</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">creating: createGaussianNoise</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.GaussianSampler">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">GaussianSampler</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#GaussianSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.GaussianSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Takes {mean, log_variance} as input and samples from the Gaussian distribution
&gt;&gt;&gt; sampler = GaussianSampler()
creating: createGaussianSampler</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.GradientReversal">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">GradientReversal</code><span class="sig-paren">(</span><em>the_lambda=1.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#GradientReversal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.GradientReversal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a simple module preserves the input, but takes the
gradient from the subsequent layer, multiplies it by -lambda
and passes it to the preceding layer. This can be used to maximise
an objective function whilst using gradient descent, as described in
[“Domain-Adversarial Training of Neural Networks”
(<a class="reference external" href="http://arxiv.org/abs/1505.07818">http://arxiv.org/abs/1505.07818</a>)]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambda</strong> – hyper-parameter lambda can be set dynamically during training</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gradientReversal</span> <span class="o">=</span> <span class="n">GradientReversal</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createGradientReversal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradientReversal</span> <span class="o">=</span> <span class="n">GradientReversal</span><span class="p">()</span>
<span class="go">creating: createGradientReversal</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.HardShrink">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">HardShrink</code><span class="sig-paren">(</span><em>the_lambda=0.5</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#HardShrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.HardShrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This is a transfer layer which applies the hard shrinkage function
element-wise to the input Tensor. The parameter lambda is set to 0.5
by default</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">x</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span>  <span class="k">lambda</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span>  <span class="n">x</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="k">lambda</span>
        <span class="mi">0</span><span class="p">,</span> <span class="n">otherwise</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>the_lambda</strong> – a threshold value whose default value is 0.5</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hardShrink</span> <span class="o">=</span> <span class="n">HardShrink</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createHardShrink</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.HardSigmoid">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">HardSigmoid</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#HardSigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.HardSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply Hard-sigmoid function</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="o">|</span>  <span class="mi">0</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">2.5</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">|</span>  <span class="mi">1</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mf">2.5</span>
       <span class="o">|</span>  <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">otherwise</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hardSigmoid</span> <span class="o">=</span> <span class="n">HardSigmoid</span><span class="p">()</span>
<span class="go">creating: createHardSigmoid</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.HardTanh">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">HardTanh</code><span class="sig-paren">(</span><em>min_value=-1.0</em>, <em>max_value=1.0</em>, <em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#HardTanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.HardTanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies HardTanh to each element of input, HardTanh is defined:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="o">|</span>  <span class="n">maxValue</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">maxValue</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">|</span>  <span class="n">minValue</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">minValue</span>
       <span class="o">|</span>  <span class="n">x</span><span class="p">,</span> <span class="n">otherwise</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>min_value</strong> – minValue in f(x), default is -1.</li>
<li><strong>max_value</strong> – maxValue in f(x), default is 1.</li>
<li><strong>inplace</strong> – whether enable inplace model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hardTanh</span> <span class="o">=</span> <span class="n">HardTanh</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createHardTanh</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardTanh</span> <span class="o">=</span> <span class="n">HardTanh</span><span class="p">()</span>
<span class="go">creating: createHardTanh</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Highway">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Highway</code><span class="sig-paren">(</span><em>size</em>, <em>with_bias=True</em>, <em>activation=None</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Highway"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Highway" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Densely connected highway network.
Highway layers are a natural extension of LSTMs to feedforward networks.</p>
<p>:param size input size
:param with_bias whether to include a bias
:param activation activation function. It can also be the name of an existing activation as a string.
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]], applied to the bias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">highway</span> <span class="o">=</span> <span class="n">Highway</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createHighway</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Identity">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Identity</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Identity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Identity just return the input to output.
It’s useful in same parallel container to get an origin input.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">identity</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
<span class="go">creating: createIdentity</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Index">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Index</code><span class="sig-paren">(</span><em>dimension</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Index"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Index" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the Tensor index operation along the given dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dimension</strong> – the dimension to be indexed</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">Index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createIndex</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.InferReshape">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">InferReshape</code><span class="sig-paren">(</span><em>size</em>, <em>batch_mode=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#InferReshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.InferReshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Reshape the input tensor with automatic size inference support.
Positive numbers in the <cite>size</cite> argument are used to reshape the input to the
corresponding dimension size.
There are also two special values allowed in <cite>size</cite>:
a. <cite>0</cite> means keep the corresponding dimension size of the input unchanged.
i.e., if the 1st dimension size of the input is 2,
the 1st dimension size of output will be set as 2 as well.
b. <cite>-1</cite> means infer this dimension size from other dimensions.
This dimension size is calculated by keeping the amount of output elements
consistent with the input.
Only one <cite>-1</cite> is allowable in <cite>size</cite>.</p>
<p>For example,
Input tensor with size: (4, 5, 6, 7)
-&gt; InferReshape(Array(4, 0, 3, -1))
Output tensor with size: (4, 5, 3, 14)
The 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,
and inferred the last dim as 14.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> – the target tensor size</li>
<li><strong>batch_mode</strong> – whether in batch mode</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inferReshape</span> <span class="o">=</span> <span class="n">InferReshape</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createInferReshape</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Input">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Input</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Input" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Node" title="bigdl.nn.layer.Node"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Node</span></code></a></p>
<p>Input layer do nothing to the input tensors, just passing them through. It is used as input to
the Graph container (add a link) when the first layer of the graph container accepts multiple
tensors as inputs.</p>
<p>Each input node of the graph container should accept one tensor as input. If you want a module
accepting multiple tensors as input, you should add some Input module before it and connect
the outputs of the Input nodes to it.</p>
<p>Please note that the return is not a layer but a Node containing input layer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">()</span>
<span class="go">creating: createInput</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.JoinTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">JoinTable</code><span class="sig-paren">(</span><em>dimension</em>, <em>n_input_dims</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#JoinTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.JoinTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a table module which takes a table of Tensors as input and
outputs a Tensor by joining them together along the dimension <cite>dimension</cite>.</p>
<p>The input to this layer is expected to be a tensor, or a batch of tensors;
when using mini-batch, a batch of sample tensors will be passed to the layer and
the user need to specify the number of dimensions of each sample tensor in the
batch using <cite>nInputDims</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> – to be join in this dimension</li>
<li><strong>nInputDims</strong> – specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">joinTable</span> <span class="o">=</span> <span class="n">JoinTable</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createJoinTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.L1Penalty">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">L1Penalty</code><span class="sig-paren">(</span><em>l1weight</em>, <em>size_average=False</em>, <em>provide_output=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#L1Penalty"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.L1Penalty" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>adds an L1 penalty to an input (for sparsity).
L1Penalty is an inline module that in its forward propagation copies the input Tensor
directly to the output, and computes an L1 loss of the latent state (input) and stores
it in the module’s loss field. During backward propagation: gradInput = gradOutput + gradLoss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>l1weight</strong> – </li>
<li><strong>sizeAverage</strong> – </li>
<li><strong>provideOutput</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l1Penalty</span> <span class="o">=</span> <span class="n">L1Penalty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createL1Penalty</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LSTM">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>p=0.0</em>, <em>activation=None</em>, <em>inner_activation=None</em>, <em>wRegularizer=None</em>, <em>uRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<div class="line-block">
<div class="line">Long Short Term Memory architecture.</div>
<div class="line">Ref.</div>
<div class="line">A.: <a class="reference external" href="http://arxiv.org/pdf/1303.5778v1">http://arxiv.org/pdf/1303.5778v1</a> (blueprint for this module)</div>
<div class="line">B. <a class="reference external" href="http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf">http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</a></div>
<div class="line">C. <a class="reference external" href="http://arxiv.org/pdf/1503.04069v1.pdf">http://arxiv.org/pdf/1503.04069v1.pdf</a></div>
<div class="line">D. <a class="reference external" href="https://github.com/wojzaremba/lstm">https://github.com/wojzaremba/lstm</a></div>
<div class="line">E. <a class="reference external" href="https://github.com/Element-Research/rnn/blob/master/FastLSTM.lua">https://github.com/Element-Research/rnn/blob/master/FastLSTM.lua</a></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>inputSize</strong> – the size of each input vector</li>
<li><strong>hiddenSize</strong> – Hidden unit size in the LSTM</li>
<li><strong>p</strong> – is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](<a class="reference external" href="http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A">http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A</a> Theoretically Grounded Application of Dropout in Recurrent Neural Networks](<a class="reference external" href="https://arxiv.org/pdf/1512.05287.pdf">https://arxiv.org/pdf/1512.05287.pdf</a>)</li>
<li><strong>activation</strong> – activation function, by default to be Tanh if not specified.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>It can also be the name of an existing activation as a string.
:param inner_activation: activation function for the inner cells, by default to be Sigmoid if not specified.
It can also be the name of an existing activation as a string.
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">Sigmoid</span><span class="p">(),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createSigmoid</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createTanh</span>
<span class="go">creating: createLSTM</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LSTMPeephole">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LSTMPeephole</code><span class="sig-paren">(</span><em>input_size=4</em>, <em>hidden_size=3</em>, <em>p=0.0</em>, <em>wRegularizer=None</em>, <em>uRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LSTMPeephole"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LSTMPeephole" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<div class="line-block">
<div class="line">Long Short Term Memory architecture with peephole.</div>
<div class="line">Ref. A.: <a class="reference external" href="http://arxiv.org/pdf/1303.5778v1">http://arxiv.org/pdf/1303.5778v1</a> (blueprint for this module)</div>
<div class="line">B. <a class="reference external" href="http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf">http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</a></div>
<div class="line">C. <a class="reference external" href="http://arxiv.org/pdf/1503.04069v1.pdf">http://arxiv.org/pdf/1503.04069v1.pdf</a></div>
<div class="line">D. <a class="reference external" href="https://github.com/wojzaremba/lstm">https://github.com/wojzaremba/lstm</a></div>
<div class="line">E. <a class="reference external" href="https://github.com/Element-Research/rnn/blob/master/LSTM.lua">https://github.com/Element-Research/rnn/blob/master/LSTM.lua</a></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – the size of each input vector</li>
<li><strong>hidden_size</strong> – Hidden unit size in the LSTM</li>
<li><strong>p</strong> – is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](<a class="reference external" href="http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A">http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A</a> Theoretically Grounded Application of Dropout in Recurrent Neural Networks](<a class="reference external" href="https://arxiv.org/pdf/1512.05287.pdf">https://arxiv.org/pdf/1512.05287.pdf</a>)</li>
<li><strong>wRegularizer</strong> – instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</li>
<li><strong>uRegularizer</strong> – instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</li>
<li><strong>bRegularizer</strong> – instance of [[Regularizer]]applied to the bias.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTMPeephole</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createLSTMPeephole</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Layer">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Layer</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="bigdl.util.html#bigdl.util.common.JavaValue" title="bigdl.util.common.JavaValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.util.common.JavaValue</span></code></a></p>
<p>Layer is the basic component of a neural network
and it’s also the base class of layers.
Layer can connect to others to construct a complex neural network.</p>
<dl class="method">
<dt id="bigdl.nn.layer.Layer.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>input</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>NB: It’s for debug only, please use optimizer.optimize() in production.
Performs a back-propagation step through the module, with respect to the given input. In
general this method makes the assumption forward(input) has been called before, with the same
input. This is necessary for optimization reasons. If you do not respect this rule, backward()
will compute incorrect gradients.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> – ndarray or list of ndarray or JTensor or list of JTensor.</li>
<li><strong>grad_output</strong> – ndarray or list of ndarray or JTensor or list of JTensor.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">ndarray or list of ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Layer.check_input">
<em class="property">static </em><code class="descname">check_input</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.check_input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.check_input" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> – ndarray or list of ndarray or JTensor or list of JTensor.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(list of JTensor, isTable)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Layer.convert_output">
<em class="property">static </em><code class="descname">convert_output</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.convert_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.convert_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>No argument passed in:
Evaluate the model to set train = false, useful when doing test/forward
:return: layer itself</p>
<p>Three arguments passed in:
A method to benchmark the model quality.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>val_rdd</strong> – the input data</li>
<li><strong>batch_size</strong> – batch size</li>
<li><strong>val_methods</strong> – a list of validation methods. i.e: Top1Accuracy,Top5Accuracy and Loss.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a list of the metrics result</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>NB: It’s for debug only, please use optimizer.optimize() in production.
Takes an input object, and computes the corresponding output of the module</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> – ndarray or list of ndarray</li>
<li><strong>input</strong> – ndarray or list of ndarray or JTensor or list of JTensor.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">ndarray or list of ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><em>names=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.freeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>freeze module, if names is not None, set an array of layers that match given names
to be freezed
:param names: an array of layer names
:return:</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.get_dtype">
<code class="descname">get_dtype</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.get_dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.get_dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.get_weights">
<code class="descname">get_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.get_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.get_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Get weights for this layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">list of numpy arrays which represent weight and bias</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.is_training">
<code class="descname">is_training</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.is_training"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.is_training" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Whether this layer is in the training mode</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">()</span>
<span class="go">creating: createDropout</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span><span class="o">.</span><span class="n">is_training</span><span class="p">()</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">training</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span><span class="o">.</span><span class="n">is_training</span><span class="p">()</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.is_with_weights">
<code class="descname">is_with_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.is_with_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.is_with_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.name">
<code class="descname">name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.name" title="Permalink to this definition">¶</a></dt>
<dd><p>Name of this layer</p>
</dd></dl>

<dl class="classmethod">
<dt id="bigdl.nn.layer.Layer.of">
<em class="property">classmethod </em><code class="descname">of</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.of"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.of" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a Python Layer base on the given java value
:param jvalue: Java object create by Py4j
:return: A Python Layer</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the model parameters which containing: weight, bias, gradBias, gradWeight</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">dict(layername -&gt; dict(parametername -&gt; ndarray))</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>features</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Model inference base on the given data.
:param features: it can be a ndarray or list of ndarray for locally inference
or RDD[Sample] for running in distributed fashion
:return: ndarray or RDD[Sample] depend on the the type of features.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict_class">
<code class="descname">predict_class</code><span class="sig-paren">(</span><em>features</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict_class"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Model inference base on the given data which returning label
:param features: it can be a ndarray or list of ndarray for locally inference
or RDD[Sample] for running in distributed fashion
:return: ndarray or RDD[Sample] depend on the the type of features.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict_class_distributed">
<code class="descname">predict_class_distributed</code><span class="sig-paren">(</span><em>data_rdd</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict_class_distributed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict_class_distributed" title="Permalink to this definition">¶</a></dt>
<dd><p>module predict, return the predict label</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data_rdd</strong> – the data to be predict.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">An RDD represent the predict label.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict_class_local">
<code class="descname">predict_class_local</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict_class_local"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict_class_local" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – X can be a ndarray or list of ndarray if the model has multiple inputs.</td>
</tr>
</tbody>
</table>
<p>The first dimension of X should be batch.
:return: a ndarray as the prediction result.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict_distributed">
<code class="descname">predict_distributed</code><span class="sig-paren">(</span><em>data_rdd</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict_distributed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict_distributed" title="Permalink to this definition">¶</a></dt>
<dd><p>Model inference base on the given data.
You need to invoke collect() to trigger those action         as the returning result is an RDD.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data_rdd</strong> – the data to be predict.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">An RDD represent the predict result.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict_image">
<code class="descname">predict_image</code><span class="sig-paren">(</span><em>image_frame</em>, <em>output_layer=None</em>, <em>share_buffer=False</em>, <em>batch_per_partition=4</em>, <em>predict_key='predict'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict_image"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict_image" title="Permalink to this definition">¶</a></dt>
<dd><p>model predict images, return imageFrame with predicted tensor
:param image_frame imageFrame that contains images
:param output_layer if output_layer is not null, the output of layer that matches
output_layer will be used as predicted output
:param share_buffer whether to share same memory for each batch predict results
:param batch_per_partition batch size per partition, default is 4
:param predict_key key to store predicted results</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.predict_local">
<code class="descname">predict_local</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.predict_local"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.predict_local" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – X can be a ndarray or list of ndarray if the model has multiple inputs.</td>
</tr>
</tbody>
</table>
<p>The first dimension of X should be batch.
:return: a ndarray as the prediction result.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.quantize">
<code class="descname">quantize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Clone self and quantize it, at last return a new quantized model.
:return: A new quantized model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fc</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fc</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,))])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">fc</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_fc</span> <span class="o">=</span> <span class="n">fc</span><span class="o">.</span><span class="n">quantize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_output</span> <span class="o">=</span> <span class="n">quantized_fc</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_quantized_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">quantized_output</span><span class="p">,</span> <span class="n">expected_quantized_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span><span class="p">(</span><span class="s2">&quot;quantized.Linear&quot;</span> <span class="ow">in</span> <span class="n">quantized_fc</span><span class="o">.</span><span class="fm">__str__</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">SpatialConvolution</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">creating: createSpatialConvolution</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,))])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_conv</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">quantize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_output</span> <span class="o">=</span> <span class="n">quantized_conv</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_quantized_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">quantized_output</span><span class="p">,</span> <span class="n">expected_quantized_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span><span class="p">(</span><span class="s2">&quot;quantized.SpatialConvolution&quot;</span> <span class="ow">in</span> <span class="n">quantized_conv</span><span class="o">.</span><span class="fm">__str__</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="go">creating: createSequential</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="kc">False</span><span class="p">))</span>
<span class="go">creating: createReshape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fc</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_seq</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">quantize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_output</span> <span class="o">=</span> <span class="n">quantized_seq</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_quantized_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">],</span> <span class="p">[</span><span class="mf">37.</span><span class="p">,</span> <span class="mf">37.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">quantized_output</span><span class="p">,</span> <span class="n">expected_quantized_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span><span class="p">(</span><span class="s2">&quot;quantized.Linear&quot;</span> <span class="ow">in</span> <span class="n">quantized_seq</span><span class="o">.</span><span class="fm">__str__</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span><span class="p">(</span><span class="s2">&quot;quantized.SpatialConvolution&quot;</span> <span class="ow">in</span> <span class="n">quantized_seq</span><span class="o">.</span><span class="fm">__str__</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the model weights.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em>, <em>over_write=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.saveModel">
<code class="descname">saveModel</code><span class="sig-paren">(</span><em>modelPath</em>, <em>weightPath=None</em>, <em>over_write=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.saveModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.saveModel" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.save_caffe">
<code class="descname">save_caffe</code><span class="sig-paren">(</span><em>prototxt_path</em>, <em>model_path</em>, <em>use_v2=True</em>, <em>overwrite=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.save_caffe"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.save_caffe" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.save_tensorflow">
<code class="descname">save_tensorflow</code><span class="sig-paren">(</span><em>inputs</em>, <em>path</em>, <em>byte_order='little_endian'</em>, <em>data_format='nhwc'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.save_tensorflow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.save_tensorflow" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a model to protobuf files so that it can be used in tensorflow inference.</p>
<p>When saving the model, placeholders will be added to the tf model as input nodes. So
you need to pass in the names and shapes of the placeholders. BigDL model doesn’t have
such information. The order of the placeholder information should be same as the inputs
of the graph model.
:param inputs: placeholder information, should be an array of tuples (input_name, shape)
where ‘input_name’ is a string and shape is an array of integer
:param path: the path to be saved to
:param byte_order: model byte order
:param data_format: model data format, should be “nhwc” or “nchw”</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.setBRegularizer">
<code class="descname">setBRegularizer</code><span class="sig-paren">(</span><em>bRegularizer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.setBRegularizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.setBRegularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>set bias regularizer
:param wRegularizer: bias regularizer
:return:</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.setWRegularizer">
<code class="descname">setWRegularizer</code><span class="sig-paren">(</span><em>wRegularizer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.setWRegularizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.setWRegularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>set weight regularizer
:param wRegularizer: weight regularizer
:return:</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.set_name">
<code class="descname">set_name</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.set_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.set_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Give this model a name. There would be a generated name
consist of class name and UUID if user doesn’t set it.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.set_running_mean">
<code class="descname">set_running_mean</code><span class="sig-paren">(</span><em>running_mean</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.set_running_mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.set_running_mean" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>running_mean</strong> – a ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.set_running_std">
<code class="descname">set_running_std</code><span class="sig-paren">(</span><em>running_std</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.set_running_std"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.set_running_std" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>running_mean</strong> – a ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.set_seed">
<code class="descname">set_seed</code><span class="sig-paren">(</span><em>seed=123</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.set_seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.set_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>You can control the random seed which used to init weights for this model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>seed</strong> – random seed</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Model itself.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.set_weights">
<code class="descname">set_weights</code><span class="sig-paren">(</span><em>weights</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.set_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Set weights for this layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>weights</strong> – a list of numpy arrays which represent weight and bias</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">linear</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
<span class="go">creating: createReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">py4j.protocol</span> <span class="k">import</span> <span class="n">Py4JJavaError</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">relu</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">])])</span>
<span class="gp">... </span><span class="k">except</span> <span class="n">Py4JJavaError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">java_exception</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">java.lang.IllegalArgumentException: requirement failed: this layer does not have weight/bias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="go">The layer does not have weight/bias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createAdd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">add</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])])</span>
<span class="gp">... </span><span class="k">except</span> <span class="n">Py4JJavaError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">java_exception</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">java.lang.IllegalArgumentException: requirement failed: the number of input weight/bias is not consistant with number of weight/bias of this layer, number of input 1, number of output 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cAdd</span> <span class="o">=</span> <span class="n">CAdd</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">creating: createCAdd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cAdd</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">cAdd</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.training">
<code class="descname">training</code><span class="sig-paren">(</span><em>is_training=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.training"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.training" title="Permalink to this definition">¶</a></dt>
<dd><p>Set this layer in the training mode or in predition mode if is_training=False</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><em>names=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.unfreeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>unfreeze module, if names is not None, unfreeze layers that match given names
:param names: an array of layer names
:return:</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.update_parameters">
<code class="descname">update_parameters</code><span class="sig-paren">(</span><em>learning_rate</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.update_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.update_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>NB: It’s for debug only, please use optimizer.optimize() in production.</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Layer.zero_grad_parameters">
<code class="descname">zero_grad_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Layer.zero_grad_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Layer.zero_grad_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>NB: It’s for debug only, please use optimizer.optimize() in production.
If the module has parameters, this will zero the accumulation of the gradients with respect
to these parameters. Otherwise, it does nothing.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LeakyReLU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LeakyReLU</code><span class="sig-paren">(</span><em>negval=0.01</em>, <em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LeakyReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a transfer module that applies LeakyReLU, which parameter negval sets the slope of the
negative part: LeakyReLU is defined as: f(x) = max(0, x) + negval * min(0, x)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>negval</strong> – sets the slope of the negative partl</li>
<li><strong>inplace</strong> – if it is true, doing the operation in-place without using extra state memory</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">leakyReLU</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createLeakyReLU</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Linear">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>with_bias=True</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>The [[Linear]] module applies a linear transformation to the input data,
i.e. <cite>y = Wx + b</cite>. The input given in <cite>forward(input)</cite> must be either
a vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must
have the size of <cite>inputSize</cite>. If it is a matrix, then each row is assumed to be
an input sample of given batch (the number of rows means the batch size and
the number of columns should be equal to the <cite>inputSize</cite>).</p>
<p>:param input_size the size the each input sample
:param output_size the size of the module output of each sample
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.
:param init_weight: the optional initial value for the weight
:param init_bias: the optional initial value for the bias
:param init_grad_weight: the optional initial value for the grad_weight
:param init_grad_bias: the optional initial value for the grad_bias</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">)</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createLinear</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Linear.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Linear.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Linear.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LocallyConnected1D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LocallyConnected1D</code><span class="sig-paren">(</span><em>n_input_frame</em>, <em>input_frame_size</em>, <em>output_frame_size</em>, <em>kernel_w</em>, <em>stride_w=1</em>, <em>propagate_back=True</em>, <em>weight_regularizer=None</em>, <em>bias_regularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LocallyConnected1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LocallyConnected1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>The <cite>LocallyConnected1D</cite> layer works similarly to
the <cite>TemporalConvolution</cite> layer, except that weights are unshared,
that is, a different set of filters is applied at each different patch
of the input.
The input tensor in <cite>forward(input)</cite> is expected to be a 2D tensor
(<cite>nInputFrame</cite> x <cite>inputFrameSize</cite>) or a 3D tensor
(<cite>nBatchFrame</cite> x <cite>nInputFrame</cite> x <cite>inputFrameSize</cite>).
:param nInputFrame the input frame channel
:param input_frame_size The input frame size expected in sequences given into <cite>forward()</cite>
:param output_frame_size The output frame size the convolution layer will produce.
:param kernel_w The kernel width of the convolution
:param stride_w The step of the convolution in the width dimension.
:param propagate_back Whether propagate gradient back, default is true.
:param weight_regularizer instance of [[Regularizer]]
(eg. L1 or L2 regularization), applied to the input weights matrices.
:param bias_regularizer instance of [[Regularizer]]
applied to the bias.
:param init_weight Initial weight
:param init_bias Initial bias
:param init_grad_weight Initial gradient weight
:param init_grad_bias Initial gradient bias
&gt;&gt;&gt; locallyConnected1D = LocallyConnected1D(10, 6, 12, 5, 5)
creating: createLocallyConnected1D
&gt;&gt;&gt; locallyConnected1D.setWRegularizer(L1Regularizer(0.5))
creating: createL1Regularizer
&gt;&gt;&gt; locallyConnected1D.setBRegularizer(L1Regularizer(0.5))
creating: createL1Regularizer</p>
<dl class="method">
<dt id="bigdl.nn.layer.LocallyConnected1D.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LocallyConnected1D.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LocallyConnected1D.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LocallyConnected2D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LocallyConnected2D</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>input_width</em>, <em>input_height</em>, <em>n_output_plane</em>, <em>kernel_w</em>, <em>kernel_h</em>, <em>stride_w=1</em>, <em>stride_h=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>propagate_back=True</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>with_bias=True</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LocallyConnected2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LocallyConnected2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>The LocallyConnected2D layer works similarly to the [[SpatialConvolution]] layer,
except that weights are unshared, that is, a different set of filters
is applied at each different patch of the input.</p>
<p>:param n_input_plane The number of expected input planes in the image given into forward()
:param input_width The expected width of input
:param input_height The expected height of input
:param n_output_plane The number of output planes the convolution layer will produce.
:param kernel_w The kernel width of the convolution
:param kernel_h The kernel height of the convolution
:param stride_w The step of the convolution in the width dimension.
:param stride_h The step of the convolution in the height dimension
:param pad_w The additional zeros added per width to the input planes.
:param pad_h The additional zeros added per height to the input planes.
:param propagate_back Propagate gradient back
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.
:param init_weight: the optional initial value for the weight
:param init_bias: the optional initial value for the bias
:param init_grad_weight: the optional initial value for the grad_weight
:param init_grad_bias: the optional initial value for the grad_bias
:param with_bias: the optional initial value for if need bias
:param data_format: a string value of “NHWC” or “NCHW” to specify the input data format of this layer. In “NHWC” format
data is stored in the order of [batch_size, height, width, channels], in “NCHW” format data is stored
in the order of [batch_size, channels, height, width].</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">locallyConnected2D</span> <span class="o">=</span> <span class="n">LocallyConnected2D</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="go">creating: createLocallyConnected2D</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">locallyConnected2D</span><span class="o">.</span><span class="n">setWRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">locallyConnected2D</span><span class="o">.</span><span class="n">setBRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.LocallyConnected2D.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LocallyConnected2D.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LocallyConnected2D.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Log">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Log</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Log" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the log function element-wise to the input Tensor,
thus outputting a Tensor of the same dimension.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">log</span> <span class="o">=</span> <span class="n">Log</span><span class="p">()</span>
<span class="go">creating: createLog</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LogSigmoid">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LogSigmoid</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LogSigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This class is a transform layer corresponding to the sigmoid function:
f(x) = Log(1 / (1 + e ^^ (-x)))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">logSigmoid</span> <span class="o">=</span> <span class="n">LogSigmoid</span><span class="p">()</span>
<span class="go">creating: createLogSigmoid</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LogSoftMax">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LogSoftMax</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LogSoftMax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LogSoftMax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the LogSoftMax function to an n-dimensional input Tensor.
LogSoftmax is defined as: f_i(x) = log(1 / a exp(x_i))
where a = sum_j[exp(x_j)].</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">logSoftMax</span> <span class="o">=</span> <span class="n">LogSoftMax</span><span class="p">()</span>
<span class="go">creating: createLogSoftMax</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LookupTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LookupTable</code><span class="sig-paren">(</span><em>n_index</em>, <em>n_output</em>, <em>padding_value=0.0</em>, <em>max_norm=1.7976931348623157e+308</em>, <em>norm_type=2.0</em>, <em>should_scale_grad_by_freq=False</em>, <em>wRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LookupTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LookupTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>a convolution of width 1, commonly used for word embeddings</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>wRegularizer</strong> – instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lookupTable</span> <span class="o">=</span> <span class="n">LookupTable</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createLookupTable</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.LookupTable.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LookupTable.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LookupTable.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.LookupTableSparse">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">LookupTableSparse</code><span class="sig-paren">(</span><em>n_index</em>, <em>n_output</em>, <em>combiner='sum'</em>, <em>max_norm=-1.0</em>, <em>wRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LookupTableSparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LookupTableSparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>LookupTable for multi-values.
Also called embedding_lookup_sparse in TensorFlow.</p>
<p>The input of LookupTableSparse should be a 2D SparseTensor or two 2D SparseTensors.
If the input is a SparseTensor, the values are positive integer ids,
values in each row of this SparseTensor will be turned into a dense vector.
If the input is two SparseTensors, the first tensor should be the integer ids, just
like the SparseTensor input. And the second tensor is the corresponding
weights of the integer ids.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>wRegularizer</strong> – instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lookupTableSparse</span> <span class="o">=</span> <span class="n">LookupTableSparse</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createLookupTableSparse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weightValues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="n">weightValues</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer1</span> <span class="o">=</span> <span class="n">LookupTableSparse</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="go">creating: createLookupTableSparse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer1</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="c1"># set weight to 1 to 40</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer1</span><span class="o">.</span><span class="n">forward</span><span class="p">([</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">6.5999999</span> <span class="p">,</span> <span class="mf">7.60000038</span><span class="p">,</span> <span class="mf">8.60000038</span><span class="p">,</span> <span class="mf">9.60000038</span><span class="p">],[</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.LookupTableSparse.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#LookupTableSparse.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.LookupTableSparse.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MM">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MM</code><span class="sig-paren">(</span><em>trans_a=False</em>, <em>trans_b=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Module to perform matrix multiplication on two mini-batch inputs, producing a mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>trans_a</strong> – specifying whether or not transpose the first input matrix</li>
<li><strong>trans_b</strong> – specifying whether or not transpose the second input matrix</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mM</span> <span class="o">=</span> <span class="n">MM</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createMM</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MV">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MV</code><span class="sig-paren">(</span><em>trans=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MV"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a module to perform matrix vector multiplication on two mini-batch inputs,
producing a mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>trans</strong> – whether make matrix transpose before multiplication</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mV</span> <span class="o">=</span> <span class="n">MV</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createMV</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MapTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MapTable</code><span class="sig-paren">(</span><em>module=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MapTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MapTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>This class is a container for a single module which will be applied
to all input elements. The member module is cloned as necessary to
process all input elements.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mapTable</span> <span class="o">=</span> <span class="n">MapTable</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="go">creating: createLinear</span>
<span class="go">creating: createMapTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MaskedSelect">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MaskedSelect</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MaskedSelect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MaskedSelect" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Performs a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument with
the input on the forward and backward passes.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">maskedSelect</span> <span class="o">=</span> <span class="n">MaskedSelect</span><span class="p">()</span>
<span class="go">creating: createMaskedSelect</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Masking">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Masking</code><span class="sig-paren">(</span><em>mask_value</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Masking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Masking" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Use a mask value to skip timesteps for a sequence</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">:</span><span class="n">param</span> <span class="n">mask_value</span><span class="p">:</span> <span class="n">mask</span> <span class="n">value</span>

 <span class="o">&gt;&gt;&gt;</span> <span class="n">masking</span> <span class="o">=</span> <span class="n">Masking</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
 <span class="n">creating</span><span class="p">:</span> <span class="n">createMasking</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Max">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Max</code><span class="sig-paren">(</span><em>dim</em>, <em>num_input_dims=-2147483648</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Max" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a max operation over dimension <cite>dim</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> – max along this dimension</li>
<li><strong>num_input_dims</strong> – Optional. If in a batch model, set to the inputDims.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">max</span> <span class="o">=</span> <span class="n">Max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createMax</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Maxout">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Maxout</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>maxout_number</em>, <em>with_bias=True</em>, <em>w_regularizer=None</em>, <em>b_regularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Maxout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Maxout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>A linear maxout layer Maxout layer select the element-wise maximum value of
maxoutNumber Linear(inputSize, outputSize) layers</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">:</span><span class="n">param</span> <span class="n">input_size</span><span class="p">:</span> <span class="n">the</span> <span class="n">size</span> <span class="n">the</span> <span class="n">each</span> <span class="nb">input</span> <span class="n">sample</span>
<span class="p">:</span><span class="n">param</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">the</span> <span class="n">size</span> <span class="n">of</span> <span class="n">the</span> <span class="n">module</span> <span class="n">output</span> <span class="n">of</span> <span class="n">each</span> <span class="n">sample</span>
<span class="p">:</span><span class="n">param</span> <span class="n">maxout_number</span><span class="p">:</span> <span class="n">number</span> <span class="n">of</span> <span class="n">Linear</span> <span class="n">layers</span> <span class="n">to</span> <span class="n">use</span>
<span class="p">:</span><span class="n">param</span> <span class="n">with_bias</span><span class="p">:</span> <span class="n">whether</span> <span class="n">use</span> <span class="n">bias</span> <span class="ow">in</span> <span class="n">Linear</span>
<span class="p">:</span><span class="n">param</span> <span class="n">w_regularizer</span><span class="p">:</span> <span class="n">instance</span> <span class="n">of</span> <span class="p">[[</span><span class="n">Regularizer</span><span class="p">]]</span>
      <span class="p">(</span><span class="n">eg</span><span class="o">.</span> <span class="n">L1</span> <span class="ow">or</span> <span class="n">L2</span> <span class="n">regularization</span><span class="p">),</span> <span class="n">applied</span> <span class="n">to</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">weights</span> <span class="n">matrices</span><span class="o">.</span>
<span class="p">:</span><span class="n">param</span> <span class="n">b_regularizer</span><span class="p">:</span> <span class="n">instance</span> <span class="n">of</span> <span class="p">[[</span><span class="n">Regularizer</span><span class="p">]]</span>
       <span class="n">applied</span> <span class="n">to</span> <span class="n">the</span> <span class="n">bias</span><span class="o">.</span>
<span class="p">:</span><span class="n">param</span> <span class="n">init_weight</span><span class="p">:</span> <span class="n">initial</span> <span class="n">weight</span>
<span class="p">:</span><span class="n">param</span> <span class="n">init_bias</span><span class="p">:</span> <span class="n">initial</span> <span class="n">bias</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">maxout</span> <span class="o">=</span> <span class="n">Maxout</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">creating</span><span class="p">:</span> <span class="n">createMaxout</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Mean">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Mean</code><span class="sig-paren">(</span><em>dimension=1</em>, <em>n_input_dims=-1</em>, <em>squeeze=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a simple layer which applies a mean operation over the given dimension. When nInputDims
is provided, the input will be considered as batches. Then the mean operation will be applied
in (dimension + 1). The input to this layer is expected to be a tensor, or a batch of
tensors; when using mini-batch, a batch of sample tensors will be passed to the layer and the
user need to specify the number of dimensions of each sample tensor in the batch using
nInputDims.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> – the dimension to be applied mean operation</li>
<li><strong>n_input_dims</strong> – specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimension would be consideredas batch size</li>
<li><strong>squeeze</strong> – default is true, which will squeeze the sum dimension; set it to false to keep the sum dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createMean</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Min">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Min</code><span class="sig-paren">(</span><em>dim=1</em>, <em>num_input_dims=-2147483648</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Min"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Min" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a min operation over dimension <cite>dim</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> – min along this dimension</li>
<li><strong>num_input_dims</strong> – Optional. If in a batch model, set to the input_dim.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">min</span> <span class="o">=</span> <span class="n">Min</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createMin</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MixtureTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MixtureTable</code><span class="sig-paren">(</span><em>dim=2147483647</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MixtureTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MixtureTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Creates a module that takes a table {gater, experts} as input and outputs the mixture of experts
(a Tensor or table of Tensors) using a gater Tensor. When dim is provided, it specifies the
dimension of the experts Tensor that will be interpolated (or mixed). Otherwise, the experts
should take the form of a table of Tensors. This Module works for experts of dimension 1D or
more, and for a 1D or 2D gater, i.e. for single examples or mini-batches.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mixtureTable</span> <span class="o">=</span> <span class="n">MixtureTable</span><span class="p">()</span>
<span class="go">creating: createMixtureTable</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mixtureTable</span> <span class="o">=</span> <span class="n">MixtureTable</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="go">creating: createMixtureTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Model">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Model</code><span class="sig-paren">(</span><em>inputs</em>, <em>outputs</em>, <em>is_keras=False</em>, <em>jvalue=None</em>, <em>bigdl_type='float'</em>, <em>byte_order='little_endian'</em>, <em>model_type='bigdl'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>A graph container. Each node can have multiple inputs. The output of the node should be a
tensor. The output tensor can be connected to multiple nodes. So the module in each node can
have a tensor or table input, and should have a tensor output.</p>
<p>The graph container can have multiple inputs and multiple outputs. If there’s one input,
the input data fed to the graph module should be a tensor. If there’re multiple inputs,
the input data fed to the graph module should be a table, which is actually an sequence of
tensor. The order of the input tensors should be same with the order of the input nodes.
This is also applied to the gradient from the module in the back propagation.</p>
<p>If there’s one output, the module output is a tensor. If there’re multiple outputs, the module
output is a table, which is actually an sequence of tensor. The order of the output tensors is
same with the order of the output modules. This is also applied to the gradient passed to the
module in the back propagation.</p>
<p>All inputs should be able to connect to outputs through some paths in the graph.
It is allowed that some successors of the inputs node are not connect to outputs.
If so, these nodes will be excluded in the computation.</p>
<p>We also support initializing a Graph directly from a tensorflow module. In this case, you should
pass your tensorflow nodes as inputs and outputs and also specify the byte_order parameter (“little_endian”
or “big_endian”) and node_type parameter (“bigdl” or “tensorflow”)
node_type parameter.</p>
<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.from_jvalue">
<em class="property">static </em><code class="descname">from_jvalue</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.from_jvalue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.from_jvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a Python Model base on the given java value
:param jvalue: Java object create by Py4j
:return: A Python Model</p>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.load">
<em class="property">static </em><code class="descname">load</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Bigdl model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> – The path containing the pre-trained model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A pre-trained model.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.loadModel">
<em class="property">static </em><code class="descname">loadModel</code><span class="sig-paren">(</span><em>weightPath=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.loadModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.loadModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Bigdl model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> – The path containing the pre-trained model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A pre-trained model.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.load_caffe">
<em class="property">static </em><code class="descname">load_caffe</code><span class="sig-paren">(</span><em>defPath</em>, <em>modelPath</em>, <em>match_all=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.load_caffe"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.load_caffe" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Caffe model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>model</strong> – A bigdl model definition which equivalent to the pre-trained caffe model.</li>
<li><strong>defPath</strong> – The path containing the caffe model definition.</li>
<li><strong>modelPath</strong> – The path containing the pre-trained caffe model.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A pre-trained model.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.load_caffe_model">
<em class="property">static </em><code class="descname">load_caffe_model</code><span class="sig-paren">(</span><em>modelPath</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.load_caffe_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.load_caffe_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Caffe model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>defPath</strong> – The path containing the caffe model definition.</li>
<li><strong>modelPath</strong> – The path containing the pre-trained caffe model.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A pre-trained model.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.load_keras">
<em class="property">static </em><code class="descname">load_keras</code><span class="sig-paren">(</span><em>hdf5_path=None</em>, <em>by_name=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.load_keras"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.load_keras" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Keras model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>json_path</strong> – The json path containing the keras model definition.</li>
<li><strong>hdf5_path</strong> – The HDF5 path containing the pre-trained keras model weights with or without the model architecture.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A bigdl model.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.load_tensorflow">
<em class="property">static </em><code class="descname">load_tensorflow</code><span class="sig-paren">(</span><em>inputs</em>, <em>outputs</em>, <em>byte_order='little_endian'</em>, <em>bin_file=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.load_tensorflow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.load_tensorflow" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Tensorflow model.
:param path: The path containing the pre-trained model.
:param inputs: The input node of this graph
:param outputs: The output node of this graph
:param byte_order: byte_order of the file, <cite>little_endian</cite> or <cite>big_endian</cite>
:param bin_file: the optional bin file produced by bigdl dump_model util function to store the weights
:return: A pre-trained model.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.load_torch">
<em class="property">static </em><code class="descname">load_torch</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.load_torch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.load_torch" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a pre-trained Torch model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> – The path containing the pre-trained model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A pre-trained model.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Model.node">
<code class="descname">node</code><span class="sig-paren">(</span><em>name</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.node"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.node" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the corresponding node has the given name. If the given name doesn’t match any node,
an exception will be thrown
:param name: node name
:param bigdl_type: 
:return:</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Model.save_graph_topology">
<code class="descname">save_graph_topology</code><span class="sig-paren">(</span><em>log_path</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.save_graph_topology"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.save_graph_topology" title="Permalink to this definition">¶</a></dt>
<dd><p>save current model graph to a folder, which can be display in tensorboard by running
tensorboard –logdir logPath
:param log_path: path to save the model graph
:param bigdl_type:
:return:</p>
</dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Model.stop_gradient">
<code class="descname">stop_gradient</code><span class="sig-paren">(</span><em>stop_layers</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.stop_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.stop_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>stop the input gradient of layers that match the given <code class="docutils literal notranslate"><span class="pre">`names`</span></code>
their input gradient are not computed.
And they will not contributed to the input gradient computation of
layers that depend on them.
:param stop_layers:  an array of layer names
:param bigdl_type:
:return:</p>
</dd></dl>

<dl class="staticmethod">
<dt id="bigdl.nn.layer.Model.train">
<em class="property">static </em><code class="descname">train</code><span class="sig-paren">(</span><em>data</em>, <em>label</em>, <em>opt_method</em>, <em>criterion</em>, <em>batch_size</em>, <em>end_when</em>, <em>session=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Model.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Model.train" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Mul">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Mul</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Mul"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Mul" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Multiply a single scalar factor to the incoming data</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mul</span> <span class="o">=</span> <span class="n">Mul</span><span class="p">()</span>
<span class="go">creating: createMul</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Mul.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Mul.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Mul.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MulConstant">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MulConstant</code><span class="sig-paren">(</span><em>scalar</em>, <em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MulConstant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MulConstant" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Multiplies input Tensor by a (non-learnable) scalar constant.
This module is sometimes useful for debugging purposes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>scalar</strong> – scalar constant</li>
<li><strong>inplace</strong> – Can optionally do its operation in-place without using extra state memory</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mulConstant</span> <span class="o">=</span> <span class="n">MulConstant</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="go">creating: createMulConstant</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.MultiRNNCell">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">MultiRNNCell</code><span class="sig-paren">(</span><em>cells</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#MultiRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.MultiRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>A cell that enables stack multiple simple rnn cells</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cells</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cells</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ConvLSTMPeephole3D</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">creating: createConvLSTMPeephole3D</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cells</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ConvLSTMPeephole3D</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">creating: createConvLSTMPeephole3D</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stacked_convlstm</span> <span class="o">=</span> <span class="n">MultiRNNCell</span><span class="p">(</span><span class="n">cells</span><span class="p">)</span>
<span class="go">creating: createMultiRNNCell</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Narrow">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Narrow</code><span class="sig-paren">(</span><em>dimension</em>, <em>offset</em>, <em>length=1</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Narrow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Narrow is application of narrow operation in a module.
The module further supports a negative length in order to handle inputs with an unknown size.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">narrow</span> <span class="o">=</span> <span class="n">Narrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createNarrow</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.NarrowTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">NarrowTable</code><span class="sig-paren">(</span><em>offset</em>, <em>length=1</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#NarrowTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.NarrowTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Creates a module that takes a table as input and outputs the subtable starting at index
offset having length elements (defaults to 1 element). The elements can be either
a table or a Tensor. If <cite>length</cite> is negative, it means selecting the elements from the
offset to element which located at the abs(<cite>length</cite>) to the last element of the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>offset</strong> – the start index of table</li>
<li><strong>length</strong> – the length want to select</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">narrowTable</span> <span class="o">=</span> <span class="n">NarrowTable</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createNarrowTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Negative">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Negative</code><span class="sig-paren">(</span><em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Negative"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Negative" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Create an Negative layer.  Computing negative value of each element of input tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – if output tensor reuse input tensor storage. Default value is false</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">negative</span> <span class="o">=</span> <span class="n">Negative</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createNegative</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.NegativeEntropyPenalty">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">NegativeEntropyPenalty</code><span class="sig-paren">(</span><em>beta=0.01</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#NegativeEntropyPenalty"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.NegativeEntropyPenalty" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Penalize the input multinomial distribution if it has low entropy.
The input to this layer should be a batch of vector each representing a
multinomial distribution. The input is typically the output of a softmax layer.</p>
<p>For forward, the output is the same as input and a NegativeEntropy loss of
the latent state will be calculated each time. For backward,
gradInput = gradOutput + gradLoss</p>
<p>This can be used in reinforcement learning to discourage the policy from
collapsing to a single action for a given state, which improves exploration.
See the A3C paper for more detail (<a class="reference external" href="https://arxiv.org/pdf/1602.01783.pdf">https://arxiv.org/pdf/1602.01783.pdf</a>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ne</span> <span class="o">=</span> <span class="n">NegativeEntropyPenalty</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="go">creating: createNegativeEntropyPenalty</span>
</pre></div>
</div>
<p>:param beta penalty coefficient</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Node">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Node</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Node"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Node" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="bigdl.util.html#bigdl.util.common.JavaValue" title="bigdl.util.common.JavaValue"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.util.common.JavaValue</span></code></a></p>
<p>Represent a node in a graph. The connections between nodes are directed.</p>
<dl class="method">
<dt id="bigdl.nn.layer.Node.element">
<code class="descname">element</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Node.element"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Node.element" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="bigdl.nn.layer.Node.of">
<em class="property">classmethod </em><code class="descname">of</code><span class="sig-paren">(</span><em>jvalue</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Node.of"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Node.of" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Node.remove_next_edges">
<code class="descname">remove_next_edges</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Node.remove_next_edges"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Node.remove_next_edges" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="bigdl.nn.layer.Node.remove_pre_edges">
<code class="descname">remove_pre_edges</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Node.remove_pre_edges"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Node.remove_pre_edges" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Normalize">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Normalize</code><span class="sig-paren">(</span><em>p</em>, <em>eps=1e-10</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Normalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents
division by zero when the input contains all zero elements (default = 1e-10).
p can be the max value of double</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">normalize</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createNormalize</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.NormalizeScale">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">NormalizeScale</code><span class="sig-paren">(</span><em>p</em>, <em>scale</em>, <em>size</em>, <em>w_regularizer=None</em>, <em>eps=1e-10</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#NormalizeScale"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.NormalizeScale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>NormalizeScale is conposed of normalize and scale, this is equal to caffe Normalize layer
:param p L_p norm
:param eps smoothing parameter
:param scale scale parameter
:param size size of scale input
:param w_regularizer weight regularizer
&gt;&gt;&gt; layer = NormalizeScale(2.0, scale = 20.0, size = [1, 5, 1, 1])
creating: createNormalizeScale</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.PReLU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>n_output_plane=0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#PReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies parametric ReLU, which parameter varies the slope of the negative part.</p>
<p>PReLU: f(x) = max(0, x) + a * min(0, x)</p>
<p>nOutputPlane’s default value is 0, that means using PReLU in shared version and has
only one parameters.</p>
<p>Notice: Please don’t use weight decay on this.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n_output_plane</strong> – input map number. Default is 0.</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pReLU</span> <span class="o">=</span> <span class="n">PReLU</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createPReLU</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.PReLU.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#PReLU.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.PReLU.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Pack">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Pack</code><span class="sig-paren">(</span><em>dimension</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Pack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Pack" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Stacks a list of n-dimensional tensors into one (n+1)-dimensional tensor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createPack</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Padding">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Padding</code><span class="sig-paren">(</span><em>dim</em>, <em>pad</em>, <em>n_input_dim</em>, <em>value=0.0</em>, <em>n_index=1</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Padding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This module adds pad units of padding to dimension dim of the input. If pad is negative,
padding is added to the left, otherwise, it is added to the right of the dimension.</p>
<p>The input to this layer is expected to be a tensor, or a batch of tensors;
when using mini-batch, a batch of sample tensors will be passed to the layer and
the user need to specify the number of dimensions of each sample tensor in the
batch using n_input_dim.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> – the dimension to be applied padding operation</li>
<li><strong>pad</strong> – num of the pad units</li>
<li><strong>n_input_dim</strong> – specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</li>
<li><strong>value</strong> – padding value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">padding</span> <span class="o">=</span> <span class="n">Padding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createPadding</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.PairwiseDistance">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">PairwiseDistance</code><span class="sig-paren">(</span><em>norm=2</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#PairwiseDistance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.PairwiseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a module that takes a table of two vectors as input and outputs
the distance between them using the p-norm.
The input given in <cite>forward(input)</cite> is a [[Table]] that contains two tensors which
must be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,
it must have the size of <cite>inputSize</cite>. If it is a matrix, then each row is assumed to be
an input sample of the given batch (the number of rows means the batch size and
the number of columns should be equal to the <cite>inputSize</cite>).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>norm</strong> – the norm of distance</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pairwiseDistance</span> <span class="o">=</span> <span class="n">PairwiseDistance</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createPairwiseDistance</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ParallelTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ParallelTable</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ParallelTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ParallelTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>It is a container module that applies the i-th member module to the i-th
input, and outputs an output in the form of Table</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">parallelTable</span> <span class="o">=</span> <span class="n">ParallelTable</span><span class="p">()</span>
<span class="go">creating: createParallelTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Power">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Power</code><span class="sig-paren">(</span><em>power</em>, <em>scale=1.0</em>, <em>shift=0.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Power"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Power" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply an element-wise power operation with scale and shift.
f(x) = (shift + scale * x)^power^</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>power</strong> – the exponent.</li>
<li><strong>scale</strong> – Default is 1.</li>
<li><strong>shift</strong> – Default is 0.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">power</span> <span class="o">=</span> <span class="n">Power</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createPower</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.PriorBox">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">PriorBox</code><span class="sig-paren">(</span><em>min_sizes</em>, <em>max_sizes=None</em>, <em>aspect_ratios=None</em>, <em>is_flip=True</em>, <em>is_clip=False</em>, <em>variances=None</em>, <em>offset=0.5</em>, <em>img_h=0</em>, <em>img_w=0</em>, <em>img_size=0</em>, <em>step_h=0.0</em>, <em>step_w=0.0</em>, <em>step=0.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#PriorBox"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.PriorBox" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Generate the prior boxes of designated sizes and aspect ratios across
all dimensions (H * W)
Intended for use with MultiBox detection method to generate prior
:param min_sizes minimum box size in pixels. can be multiple. required!
:param max_sizes maximum box size in pixels. can be ignored or same as the # of min_size.
:param aspect_ratios optional aspect ratios of the boxes. can be multiple
:param is_flip optional bool, default true. if set, flip the aspect ratio.
:param is_clip whether to clip the prior’s coordidate such that it is within [0, 1]
&gt;&gt;&gt; layer = PriorBox([0.1])
creating: createPriorBox</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Proposal">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Proposal</code><span class="sig-paren">(</span><em>pre_nms_topn</em>, <em>post_nms_topn</em>, <em>ratios</em>, <em>scales</em>, <em>rpn_pre_nms_topn_train=12000</em>, <em>rpn_post_nms_topn_train=2000</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Proposal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Proposal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Outputs object detection proposals by applying estimated bounding-box
transformations to a set of regular boxes (called “anchors”).
rois: holds R regions of interest, each is a 5-tuple
(n, x1, y1, x2, y2) specifying an image batch index n and a rectangle (x1, y1, x2, y2)
scores: holds scores for R regions of interest
&gt;&gt;&gt; layer = Proposal(1000, 200, [0.1, 0.2], [2.0, 3.0])
creating: createProposal</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.RReLU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">RReLU</code><span class="sig-paren">(</span><em>lower=0.125</em>, <em>upper=0.3333333333333333</em>, <em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#RReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.RReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,
thus outputting a Tensor of the same dimension. Informally the RReLU is also known as
‘insanity’ layer. RReLU is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="n">where</span> <span class="n">a</span> <span class="o">~</span> <span class="n">U</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>In training mode negative inputs are multiplied by a factor drawn from a uniform random
distribution U(l, u).</p>
<p>In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean factor
a = (l + u) / 2.</p>
<p>By default, l = 1/8 and u = 1/3. If l == u a RReLU effectively becomes a LeakyReLU.</p>
<p>Regardless of operating in in-place mode a RReLU will internally allocate an input-sized
noise tensor to store random factors for negative inputs.</p>
<p>The backward() operation assumes that forward() has been called before.</p>
<p>For reference see [Empirical Evaluation of Rectified Activations in Convolutional Network](
<a class="reference external" href="http://arxiv.org/abs/1505.00853">http://arxiv.org/abs/1505.00853</a>).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>lower</strong> – lower boundary of uniform random distribution</li>
<li><strong>upper</strong> – upper boundary of uniform random distribution</li>
<li><strong>inplace</strong> – optionally do its operation in-place without using extra state memory</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rReLU</span> <span class="o">=</span> <span class="n">RReLU</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createRReLU</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ReLU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>ip=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the rectified linear unit (ReLU) function element-wise to the input Tensor,
thus outputting a Tensor of the same dimension.</p>
<p>ReLU is defined as: f(x) = max(0, x)
Can optionally do its operation in-place without using extra state memory</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
<span class="go">creating: createReLU</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ReLU6">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ReLU6</code><span class="sig-paren">(</span><em>inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ReLU6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Same as ReLU except that the rectifying function f(x) saturates at x = 6</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – either True = in-place or False = keeping separate state</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reLU6</span> <span class="o">=</span> <span class="n">ReLU6</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createReLU6</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Recurrent">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Recurrent</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Recurrent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Recurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>Recurrent module is a container of rnn cells
Different types of rnn cells can be added using add() function</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">recurrent</span> <span class="o">=</span> <span class="n">Recurrent</span><span class="p">()</span>
<span class="go">creating: createRecurrent</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.Recurrent.get_hidden_state">
<code class="descname">get_hidden_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Recurrent.get_hidden_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Recurrent.get_hidden_state" title="Permalink to this definition">¶</a></dt>
<dd><p>get hidden state and cell at last time step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">list of hidden state and cell</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.RecurrentDecoder">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">RecurrentDecoder</code><span class="sig-paren">(</span><em>output_length</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#RecurrentDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.RecurrentDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Recurrent" title="bigdl.nn.layer.Recurrent"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Recurrent</span></code></a></p>
<p>RecurrentDecoder module is a container of rnn cells which used to make
a prediction of the next timestep based on the prediction we made from
the previous timestep. Input for RecurrentDecoder is dynamically composed
during training. input at t(i) is output at t(i-1), input at t(0) is
user input, and user input has to be batch x stepShape(shape of the input
at a single time step).</p>
<p>Different types of rnn cells can be added using add() function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">recurrent_decoder</span> <span class="o">=</span> <span class="n">RecurrentDecoder</span><span class="p">(</span><span class="n">output_length</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="go">creating: createRecurrentDecoder</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Replicate">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Replicate</code><span class="sig-paren">(</span><em>n_features</em>, <em>dim=1</em>, <em>n_dim=2147483647</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Replicate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Replicate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Replicate repeats input <cite>nFeatures</cite> times along its <cite>dim</cite> dimension.
Notice: No memory copy, it set the stride along the <cite>dim</cite>-th dimension to zero.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_features</strong> – replicate times.</li>
<li><strong>dim</strong> – dimension to be replicated.</li>
<li><strong>n_dim</strong> – specify the number of non-batch dimensions.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replicate</span> <span class="o">=</span> <span class="n">Replicate</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createReplicate</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Reshape">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Reshape</code><span class="sig-paren">(</span><em>size</em>, <em>batch_mode=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>The forward(input) reshape the input tensor into a size(0) * size(1) * … tensor, taking the
elements row-wise.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size</strong> – the reshape size</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reshape</span> <span class="o">=</span> <span class="n">Reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="go">creating: createReshape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reshape</span> <span class="o">=</span> <span class="n">Reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span> <span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createReshape</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.ResizeBilinear">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">ResizeBilinear</code><span class="sig-paren">(</span><em>output_height</em>, <em>output_width</em>, <em>align_corner=False</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#ResizeBilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.ResizeBilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Resize the input image with bilinear interpolation. The input image must be a float tensor with
NHWC or NCHW layout</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_height</strong> – output height</li>
<li><strong>output_width</strong> – output width</li>
<li><strong>align_corner</strong> – align corner or not</li>
<li><strong>data_format</strong> – the data format of the input image, NHWC or NCHW</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">resizeBilinear</span> <span class="o">=</span> <span class="n">ResizeBilinear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
<span class="go">creating: createResizeBilinear</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Reverse">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Reverse</code><span class="sig-paren">(</span><em>dimension=1</em>, <em>is_inplace=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Reverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Reverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Reverse the input w.r.t given dimension.
The input can be a Tensor or Table.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> – </td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reverse</span> <span class="o">=</span> <span class="n">Reverse</span><span class="p">()</span>
<span class="go">creating: createReverse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reverse</span> <span class="o">=</span> <span class="n">Reverse</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="go">creating: createReverse</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.RnnCell">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">RnnCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>activation</em>, <em>isInputWithBias=True</em>, <em>isHiddenWithBias=True</em>, <em>wRegularizer=None</em>, <em>uRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#RnnCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.RnnCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a simple RNN. User can pass an activation function to the RNN.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – the size of each input vector</li>
<li><strong>hidden_size</strong> – Hidden unit size in simple RNN</li>
<li><strong>activation</strong> – activation function. It can also be the name of an existing activation as a string.</li>
<li><strong>isInputWithBias</strong> – boolean</li>
<li><strong>isHiddenWithBias</strong> – boolean</li>
<li><strong>wRegularizer</strong> – instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</li>
<li><strong>uRegularizer</strong> – instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</li>
<li><strong>bRegularizer</strong> – instance of [[Regularizer]](../regularizers.md),applied to the bias.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">RnnCell</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">(),</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createTanh</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createRnnCell</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.RoiPooling">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">RoiPooling</code><span class="sig-paren">(</span><em>pooled_w</em>, <em>pooled_h</em>, <em>spatial_scale</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#RoiPooling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.RoiPooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Region of interest pooling
The RoIPooling uses max pooling to convert the features inside any valid region of interest
into a small feature map with a fixed spatial extent of pooledH * pooledW (e.g., 7 * 7)
an RoI is a rectangular window into a conv feature map.
Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its
top-left corner (x1, y1) and its bottom-right corner (x2, y2).
RoI max pooling works by dividing the h * w RoI window into an pooledH * pooledW grid of
sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window
into the corresponding output grid cell.
Pooling is applied independently to each feature map channel</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pooled_w</strong> – spatial extent in width</li>
<li><strong>pooled_h</strong> – spatial extent in height</li>
<li><strong>spatial_scale</strong> – spatial scale</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_rois</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">RoiPooling</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span>
<span class="go">creating: createRoiPooling</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">([</span><span class="n">input_data</span><span class="p">,</span><span class="n">input_rois</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SReLU">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SReLU</code><span class="sig-paren">(</span><em>input_shape</em>, <em>share_axes=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>S-shaped Rectified Linear Unit.</p>
<p>It follows:
<cite>f(x) = t^r + a^r(x - t^r) for x &gt;= t^r</cite>,
<cite>f(x) = x for t^r &gt; x &gt; t^l</cite>,
<cite>f(x) = t^l + a^l(x - t^l) for x &lt;= t^l</cite>.</p>
<p># References
- [Deep Learning with S-shaped Rectified Linear Activation Units](<a class="reference external" href="http://arxiv.org/abs/1512.07030">http://arxiv.org/abs/1512.07030</a>)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input_shape</strong> – shape for tleft, aleft, tright, aright.</td>
</tr>
</tbody>
</table>
<p>E.g. for a 4-D input, the shape is the last 3-D
:param shared_axes: the axes along which to share learnable
parameters for the activation function.
For example, if the incoming feature maps
are from a 2D convolution
with output shape <cite>(batch, height, width, channels)</cite>,
and you wish to share parameters across space
so that each filter only has one set of parameters,
set <cite>shared_axes=[1, 2]</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">srelu</span> <span class="o">=</span> <span class="n">SReLU</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">creating: createSReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srelu</span> <span class="o">=</span> <span class="n">SReLU</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">creating: createSReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.nn.initialization_method</span> <span class="k">import</span> <span class="n">Xavier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span> <span class="o">=</span> <span class="n">Xavier</span><span class="p">()</span>
<span class="go">creating: createXavier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srelu</span> <span class="o">=</span> <span class="n">srelu</span><span class="o">.</span><span class="n">set_init_method</span><span class="p">(</span><span class="n">tLeftInit</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">aLeftInit</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">tRightInit</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">aRightInit</span><span class="o">=</span><span class="n">init</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SReLU.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>tLeftInit=None</em>, <em>aLeftInit=None</em>, <em>tRightInit=None</em>, <em>aRightInit=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SReLU.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SReLU.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Scale">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Scale</code><span class="sig-paren">(</span><em>size</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Scale"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Scale is the combination of CMul and CAdd
Computes the elementwise product of input and weight, with the shape of the weight “expand” to
match the shape of the input.
Similarly, perform a expand cdd bias and perform an elementwise add</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size</strong> – size of weight and bias</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">Scale</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="go">creating: createScale</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Select">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Select" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>A Simple layer selecting an index of the input tensor in the given dimension</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> – the dimension to select</li>
<li><strong>index</strong> – the index of the dimension to be selected</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">select</span> <span class="o">=</span> <span class="n">Select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSelect</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SelectTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SelectTable</code><span class="sig-paren">(</span><em>index</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SelectTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SelectTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Creates a module that takes a table as input and outputs the element at index <cite>index</cite>
(positive or negative). This can be either a table or a Tensor.
The gradients of the non-index elements are zeroed Tensors of the same size.
This is true regardless of the depth of the encapsulated Tensor as the function used
internally to do so is recursive.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> – the index to be selected</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">selectTable</span> <span class="o">=</span> <span class="n">SelectTable</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSelectTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Sequential">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>bigdl_type='float'</em>, <em>is_keras=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Container" title="bigdl.nn.layer.Container"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Container</span></code></a></p>
<p>Sequential provides a means to plug layers together
in a feed-forward fully connected manner.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">echo</span> <span class="o">=</span> <span class="n">Echo</span><span class="p">()</span>
<span class="go">creating: createEcho</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="go">creating: createSequential</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">echo</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Sigmoid">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Sigmoid</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the Sigmoid function element-wise to the input Tensor,
thus outputting a Tensor of the same dimension.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
<span class="go">creating: createSigmoid</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SoftMax">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SoftMax</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SoftMax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SoftMax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the
elements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.
Softmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)
where shift = max_i(x_i).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softMax</span> <span class="o">=</span> <span class="n">SoftMax</span><span class="p">()</span>
<span class="go">creating: createSoftMax</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SoftMin">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SoftMin</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SoftMin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SoftMin" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the
elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.
Softmin is defined as: f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)
where shift = max_i(-x_i).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softMin</span> <span class="o">=</span> <span class="n">SoftMin</span><span class="p">()</span>
<span class="go">creating: createSoftMin</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SoftPlus">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SoftPlus</code><span class="sig-paren">(</span><em>beta=1.0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SoftPlus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SoftPlus" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply the SoftPlus function to an n-dimensional input tensor.
SoftPlus function: f_i(x) = 1/beta * log(1 + exp(beta * x_i))</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>beta</strong> – Controls sharpness of transfer function</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softPlus</span> <span class="o">=</span> <span class="n">SoftPlus</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createSoftPlus</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SoftShrink">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SoftShrink</code><span class="sig-paren">(</span><em>the_lambda=0.5</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SoftShrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SoftShrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply the soft shrinkage function element-wise to the input Tensor</p>
<p>SoftShrinkage operator:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="o">|</span> <span class="n">x</span> <span class="o">-</span> <span class="k">lambda</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span>  <span class="k">lambda</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">|</span> <span class="n">x</span> <span class="o">+</span> <span class="k">lambda</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="k">lambda</span>
       <span class="o">|</span> <span class="mi">0</span><span class="p">,</span> <span class="n">otherwise</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>the_lambda</strong> – lambda, default is 0.5</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softShrink</span> <span class="o">=</span> <span class="n">SoftShrink</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="go">creating: createSoftShrink</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SoftSign">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SoftSign</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SoftSign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SoftSign" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply SoftSign function to an n-dimensional input Tensor.</p>
<p>SoftSign function: f_i(x) = x_i / (1+|x_i|)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">softSign</span> <span class="o">=</span> <span class="n">SoftSign</span><span class="p">()</span>
<span class="go">creating: createSoftSign</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SparseJoinTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SparseJoinTable</code><span class="sig-paren">(</span><em>dimension</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SparseJoinTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SparseJoinTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>:: Experimental</p>
<p>Sparse version of JoinTable. Backward just pass the origin gradOutput back to
the next layers without split. So this layer may just works in Wide&amp;Deep like models.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dimension</strong> – to be join in this dimension</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">joinTable</span> <span class="o">=</span> <span class="n">SparseJoinTable</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSparseJoinTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SparseLinear">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SparseLinear</code><span class="sig-paren">(</span><em>input_size</em>, <em>output_size</em>, <em>with_bias=True</em>, <em>backwardStart=-1</em>, <em>backwardLength=-1</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SparseLinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SparseLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>SparseLinear is the sparse version of module Linear. SparseLinear has two different from Linear:
firstly, SparseLinear’s input Tensor is a SparseTensor. Secondly, SparseLinear doesn’t backward
gradient to next layer in the backpropagation by default, as the gradInput of SparseLinear is
useless and very big in most cases.</p>
<p>But, considering model like Wide&amp;Deep, we provide backwardStart and backwardLength to backward
part of the gradient to next layer.</p>
<p>:param input_size the size the each input sample
:param output_size the size of the module output of each sample
:param backwardStart backwardStart index, counting from 1
:param backwardLength backward length
:param withBias if has bias
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.
:param init_weight: the optional initial value for the weight
:param init_bias: the optional initial value for the bias
:param init_grad_weight: the optional initial value for the grad_weight
:param init_grad_bias: the optional initial value for the grad_bias</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sparselinear</span> <span class="o">=</span> <span class="n">SparseLinear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createSparseLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparselinear</span> <span class="o">=</span> <span class="n">SparseLinear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">)</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createSparseLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparselinear</span> <span class="o">=</span> <span class="n">SparseLinear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">init_weight</span><span class="o">=</span><span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="o">=</span><span class="n">init_bias</span><span class="p">)</span>
<span class="go">creating: createSparseLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">JTensor</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">sparselinear</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">10.09569263</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.94844246</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1086688</span><span class="p">,</span> <span class="mf">1.02527523</span><span class="p">,</span> <span class="mf">11.80737209</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.9651413</span><span class="p">,</span> <span class="mf">9.7131443</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.22719955</span><span class="p">,</span> <span class="mf">0.02345783</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.74368906</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SparseLinear.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SparseLinear.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SparseLinear.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialAveragePooling">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialAveragePooling</code><span class="sig-paren">(</span><em>kw</em>, <em>kh</em>, <em>dw=1</em>, <em>dh=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>global_pooling=False</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em>, <em>divide=True</em>, <em>format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialAveragePooling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialAveragePooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies 2D average-pooling operation in kWxkH regions by step size dWxdH steps.
The number of output features is equal to the number of input planes.</p>
<p>When padW and padH are both -1, we use a padding algorithm similar to the “SAME”
padding of tensorflow. That is</p>
<p>outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)
outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)</p>
<p>padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)
padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)</p>
<p>padTop = padAlongHeight / 2
padLeft = padAlongWidth / 2</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kW</strong> – kernel width</li>
<li><strong>kH</strong> – kernel height</li>
<li><strong>dW</strong> – step width</li>
<li><strong>dH</strong> – step height</li>
<li><strong>padW</strong> – padding width</li>
<li><strong>padH</strong> – padding height</li>
<li><strong>global_pooling</strong> – If globalPooling then it will pool over the size of the input by doing</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>kH = input-&gt;height and kW = input-&gt;width
:param ceilMode: whether the output size is to be ceiled or floored
:param countIncludePad: whether to include padding when dividing thenumber of elements in pooling region
:param divide: whether to do the averaging
:param format:          “NCHW” or “NHWC”, indicating the input data format</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialAveragePooling</span> <span class="o">=</span> <span class="n">SpatialAveragePooling</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
<span class="go">creating: createSpatialAveragePooling</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialAveragePooling</span> <span class="o">=</span> <span class="n">SpatialAveragePooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;NHWC&quot;</span><span class="p">)</span>
<span class="go">creating: createSpatialAveragePooling</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SpatialAveragePooling.set_weights">
<code class="descname">set_weights</code><span class="sig-paren">(</span><em>weights</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialAveragePooling.set_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialAveragePooling.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Set weights for this layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>weights</strong> – a list of numpy arrays which represent weight and bias</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createLinear</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">linear</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
<span class="go">creating: createReLU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">py4j.protocol</span> <span class="k">import</span> <span class="n">Py4JJavaError</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">relu</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">])])</span>
<span class="gp">... </span><span class="k">except</span> <span class="n">Py4JJavaError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">java_exception</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">java.lang.IllegalArgumentException: requirement failed: this layer does not have weight/bias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="go">The layer does not have weight/bias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createAdd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">add</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])])</span>
<span class="gp">... </span><span class="k">except</span> <span class="n">Py4JJavaError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">java_exception</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">java.lang.IllegalArgumentException: requirement failed: the number of input weight/bias is not consistant with number of weight/bias of this layer, number of input 1, number of output 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cAdd</span> <span class="o">=</span> <span class="n">CAdd</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">creating: createCAdd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cAdd</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">cAdd</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialBatchNormalization">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialBatchNormalization</code><span class="sig-paren">(</span><em>n_output</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialBatchNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialBatchNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This file implements Batch Normalization as described in the paper:
“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”
by Sergey Ioffe, Christian Szegedy
This implementation is useful for inputs coming from convolution layers.
For non-convolutional layers, see [[BatchNormalization]]
The operation implemented is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>      <span class="p">(</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">--------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
   <span class="n">standard</span><span class="o">-</span><span class="n">deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>where gamma and beta are learnable parameters.
The learning of gamma and beta is optional.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_output</strong> – output feature map number</li>
<li><strong>eps</strong> – avoid divide zero</li>
<li><strong>momentum</strong> – momentum for weight update</li>
<li><strong>affine</strong> – affine operation on output or not</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param data_format a string value (or DataFormat Object in Scala) of “NHWC” or “NCHW” to specify the input data format of this layer. In “NHWC” format
data is stored in the order of [batch_size, height, width, channels], in “NCHW” format data is stored
in the order of [batch_size, channels, height, width].</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialBatchNormalization</span> <span class="o">=</span> <span class="n">SpatialBatchNormalization</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSpatialBatchNormalization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialBatchNormalization</span> <span class="o">=</span> <span class="n">SpatialBatchNormalization</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">)</span>
<span class="go">creating: createSpatialBatchNormalization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialBatchNormalization</span> <span class="o">=</span> <span class="n">SpatialBatchNormalization</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">,</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">)</span>
<span class="go">creating: createSpatialBatchNormalization</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SpatialBatchNormalization.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialBatchNormalization.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialBatchNormalization.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialContrastiveNormalization">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialContrastiveNormalization</code><span class="sig-paren">(</span><em>n_input_plane=1</em>, <em>kernel=None</em>, <em>threshold=0.0001</em>, <em>thresval=0.0001</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialContrastiveNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialContrastiveNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Subtractive + divisive contrast normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_input_plane</strong> – </li>
<li><strong>kernel</strong> – </li>
<li><strong>threshold</strong> – </li>
<li><strong>thresval</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialContrastiveNormalization</span> <span class="o">=</span> <span class="n">SpatialContrastiveNormalization</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
<span class="go">creating: createSpatialContrastiveNormalization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialContrastiveNormalization</span> <span class="o">=</span> <span class="n">SpatialContrastiveNormalization</span><span class="p">()</span>
<span class="go">creating: createSpatialContrastiveNormalization</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialConvolution</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>n_output_plane</em>, <em>kernel_w</em>, <em>kernel_h</em>, <em>stride_w=1</em>, <em>stride_h=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>n_group=1</em>, <em>propagate_back=True</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>with_bias=True</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a 2D convolution over an input image composed of several input planes.
The input tensor in forward(input) is expected to be
a 3D tensor (nInputPlane x height x width).</p>
<p>:param n_input_plane The number of expected input planes in the image given into forward()
:param n_output_plane The number of output planes the convolution layer will produce.
:param kernel_w The kernel width of the convolution
:param kernel_h The kernel height of the convolution
:param stride_w The step of the convolution in the width dimension.
:param stride_h The step of the convolution in the height dimension
:param pad_w The additional zeros added per width to the input planes.
:param pad_h The additional zeros added per height to the input planes.
:param n_group Kernel group number
:param propagate_back Propagate gradient back
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.
:param init_weight: the optional initial value for the weight
:param init_bias: the optional initial value for the bias
:param init_grad_weight: the optional initial value for the grad_weight
:param init_grad_bias: the optional initial value for the grad_bias
:param with_bias: the optional initial value for if need bias
:param data_format: a string value of “NHWC” or “NCHW” to specify the input data format of this layer. In “NHWC” format
data is stored in the order of [batch_size, height, width, channels], in “NCHW” format data is stored
in the order of [batch_size, channels, height, width].</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialConvolution</span> <span class="o">=</span> <span class="n">SpatialConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="go">creating: createSpatialConvolution</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialConvolution</span><span class="o">.</span><span class="n">setWRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialConvolution</span><span class="o">.</span><span class="n">setBRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">12</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialConvolution</span> <span class="o">=</span> <span class="n">SpatialConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createSpatialConvolution</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SpatialConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialConvolutionMap">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialConvolutionMap</code><span class="sig-paren">(</span><em>conn_table</em>, <em>kw</em>, <em>kh</em>, <em>dw=1</em>, <em>dh=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialConvolutionMap"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialConvolutionMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This class is a generalization of SpatialConvolution.
It uses a generic connection table between input and output features.
The SpatialConvolution is equivalent to using a full connection table.</p>
<p>When padW and padH are both -1, we use a padding algorithm similar to the “SAME”
padding of tensorflow. That is</p>
<p>outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)
outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)</p>
<p>padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)
padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)</p>
<p>padTop = padAlongHeight / 2
padLeft = padAlongWidth / 2</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>wRegularizer</strong> – instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</li>
<li><strong>bRegularizer</strong> – instance of [[Regularizer]]applied to the bias.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialConvolutionMap</span> <span class="o">=</span> <span class="n">SpatialConvolutionMap</span><span class="p">(</span><span class="n">ct</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="go">creating: createSpatialConvolutionMap</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialCrossMapLRN">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialCrossMapLRN</code><span class="sig-paren">(</span><em>size=5</em>, <em>alpha=1.0</em>, <em>beta=0.75</em>, <em>k=1.0</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialCrossMapLRN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialCrossMapLRN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies Spatial Local Response Normalization between different feature maps.
The operation implemented is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                             <span class="n">x_f</span>
<span class="n">y_f</span> <span class="o">=</span>  <span class="o">-------------------------------------------------</span>
        <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="n">size</span><span class="p">)</span><span class="o">*</span> <span class="n">sum_</span><span class="p">{</span><span class="n">l</span><span class="o">=</span><span class="n">l1</span> <span class="n">to</span> <span class="n">l2</span><span class="p">}</span> <span class="p">(</span><span class="n">x_l</span><span class="o">^</span><span class="mi">2</span><span class="o">^</span><span class="p">))</span><span class="o">^</span><span class="n">beta</span><span class="o">^</span>
</pre></div>
</div>
<p>where x_f is the input at spatial locations h,w (not shown for simplicity) and feature map f,
l1 corresponds to max(0,f-ceil(size/2)) and l2 to min(F, f-ceil(size/2) + size).
Here, F is the number of feature maps.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> – the number of channels to sum over</li>
<li><strong>alpha</strong> – the scaling parameter</li>
<li><strong>beta</strong> – the exponent</li>
<li><strong>k</strong> – a constant</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param data_format a string value (or DataFormat Object in Scala) of “NHWC” or “NCHW” to specify the input data format of this layer. In “NHWC” format
data is stored in the order of [batch_size, height, width, channels], in “NCHW” format data is stored
in the order of [batch_size, channels, height, width]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialCrossMapLRN</span> <span class="o">=</span> <span class="n">SpatialCrossMapLRN</span><span class="p">()</span>
<span class="go">creating: createSpatialCrossMapLRN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialCrossMapLRN</span> <span class="o">=</span> <span class="n">SpatialCrossMapLRN</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">)</span>
<span class="go">creating: createSpatialCrossMapLRN</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialDilatedConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialDilatedConvolution</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>n_output_plane</em>, <em>kw</em>, <em>kh</em>, <em>dw=1</em>, <em>dh=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>dilation_w=1</em>, <em>dilation_h=1</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialDilatedConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialDilatedConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply a 2D dilated convolution over an input image.</p>
<p>The input tensor is expected to be a 3D or 4D(with batch) tensor.</p>
<p>If input is a 3D tensor nInputPlane x height x width,
owidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1
oheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1</p>
<p>Reference Paper: Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].
arXiv preprint arXiv:1511.07122, 2015.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_input_plane</strong> – The number of expected input planes in the image given into forward().</li>
<li><strong>n_output_plane</strong> – The number of output planes the convolution layer will produce.</li>
<li><strong>kw</strong> – The kernel width of the convolution.</li>
<li><strong>kh</strong> – The kernel height of the convolution.</li>
<li><strong>dw</strong> – The step of the convolution in the width dimension. Default is 1.</li>
<li><strong>dh</strong> – The step of the convolution in the height dimension. Default is 1.</li>
<li><strong>pad_w</strong> – The additional zeros added per width to the input planes. Default is 0.</li>
<li><strong>pad_h</strong> – The additional zeros added per height to the input planes. Default is 0.</li>
<li><strong>dilation_w</strong> – The number of pixels to skip. Default is 1.</li>
<li><strong>dilation_h</strong> – The number of pixels to skip. Default is 1.</li>
<li><strong>init_method</strong> – Init method, Default, Xavier.</li>
<li><strong>wRegularizer</strong> – instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</li>
<li><strong>bRegularizer</strong> – instance of [[Regularizer]]applied to the bias.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialDilatedConvolution</span> <span class="o">=</span> <span class="n">SpatialDilatedConvolution</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSpatialDilatedConvolution</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SpatialDilatedConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialDilatedConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialDilatedConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialDivisiveNormalization">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialDivisiveNormalization</code><span class="sig-paren">(</span><em>n_input_plane=1</em>, <em>kernel=None</em>, <em>threshold=0.0001</em>, <em>thresval=0.0001</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialDivisiveNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialDivisiveNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a spatial division operation on a series of 2D inputs using kernel for
computing the weighted average in a neighborhood. The neighborhood is defined for
a local spatial region that is the size as kernel and across all features. For
an input image, since there is only one feature, the region is only spatial. For
an RGB image, the weighted average is taken over RGB channels and a spatial region.</p>
<p>If the kernel is 1D, then it will be used for constructing and separable 2D kernel.
The operations will be much more efficient in this case.</p>
<p>The kernel is generally chosen as a gaussian when it is believed that the correlation
of two pixel locations decrease with increasing distance. On the feature dimension,
a uniform average is used since the weighting across features is not known.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nInputPlane</strong> – number of input plane, default is 1.</li>
<li><strong>kernel</strong> – kernel tensor, default is a 9 x 9 tensor.</li>
<li><strong>threshold</strong> – threshold</li>
<li><strong>thresval</strong> – threshhold value to replace withif data is smaller than theshold</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialDivisiveNormalization</span> <span class="o">=</span> <span class="n">SpatialDivisiveNormalization</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">kernel</span><span class="p">)</span>
<span class="go">creating: createSpatialDivisiveNormalization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialDivisiveNormalization</span> <span class="o">=</span> <span class="n">SpatialDivisiveNormalization</span><span class="p">()</span>
<span class="go">creating: createSpatialDivisiveNormalization</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialDropout1D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialDropout1D</code><span class="sig-paren">(</span><em>init_p=0.5</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialDropout1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialDropout1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This version performs the same function as Dropout, however it drops
entire 1D feature maps instead of individual elements. If adjacent frames
within feature maps are strongly correlated (as is normally the case in
early convolution layers) then regular dropout will not regularize the
activations and will otherwise just result in an effective learning rate
decrease. In this case, SpatialDropout1D will help promote independence
between feature maps and should be used instead.</p>
<p>:param initP the probability p</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dropout</span> <span class="o">=</span> <span class="n">SpatialDropout1D</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
<span class="go">creating: createSpatialDropout1D</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialDropout2D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialDropout2D</code><span class="sig-paren">(</span><em>init_p=0.5</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialDropout2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialDropout2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This version performs the same function as Dropout, however it drops
entire 2D feature maps instead of individual elements. If adjacent pixels
within feature maps are strongly correlated (as is normally the case in
early convolution layers) then regular dropout will not regularize the
activations and will otherwise just result in an effective learning rate
decrease. In this case, SpatialDropout2D will help promote independence
between feature maps and should be used instead.</p>
<p>:param initP the probability p
:param format  ‘NCHW’ or ‘NHWC’.
In ‘NCHW’ mode, the channels dimension (the depth)
is at index 1, in ‘NHWC’ mode is it at index 4.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dropout</span> <span class="o">=</span> <span class="n">SpatialDropout2D</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">)</span>
<span class="go">creating: createSpatialDropout2D</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialDropout3D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialDropout3D</code><span class="sig-paren">(</span><em>init_p=0.5</em>, <em>data_format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialDropout3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialDropout3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This version performs the same function as Dropout, however it drops
entire 3D feature maps instead of individual elements. If adjacent voxels
within feature maps are strongly correlated (as is normally the case in
early convolution layers) then regular dropout will not regularize the
activations and will otherwise just result in an effective learning rate
decrease. In this case, SpatialDropout3D will help promote independence
between feature maps and should be used instead.</p>
<p>:param initP the probability p
:param format  ‘NCHW’ or ‘NHWC’.
In ‘NCHW’ mode, the channels dimension (the depth)
is at index 1, in ‘NHWC’ mode is it at index 4.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dropout</span> <span class="o">=</span> <span class="n">SpatialDropout3D</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">)</span>
<span class="go">creating: createSpatialDropout3D</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialFullConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialFullConvolution</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>n_output_plane</em>, <em>kw</em>, <em>kh</em>, <em>dw=1</em>, <em>dh=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>adj_w=0</em>, <em>adj_h=0</em>, <em>n_group=1</em>, <em>no_bias=False</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialFullConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialFullConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply a 2D full convolution over an input image.
The input tensor is expected to be a 3D or 4D(with batch) tensor. Note that instead
of setting adjW and adjH, SpatialFullConvolution[Table, T] also accepts a table input
with two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,
and the size of sizeTensor is used to set the size of the output (will ignore the adjW and
adjH values used to construct the module). This module can be used without a bias by setting
parameter noBias = true while constructing the module.</p>
<p>If input is a 3D tensor nInputPlane x height x width,
owidth  = (width  - 1) * dW - 2*padW + kW + adjW
oheight = (height - 1) * dH - 2*padH + kH + adjH</p>
<p>Other frameworks call this operation “In-network Upsampling”, “Fractionally-strided convolution”,
“Backwards Convolution,” “Deconvolution”, or “Upconvolution.”</p>
<p>Reference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic
segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2015: 3431-3440.</p>
<p>:param nInputPlane The number of expected input planes in the image given into forward()
:param nOutputPlane The number of output planes the convolution layer will produce.
:param kW The kernel width of the convolution.
:param kH The kernel height of the convolution.
:param dW The step of the convolution in the width dimension. Default is 1.
:param dH The step of the convolution in the height dimension. Default is 1.
:param padW The additional zeros added per width to the input planes. Default is 0.
:param padH The additional zeros added per height to the input planes. Default is 0.
:param adjW Extra width to add to the output image. Default is 0.
:param adjH Extra height to add to the output image. Default is 0.
:param nGroup Kernel group number.
:param noBias If bias is needed.
:param initMethod Init method, Default, Xavier, Bilinear.
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialFullConvolution</span> <span class="o">=</span> <span class="n">SpatialFullConvolution</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSpatialFullConvolution</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SpatialFullConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialFullConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialFullConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialMaxPooling">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialMaxPooling</code><span class="sig-paren">(</span><em>kw</em>, <em>kh</em>, <em>dw</em>, <em>dh</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>to_ceil=False</em>, <em>format='NCHW'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialMaxPooling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialMaxPooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.
The number of output features is equal to the number of input planes.
If the input image is a 3D tensor nInputPlane x height x width,
the output image size will be nOutputPlane x oheight x owidth where
owidth  = op((width  + 2*padW - kW) / dW + 1)
oheight = op((height + 2*padH - kH) / dH + 1)
op is a rounding operator. By default, it is floor.
It can be changed by calling :ceil() or :floor() methods.</p>
<p>When padW and padH are both -1, we use a padding algorithm similar to the “SAME”
padding of tensorflow. That is</p>
<p>outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)
outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)</p>
<p>padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)
padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)</p>
<p>padTop = padAlongHeight / 2
padLeft = padAlongWidth / 2</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kW</strong> – kernel width</li>
<li><strong>kH</strong> – kernel height</li>
<li><strong>dW</strong> – step size in width</li>
<li><strong>dH</strong> – step size in height</li>
<li><strong>padW</strong> – padding in width</li>
<li><strong>padH</strong> – padding in height</li>
<li><strong>format</strong> – “NCHW” or “NHWC”, indicating the input data format</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialMaxPooling</span> <span class="o">=</span> <span class="n">SpatialMaxPooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createSpatialMaxPooling</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialMaxPooling</span> <span class="o">=</span> <span class="n">SpatialMaxPooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">)</span>
<span class="go">creating: createSpatialMaxPooling</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialSeperableConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialSeperableConvolution</code><span class="sig-paren">(</span><em>n_input_channel</em>, <em>n_output_channel</em>, <em>depth_multiplier</em>, <em>kernel_w</em>, <em>kernel_h</em>, <em>stride_w=1</em>, <em>stride_h=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>with_bias=True</em>, <em>data_format='NCHW'</em>, <em>w_regularizer=None</em>, <em>b_regularizer=None</em>, <em>p_regularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialSeperableConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialSeperableConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Separable convolutions consist in first performing a depthwise spatial convolution (which acts
on each input channel separately) followed by a pointwise convolution which mixes together the
resulting output channels. The  depth_multiplier argument controls how many output channels are
generated per input channel in the depthwise step.</p>
<p>:param n_input_channel The number of expected input planes in the image given into forward()
:param n_output_channel The number of output planes the convolution layer will produce.
:param depth_multiplier how many internal channels are generated per input channel
:param kernel_w The kernel width of the convolution
:param kernel_h The kernel height of the convolution
:param stride_w The step of the convolution in the width dimension.
:param stride_h The step of the convolution in the height dimension
:param pad_w The additional zeros added per width to the input planes.
:param pad_h The additional zeros added per height to the input planes.
:param with_bias: the optional initial value for if need bias
:param data_format: a string value of “NHWC” or “NCHW” to specify the input data format of this layer. In “NHWC” format
data is stored in the order of [batch_size, height, width, channels], in “NCHW” format data is stored
in the order of [batch_size, channels, height, width].
:param w_regularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the depth weights matrices.
:param b_regularizer: instance of [[Regularizer]]applied to the pointwise bias.
:param p_regularizer: instance of [[Regularizer]]applied to the pointwise weights.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">SpatialSeperableConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="go">creating: createSpatialSeperableConvolution</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span><span class="o">.</span><span class="n">setWRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span><span class="o">.</span><span class="n">setBRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">SpatialSeperableConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createSpatialSeperableConvolution</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialShareConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialShareConvolution</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>n_output_plane</em>, <em>kernel_w</em>, <em>kernel_h</em>, <em>stride_w=1</em>, <em>stride_h=1</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>n_group=1</em>, <em>propagate_back=True</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>with_bias=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialShareConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialShareConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialShareConvolution</span> <span class="o">=</span> <span class="n">SpatialShareConvolution</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSpatialShareConvolution</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_grad_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">12</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">SpatialShareConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">init_weight</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="p">)</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createL1Regularizer</span>
<span class="go">creating: createSpatialShareConvolution</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.SpatialShareConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialShareConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialShareConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialSubtractiveNormalization">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialSubtractiveNormalization</code><span class="sig-paren">(</span><em>n_input_plane=1</em>, <em>kernel=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialSubtractiveNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialSubtractiveNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a spatial subtraction operation on a series of 2D inputs using kernel for
computing the weighted average in a neighborhood. The neighborhood is defined for
a local spatial region that is the size as kernel and across all features. For a
an input image, since there is only one feature, the region is only spatial. For
an RGB image, the weighted average is taken over RGB channels and a spatial region.</p>
<p>If the kernel is 1D, then it will be used for constructing and separable 2D kernel.
The operations will be much more efficient in this case.</p>
<p>The kernel is generally chosen as a gaussian when it is believed that the correlation
of two pixel locations decrease with increasing distance. On the feature dimension,
a uniform average is used since the weighting across features is not known.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_input_plane</strong> – number of input plane, default is 1.</li>
<li><strong>kernel</strong> – kernel tensor, default is a 9 x 9 tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialSubtractiveNormalization</span> <span class="o">=</span> <span class="n">SpatialSubtractiveNormalization</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">kernel</span><span class="p">)</span>
<span class="go">creating: createSpatialSubtractiveNormalization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spatialSubtractiveNormalization</span> <span class="o">=</span> <span class="n">SpatialSubtractiveNormalization</span><span class="p">()</span>
<span class="go">creating: createSpatialSubtractiveNormalization</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialWithinChannelLRN">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialWithinChannelLRN</code><span class="sig-paren">(</span><em>size=5</em>, <em>alpha=1.0</em>, <em>beta=0.75</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialWithinChannelLRN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialWithinChannelLRN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>The local response normalization layer performs a kind of lateral inhibition
by normalizing over local input regions. the local regions extend spatially,
in separate channels (i.e., they have shape 1 x local_size x local_size).</p>
<p>:param size  the side length of the square region to sum over
:param alpha the scaling parameter
:param beta the exponent</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">SpatialWithinChannelLRN</span><span class="p">()</span>
<span class="go">creating: createSpatialWithinChannelLRN</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SpatialZeroPadding">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SpatialZeroPadding</code><span class="sig-paren">(</span><em>pad_left</em>, <em>pad_right</em>, <em>pad_top</em>, <em>pad_bottom</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SpatialZeroPadding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SpatialZeroPadding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Each feature map of a given input is padded with specified number of zeros.
If padding values are negative, then input is cropped.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>padLeft</strong> – pad left position</li>
<li><strong>padRight</strong> – pad right position</li>
<li><strong>padTop</strong> – pad top position</li>
<li><strong>padBottom</strong> – pad bottom position</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatialZeroPadding</span> <span class="o">=</span> <span class="n">SpatialZeroPadding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSpatialZeroPadding</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.SplitTable">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">SplitTable</code><span class="sig-paren">(</span><em>dimension</em>, <em>n_input_dims=-1</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#SplitTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.SplitTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Creates a module that takes a Tensor as input and
outputs several tables, splitting the Tensor along
the specified dimension <cite>dimension</cite>. Please note the dimension starts from 1.</p>
<p>The input to this layer is expected to be a tensor, or a batch of tensors;
when using mini-batch, a batch of sample tensors will be passed to the layer and
the user needs to specify the number of dimensions of each sample tensor in a
batch using <cite>nInputDims</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> – to be split along this dimension</li>
<li><strong>n_input_dims</strong> – specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">splitTable</span> <span class="o">=</span> <span class="n">SplitTable</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSplitTable</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Sqrt">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Sqrt</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Sqrt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply an element-wise sqrt operation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqrt</span> <span class="o">=</span> <span class="n">Sqrt</span><span class="p">()</span>
<span class="go">creating: createSqrt</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Square">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Square</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Square"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Square" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply an element-wise square operation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">Square</span><span class="p">()</span>
<span class="go">creating: createSquare</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Squeeze">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Squeeze</code><span class="sig-paren">(</span><em>dim</em>, <em>num_input_dims=-2147483648</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Squeeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Delete singleton all dimensions or a specific dim.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> – Optional. The dimension to be delete. Default: delete all dimensions.</li>
<li><strong>num_input_dims</strong> – Optional. If in a batch model, set to the inputDims.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">Squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createSqueeze</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Sum">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Sum</code><span class="sig-paren">(</span><em>dimension=1</em>, <em>n_input_dims=-1</em>, <em>size_average=False</em>, <em>squeeze=True</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>It is a simple layer which applies a sum operation over the given dimension.
When nInputDims is provided, the input will be considered as a batches.
Then the sum operation will be applied in (dimension + 1)
The input to this layer is expected to be a tensor, or a batch of tensors;
when using mini-batch, a batch of sample tensors will be passed to the layer and
the user need to specify the number of dimensions of each sample tensor in the
batch using <cite>nInputDims</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> – the dimension to be applied sum operation</li>
<li><strong>n_input_dims</strong> – specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</li>
<li><strong>size_average</strong> – default is false, if it is true, it will return the mean instead</li>
<li><strong>squeeze</strong> – default is true, which will squeeze the sum dimension; set it to false to keep the sum dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sum</span> <span class="o">=</span> <span class="n">Sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createSum</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Tanh">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Tanh</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies the Tanh function element-wise to the input Tensor, thus outputting a Tensor of the same
dimension. Tanh is defined as f(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>
<span class="go">creating: createTanh</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.TanhShrink">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">TanhShrink</code><span class="sig-paren">(</span><em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#TanhShrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.TanhShrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>A simple layer for each element of the input tensor, do the following operation
during the forward process:
[f(x) = tanh(x) - 1]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tanhShrink</span> <span class="o">=</span> <span class="n">TanhShrink</span><span class="p">()</span>
<span class="go">creating: createTanhShrink</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.TemporalConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">TemporalConvolution</code><span class="sig-paren">(</span><em>input_frame_size</em>, <em>output_frame_size</em>, <em>kernel_w</em>, <em>stride_w=1</em>, <em>propagate_back=True</em>, <em>weight_regularizer=None</em>, <em>bias_regularizer=None</em>, <em>init_weight=None</em>, <em>init_bias=None</em>, <em>init_grad_weight=None</em>, <em>init_grad_bias=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#TemporalConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.TemporalConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a 1D convolution over an input sequence composed of nInputFrame frames..
The input tensor in <cite>forward(input)</cite> is expected to be a 2D tensor
(<cite>nInputFrame</cite> x <cite>inputFrameSize</cite>) or a 3D tensor
(<cite>nBatchFrame</cite> x <cite>nInputFrame</cite> x <cite>inputFrameSize</cite>).</p>
<p>:param input_frame_size The input frame size expected in sequences given into <cite>forward()</cite>
:param output_frame_size The output frame size the convolution layer will produce.
:param kernel_w The kernel width of the convolution
:param stride_w The step of the convolution in the width dimension.
:param propagate_back Whether propagate gradient back, default is true.
:param weight_regularizer instance of [[Regularizer]]
(eg. L1 or L2 regularization), applied to the input weights matrices.
:param bias_regularizer instance of [[Regularizer]]
applied to the bias.
:param init_weight Initial weight
:param init_bias Initial bias
:param init_grad_weight Initial gradient weight
:param init_grad_bias Initial gradient bias</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">temporalConvolution</span> <span class="o">=</span> <span class="n">TemporalConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="go">creating: createTemporalConvolution</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">temporalConvolution</span><span class="o">.</span><span class="n">setWRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">temporalConvolution</span><span class="o">.</span><span class="n">setBRegularizer</span><span class="p">(</span><span class="n">L1Regularizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="go">creating: createL1Regularizer</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.TemporalConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#TemporalConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.TemporalConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.TemporalMaxPooling">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">TemporalMaxPooling</code><span class="sig-paren">(</span><em>k_w</em>, <em>d_w=-1</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#TemporalMaxPooling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.TemporalMaxPooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies 1D max-pooling operation in kW regions by step size dW steps.
Input sequence composed of nInputFrame frames.
The input tensor in forward(input) is expected to be a 2D tensor (nInputFrame x inputFrameSize)
or a 3D tensor (nBatchFrame x nInputFrame x inputFrameSize).</p>
<p>If the input sequence is a 2D tensor of dimension nInputFrame x inputFrameSize,
the output sequence will be nOutputFrame x inputFrameSize where</p>
<p>nOutputFrame = (nInputFrame - k_w) / d_w + 1</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>k_w</strong> – kernel width</li>
<li><strong>d_w</strong> – step size in width, default is -1, means the <cite>d_w</cite> equals <cite>k_w</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">temporalMaxPooling</span> <span class="o">=</span> <span class="n">TemporalMaxPooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createTemporalMaxPooling</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Threshold">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Threshold</code><span class="sig-paren">(</span><em>th=1e-06</em>, <em>v=0.0</em>, <em>ip=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Threshold input Tensor.
If values in the Tensor smaller than th, then replace it with v</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>th</strong> – the threshold to compare with</li>
<li><strong>v</strong> – the value to replace with</li>
<li><strong>ip</strong> – inplace mode</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">threshold</span> <span class="o">=</span> <span class="n">Threshold</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">creating: createThreshold</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Tile">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Tile</code><span class="sig-paren">(</span><em>dim=1</em>, <em>copies=2</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Tile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Tile" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Replicate ‘copies’ copy along ‘dim’ dimension</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Tile</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">creating: createTile</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.TimeDistributed">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">TimeDistributed</code><span class="sig-paren">(</span><em>model</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#TimeDistributed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.TimeDistributed" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This layer is intended to apply contained layer to each temporal time slice
of input tensor.</p>
<p>For instance, The TimeDistributed Layer can feed each time slice of input tensor
to the Linear layer.</p>
<p>The input data format is [Batch, Time, Other dims]. For the contained layer, it must not change
the Other dims length.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">td</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">creating: createLinear</span>
<span class="go">creating: createTimeDistributed</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Transpose">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Transpose</code><span class="sig-paren">(</span><em>permutations</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Transpose input along specified dimensions</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>permutations</strong> – dimension pairs that need to swap</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transpose</span> <span class="o">=</span> <span class="n">Transpose</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)])</span>
<span class="go">creating: createTranspose</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.Unsqueeze">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">Unsqueeze</code><span class="sig-paren">(</span><em>pos</em>, <em>num_input_dims=-2147483648</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#Unsqueeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.Unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Create an Unsqueeze layer.  Insert singleton dim (i.e., dimension 1) at position pos.
For an input with dim = input.dim(),
there are dim + 1 possible positions to insert the singleton dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pos</strong> – The position will be insert singleton.</li>
<li><strong>num_input_dims</strong> – Optional. If in a batch model, set to the inputDim</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unsqueeze</span> <span class="o">=</span> <span class="n">Unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createUnsqueeze</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.UpSampling1D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">UpSampling1D</code><span class="sig-paren">(</span><em>length</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#UpSampling1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.UpSampling1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Upsampling layer for 1D inputs.
Repeats each temporal step length times along the time axis.</p>
<p>If input’s size is (batch, steps, features),
then the output’s size is (batch, steps * length, features)</p>
<p>:param length integer, upsampling factor.
&gt;&gt;&gt; upsampled1d = UpSampling1D(2)
creating: createUpSampling1D</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.UpSampling2D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">UpSampling2D</code><span class="sig-paren">(</span><em>size</em>, <em>data_format='nchw'</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#UpSampling2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.UpSampling2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Upsampling layer for 2D inputs.
Repeats the heights and widths of the data by size[0] and size[1] respectively.</p>
<p>If input’s dataformat is NCHW, then the size of output is (N, C, H * size[0], W * size[1])</p>
<p>:param size tuple of 2 integers. The upsampling factors for heights and widths.
:param format DataFormat, NCHW or NHWC</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">upsampled2d</span> <span class="o">=</span> <span class="n">UpSampling2D</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">creating: createUpSampling2D</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.UpSampling3D">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">UpSampling3D</code><span class="sig-paren">(</span><em>size</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#UpSampling3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.UpSampling3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Upsampling layer for 3D inputs.
Repeats the 1st, 2nd and 3rd dimensions
of the data by size[0], size[1] and size[2] respectively.
The input data is assumed to be of the form <cite>minibatch x channels x depth x height x width</cite>.</p>
<p>:param size Repeats the depth, height, width dimensions of the data by
&gt;&gt;&gt; upsample3d = UpSampling3D([1, 2, 3])
creating: createUpSampling3D</p>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.View">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">View</code><span class="sig-paren">(</span><em>sizes</em>, <em>num_input_dims=0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#View"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.View" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>This module creates a new view of the input tensor using the sizes passed to the constructor.
The method setNumInputDims() allows to specify the expected number of dimensions of the
inputs of the modules. This makes it possible to use minibatch inputs when using a size -1
for one of the dimensions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size</strong> – sizes use for creates a new view</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">view</span> <span class="o">=</span> <span class="n">View</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="go">creating: createView</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.VolumetricAveragePooling">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">VolumetricAveragePooling</code><span class="sig-paren">(</span><em>k_t</em>, <em>k_w</em>, <em>k_h</em>, <em>d_t</em>, <em>d_w</em>, <em>d_h</em>, <em>pad_t=0</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>count_include_pad=True</em>, <em>ceil_mode=False</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#VolumetricAveragePooling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.VolumetricAveragePooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies 3D average-pooling operation in kTxkWxkH regions by step size dTxdWxdH.
The number of output features is equal to the number of input planes / dT.
The input can optionally be padded with zeros. Padding should be smaller than
half of kernel size. That is, padT &lt; kT/2, padW &lt; kW/2 and padH &lt; kH/2</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>k_t</strong> – The kernel size</li>
<li><strong>k_w</strong> – The kernel width</li>
<li><strong>k_h</strong> – The kernel height</li>
<li><strong>d_t</strong> – The step in the time dimension</li>
<li><strong>d_w</strong> – The step in the width dimension</li>
<li><strong>d_h</strong> – The step in the height dimension</li>
<li><strong>pad_t</strong> – The padding in the time dimension</li>
<li><strong>pad_w</strong> – The padding in the width dimension</li>
<li><strong>pad_h</strong> – The padding in the height dimension</li>
<li><strong>count_include_pad</strong> – whether to include padding when dividing the number of elements in pooling region</li>
<li><strong>ceil_mode</strong> – whether the output size is to be ceiled or floored</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">volumetricAveragePooling</span> <span class="o">=</span> <span class="n">VolumetricAveragePooling</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createVolumetricAveragePooling</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.VolumetricConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">VolumetricConvolution</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>n_output_plane</em>, <em>k_t</em>, <em>k_w</em>, <em>k_h</em>, <em>d_t=1</em>, <em>d_w=1</em>, <em>d_h=1</em>, <em>pad_t=0</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>with_bias=True</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#VolumetricConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.VolumetricConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies a 3D convolution over an input image composed of several input planes. The input tensor
in forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_input_plane</strong> – The number of expected input planes in the image given into forward()</li>
<li><strong>n_output_plane</strong> – The number of output planes the convolution layer will produce.</li>
<li><strong>k_t</strong> – The kernel size of the convolution in time</li>
<li><strong>k_w</strong> – The kernel width of the convolution</li>
<li><strong>k_h</strong> – The kernel height of the convolution</li>
<li><strong>d_t</strong> – The step of the convolution in the time dimension. Default is 1</li>
<li><strong>d_w</strong> – The step of the convolution in the width dimension. Default is 1</li>
<li><strong>d_h</strong> – The step of the convolution in the height dimension. Default is 1</li>
<li><strong>pad_t</strong> – Additional zeros added to the input plane data on both sides of time axis.Default is 0. (kT-1)/2 is often used here.</li>
<li><strong>pad_w</strong> – The additional zeros added per width to the input planes.</li>
<li><strong>pad_h</strong> – The additional zeros added per height to the input planes.</li>
<li><strong>with_bias</strong> – whether with bias</li>
<li><strong>wRegularizer</strong> – instance of [[Regularizer]] (eg. L1 or L2 regularization), applied to the input weights matrices.</li>
<li><strong>bRegularizer</strong> – instance of [[Regularizer]] applied to the bias.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">volumetricConvolution</span> <span class="o">=</span> <span class="n">VolumetricConvolution</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createVolumetricConvolution</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.VolumetricConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#VolumetricConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.VolumetricConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.VolumetricFullConvolution">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">VolumetricFullConvolution</code><span class="sig-paren">(</span><em>n_input_plane</em>, <em>n_output_plane</em>, <em>kt</em>, <em>kw</em>, <em>kh</em>, <em>dt=1</em>, <em>dw=1</em>, <em>dh=1</em>, <em>pad_t=0</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>adj_t=0</em>, <em>adj_w=0</em>, <em>adj_h=0</em>, <em>n_group=1</em>, <em>no_bias=False</em>, <em>wRegularizer=None</em>, <em>bRegularizer=None</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#VolumetricFullConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.VolumetricFullConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Apply a 3D full convolution over an 3D input image, a sequence of images, or a video etc.
The input tensor is expected to be a 4D or 5D(with batch) tensor. Note that instead
of setting adjT, adjW and adjH, <cite>VolumetricFullConvolution</cite> also accepts a table input
with two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,
and the size of sizeTensor is used to set the size of the output (will ignore the adjT, adjW and
adjH values used to construct the module). This module can be used without a bias by setting
parameter noBias = true while constructing the module.</p>
<p>If input is a 4D tensor nInputPlane x depth x height x width,
odepth = (depth  - 1) * dT - 2*padt + kT + adjT
owidth  = (width  - 1) * dW - 2*padW + kW + adjW
oheight = (height - 1) * dH - 2*padH + kH + adjH</p>
<p>Other frameworks call this operation “In-network Upsampling”, “Fractionally-strided convolution”,
“Backwards Convolution,” “Deconvolution”, or “Upconvolution.”</p>
<p>Reference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic
segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2015: 3431-3440.</p>
<p>:param nInputPlane The number of expected input planes in the image given into forward()
:param nOutputPlane The number of output planes the convolution layer will produce.
:param kT The kernel depth of the convolution.
:param kW The kernel width of the convolution.
:param kH The kernel height of the convolution.
:param dT The step of the convolution in the depth dimension. Default is 1.
:param dW The step of the convolution in the width dimension. Default is 1.
:param dH The step of the convolution in the height dimension. Default is 1.
:param padT The additional zeros added per depth to the input planes. Default is 0.
:param padW The additional zeros added per width to the input planes. Default is 0.
:param padH The additional zeros added per height to the input planes. Default is 0.
:param adjT Extra depth to add to the output image. Default is 0.
:param adjW Extra width to add to the output image. Default is 0.
:param adjH Extra height to add to the output image. Default is 0.
:param nGroup Kernel group number.
:param noBias If bias is needed.
:param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.
:param bRegularizer: instance of [[Regularizer]]applied to the bias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">volumetricFullConvolution</span> <span class="o">=</span> <span class="n">VolumetricFullConvolution</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createVolumetricFullConvolution</span>
</pre></div>
</div>
<dl class="method">
<dt id="bigdl.nn.layer.VolumetricFullConvolution.set_init_method">
<code class="descname">set_init_method</code><span class="sig-paren">(</span><em>weight_init_method=None</em>, <em>bias_init_method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#VolumetricFullConvolution.set_init_method"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.VolumetricFullConvolution.set_init_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="bigdl.nn.layer.VolumetricMaxPooling">
<em class="property">class </em><code class="descclassname">bigdl.nn.layer.</code><code class="descname">VolumetricMaxPooling</code><span class="sig-paren">(</span><em>k_t</em>, <em>k_w</em>, <em>k_h</em>, <em>d_t</em>, <em>d_w</em>, <em>d_h</em>, <em>pad_t=0</em>, <em>pad_w=0</em>, <em>pad_h=0</em>, <em>bigdl_type='float'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bigdl/nn/layer.html#VolumetricMaxPooling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#bigdl.nn.layer.VolumetricMaxPooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#bigdl.nn.layer.Layer" title="bigdl.nn.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.nn.layer.Layer</span></code></a></p>
<p>Applies 3D max-pooling operation in kTxkWxkH regions by step size dTxdWxdH.
The number of output features is equal to the number of input planes / dT.
The input can optionally be padded with zeros. Padding should be smaller than
half of kernel size. That is, padT &lt; kT/2, padW &lt; kW/2 and padH &lt; kH/2</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>k_t</strong> – The kernel size</li>
<li><strong>k_w</strong> – The kernel width</li>
<li><strong>k_h</strong> – The kernel height</li>
<li><strong>d_t</strong> – The step in the time dimension</li>
<li><strong>d_w</strong> – The step in the width dimension</li>
<li><strong>d_h</strong> – The step in the height dimension</li>
<li><strong>pad_t</strong> – The padding in the time dimension</li>
<li><strong>pad_w</strong> – The padding in the width dimension</li>
<li><strong>pad_h</strong> – The padding in the height dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">volumetricMaxPooling</span> <span class="o">=</span> <span class="n">VolumetricMaxPooling</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">creating: createVolumetricMaxPooling</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-bigdl.nn">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-bigdl.nn" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="bigdl.optim.html" title="bigdl.optim package"
             >next</a> |</li>
        <li class="right" >
          <a href="bigdl.models.lenet.html" title="bigdl.models.lenet package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">BigDL  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="bigdl.html" >bigdl package</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, Intel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.1.
    </div>
  </body>
</html>