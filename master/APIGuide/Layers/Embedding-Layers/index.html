<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Embedding Layers - BigDL Project</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "LookupTable", url: "#lookuptable", children: [
          ]},
          {title: "LookupTableSparse", url: "#lookuptablesparse", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
      <script src="../../../search/require.js"></script>
      <script src="../../../search/search.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Embedding Layers</strong></h1>
    <hr>
    <h2 id="lookuptable">LookupTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = LookupTable(nIndex: Int, nOutput: Int, paddingValue: Double = 0,
                                 maxNorm: Double = Double.MaxValue,
                                 normType: Double = 2.0,
                                 shouldScaleGradByFreq: Boolean = false,
                                 wRegularizer: Regularizer[T] = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = LookupTable(nIndex, nOutput, paddingValue, maxNorm, normType, shouldScaleGradByFreq)
</code></pre>

<p>This layer is a particular case of a convolution, where the width of the convolution would be 1.
Input should be a 1D or 2D tensor filled with indices. Indices are corresponding to the position
in weight. For each index element of input, it outputs the selected index part of weight.
This layer is often used in word embedding. In collaborative filtering, it can be used together with Select to create embeddings for users or items. </p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val layer = LookupTable(9, 4, 2, 0.1, 2.0, true)
val input = Tensor(Storage(Array(5.0f, 2.0f, 6.0f, 9.0f, 4.0f)), 1, Array(5))

val output = layer.forward(input)
val gradInput = layer.backward(input, output)

&gt; println(layer.weight)
res6: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.2949163      -0.8240777      -0.9440595      -0.8326071
-0.025108865    -0.025346711    0.09046136      -0.023320194
-1.7525806      0.7305201       0.3349018       0.03952092
-0.0048129847   0.023922665     0.005595926     -0.09681542
-0.01619357     -0.030372608    0.07217587      -0.060049288
0.014426847     -0.09052222     0.019132217     -0.035093457
-0.7002858      1.1149521       0.9869375       1.2580993
0.36649692      -0.6583153      0.90005803      0.12671651
0.048913725     0.033388995     -0.07938445     0.01381052
[com.intel.analytics.bigdl.tensor.DenseTensor of size 9x4]

&gt; println(input)
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
5.0
2.0
6.0
9.0
4.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]

&gt; println(output)
output: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.01619357     -0.030372608    0.07217587      -0.060049288
-0.025108865    -0.025346711    0.09046136      -0.023320194
0.014426847     -0.09052222     0.019132217     -0.035093457
0.048913725     0.033388995     -0.07938445     0.01381052
-0.0048129847   0.023922665     0.005595926     -0.09681542
[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x4]

&gt; println(gradInput)
gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.0
0.0
0.0
0.0
0.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = LookupTable(9, 4, 2.0, 0.1, 2.0, True)
input = np.array([5.0, 2.0, 6.0, 9.0, 4.0]).astype(&quot;float32&quot;)

output = layer.forward(input)
gradInput = layer.backward(input, output)

&gt; output
[array([[-0.00704637,  0.07495038,  0.06465427,  0.01235369],
        [ 0.00350313,  0.02751033, -0.02163727,  0.0936095 ],
        [ 0.02330465, -0.05696457,  0.0081728 ,  0.07839092],
        [ 0.06580321, -0.0743262 , -0.00414508, -0.01133001],
        [-0.00382435, -0.04677011,  0.02839171, -0.08361723]], dtype=float32)]

&gt; gradInput
[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)]

</code></pre>

<hr />
<h2 id="lookuptablesparse">LookupTableSparse</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = LookupTableSparse(nIndex: Int, nOutput: Int,
    combiner: String = &quot;sum&quot;,
    maxNorm: Double = -1,
    wRegularizer: Regularizer[T] = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = LookupTableSparse(nIndex, nOutput,
    combiner,
    maxNorm,
    wRegularizer)
</code></pre>

<p>LookupTable for multi-values. 
Also called embedding_lookup_sparse in TensorFlow. </p>
<p>The input of LookupTableSparse should be a 2D SparseTensor or two 2D SparseTensors. If the input is a SparseTensor, the values are positive integer ids, values in each row of this SparseTensor will be turned into a dense vector. If the input is two SparseTensor, the first tensor should be the integer ids, just like the SparseTensor input. And the second tensor is the corresponding weights of the integer ids.</p>
<p>@param nIndex Indices of input row
@param nOutput the last dimension size of output
@param combiner A string specifying the reduce type. Currently "mean", "sum", "sqrtn" is supported.
@param maxNorm If provided, each embedding is normalized to have l2 norm equal to maxNorm before combining.
@param wRegularizer: instance of <a href="../eg. L1 or L2 regularization">[Regularizer]</a>, applied to the input weights matrices.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.utils.T

val indices1 = Array(0, 0, 1, 2)
val indices2 = Array(0, 1, 0, 3)
val values = Array(2f, 4, 1, 2)
val weightValues = Array(2f, 0.5f, 1, 3)
val input = Tensor.sparse(Array(indices1, indices2), values, Array(3, 4))
val weight = Tensor.sparse(Array(indices1, indices2), weightValues, Array(3, 4))

val layer1 = LookupTableSparse(10, 4, &quot;mean&quot;)
layer1.weight.range(1, 40, 1) // set weight to 1 to 40
val output = layer1.forward(T(input, weight))
</code></pre>

<p>The output is</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.tensor.Tensor[Float] =
6.6 7.6000004   8.6 9.6
1.0 2.0 3.0 4.0
5.0 6.0 7.0 8.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

indices = np.array([[0, 0, 1, 2], [0, 1, 0, 3]])
values = np.array([2, 4, 1, 2])
weightValues = np.array([2, 0.5, 1, 3])
input = JTensor.sparse(values, indices, np.array([3, 4]))
weight = JTensor.sparse(weightValues, indices, np.array([3, 4]))

layer1 = LookupTableSparse(10, 4, &quot;mean&quot;)
layer1.set_weights(np.arange(1, 41, 1).reshape(10, 4)) # set weight to 1 to 40
output = layer1.forward([input, weight])
print(output)
</code></pre>

<p>The output is</p>
<pre><code class="python">array([[ 6.5999999 ,  7.60000038,  8.60000038,  9.60000038],
       [ 1.        ,  2.        ,  3.        ,  4.        ],
       [ 5.        ,  6.        ,  7.        ,  8.        ]], dtype=float32)
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>