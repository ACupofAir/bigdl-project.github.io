<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Optimizer - BigDL Project</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Optimizer", url: "#optimizer", children: [
              {title: "Scala API", url: "#scala-api" },
              {title: "Scala example", url: "#scala-example" },
              {title: "Python API", url: "#python-api" },
              {title: "Python example", url: "#python-example" },
              {title: "How BigDL train models in a distributed cluster?", url: "#how-bigdl-train-models-in-a-distributed-cluster" },
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-60548202-2', 'bigdl-project.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Optimizer</strong></h1>
    <hr>
    <h2 id="optimizer">Optimizer</h2>
<p>An optimizer is in general to minimize any function with respect to a set of parameters. In case of training a neural network, an optimizer tries to minimize the loss of the neural net with respect to its weights/biases, over the training set.</p>
<h3 id="scala-api">Scala API</h3>
<p><strong><em>Factory method</em></strong></p>
<p>In summary, you need to supply 3 kinds of paramters to create an optimizer:
1) train data: You could supply 
   a) a sampleRDD and batchSize (with optional featurePadding and labelPadding), or 
   b) a sampleRDD and batchSize and a customized implemenation of trait MiniBatch, or
   c) a DataSet - the type of optimizer created will be dermined by the type of Dataset. 
2) a model
3) a criterion (i.e. loss)
as shown in below interfaces:</p>
<pre><code class="scala">val optimizer = Opimizer[T: ClassTag, D](
      model: Module[T],
      sampleRDD: RDD[Sample[T]],
      criterion: Criterion[T],
      batchSize: Int,
      featurePaddingParam: PaddingParam[T]=null,
      labelPaddingParam: PaddingParam[T]=null)
</code></pre>

<p>The meaning of parameters are as below:
<code>model</code>: model will be optimized.
<code>sampleRDD</code>: training Samples.
<code>criterion</code>: loss function.
<code>batchSize</code>: mini batch size.
<code>featurePaddingParam</code>(optional): feature padding strategy.
<code>labelPaddingParam</code>(optional): label padding strategy.
 <br></p>
<pre><code class="scala">val optimizer = Opimizer[T: ClassTag, D](
  model: Module[T],
  dataset: DataSet[D],
  criterion: Criterion[T])
</code></pre>

<p><code>T</code>: the numeric type(Float/Double).<br />
<code>D</code>: should be a kind of MiniBatch.<br />
<code>model</code>: the model will be optimized.<br />
<code>dataset</code>: the training DataSet.<br />
<code>criterion</code>: the Loss function.
 <br></p>
<pre><code class="scala">val optimizer = Opimizer[T: ClassTag, D](
      model: Module[T],
      sampleRDD: RDD[Sample[T]],
      criterion: Criterion[T],
      batchSize: Int,
      miniBatch: MiniBatch[T])
</code></pre>

<p>Apply an optimizer with User-Defined <code>MiniBatch</code>.<br />
<code>model</code>: model will be optimized.<br />
<code>sampleRDD</code>: training Samples.<br />
<code>criterion</code>: loss function.<br />
<code>batchSize</code>: mini batch size.<br />
<code>miniBatch</code>: An User-Defined MiniBatch implementation.
 <br></p>
<p><strong><em>Validation</em></strong></p>
<p>Function setValidation is to set a validate evaluation in the <code>optimizer</code>.</p>
<pre><code class="scala">optimizer.setValidation(
  trigger: Trigger,
  dataset: DataSet[MiniBatch[T]],
  vMethods : Array[ValidationMethod[T])
</code></pre>

<p><code>trigger</code>: how often to evaluation validation set.<br />
<code>dataset</code>: validate data set in type of DataSet[MiniBatch].<br />
<code>vMethods</code>: a set of ValidationMethod.<br />
 <br></p>
<pre><code class="scala">optimizer.setValidation(
  trigger: Trigger,
  sampleRDD: RDD[Sample[T]],
  vMethods: Array[ValidationMethod[T]],
  batchSize: Int)
</code></pre>

<p><code>trigger</code>: how often to evaluation validation set.<br />
<code>sampleRDD</code>: validate data set in type of RDD[Sample].<br />
<code>vMethods</code>: a set of ValidationMethod.<br />
<code>batchSize</code>: size of mini batch.
 <br>
<strong><em>Checkpoint</em></strong></p>
<pre><code class="scala">optimizer.setCheckpoint(path: String, trigger: Trigger)
</code></pre>

<p>Function setCheckPoint is used to set a check point saved at <code>path</code> triggered by <code>trigger</code>.<br />
<code>path</code>: a local/HDFS directory to save checkpoint.<br />
<code>trigger</code>: how often to save the check point.<br />
 <br></p>
<pre><code class="scala">val path = optimizer.getCheckpointPath()
</code></pre>

<p>Function getCheckpointPath is used to get the directory of saving checkpoint.<br />
 <br></p>
<pre><code class="scala">optimizer.overWriteCheckpoint()
</code></pre>

<p>Function overWriteCheckpoint is enable overwrite saving checkpoint.  </p>
<p><strong><em>Summary</em></strong></p>
<pre><code class="scala">optimizer.setTrainSummary(trainSummary: TrainSummary)
</code></pre>

<p>Function setTrainSummary is used to enable train summary in this optimizer.<br />
<code>trainSummary</code>: an instance of TrainSummary.<br />
 <br></p>
<pre><code class="scala">optimizer.setValidationSummary(validationSummary: ValidationSummary)
</code></pre>

<p>Function setValidationSummary is used to enable validation summary in this optimizer.<br />
<code>validationSummary</code>: an instance of ValidationSummary.  </p>
<p><strong><em>Other important API</em></strong></p>
<pre><code class="scala">val trainedModel = optimizer.optimize()
</code></pre>

<p>Function optimize will start the training.<br />
 <br></p>
<pre><code class="scala">optimizer.setModel(newModel: Module[T])
</code></pre>

<p>Function setModel will set a new model to the optimizer.<br />
<code>newModel</code>: a model will replace the old model in optimizer.<br />
 <br></p>
<pre><code class="scala">
optimizer.setTrainData(sampleRDD: RDD[Sample[T]],
                 batchSize: Int,
                 miniBatch: MiniBatch[T])

optimizer.setTrainData(sampleRDD: RDD[Sample[T]],
                 batchSize: Int,
                 featurePaddingParam: PaddingParam[T]=null,
                 labelPaddingParam: PaddingParam[T])=null

</code></pre>

<p>the overloaded set of methods <code>setTrainData</code> allows user to replace the training data. Each time setTrainData is called, the dataset is replaced and the following call to optimize() will use the new dataset. The meaning of arguments are the same as in the Factory methods:
<code>sampleRDD</code>: training Samples.
<code>batchSize</code>: mini batch size.
<code>featurePaddingParam</code>: feature padding strategy.
<code>labelPaddingParam</code>: label padding strategy.
<code>miniBatch</code>: An User-Defined MiniBatch implemenation.
 <br></p>
<pre><code class="scala">optimizer.setCriterion(newCriterion: Criterion[T])
</code></pre>

<p><code>setCriterion</code> allows user to set a new criterion to replace the old one. 
<code>newCriterion</code>: the new Criterion.
 <br></p>
<pre><code class="scala">optimizer.setState(state: Table)
</code></pre>

<p>Function setState is used to set a state(learning rate, epochs...) to the <code>optimizer</code>.<br />
<code>state</code>: the state to be saved.<br />
 <br></p>
<pre><code class="scala">optimizer.setOptimMethod(method : OptimMethod[T])
</code></pre>

<p>Function setOptimMethod is used to set an optimization method in this <code>optimizer</code>.<br />
<code>method</code>: the method the optimize the model in this <code>optimizer</code>.<br />
 <br></p>
<pre><code class="scala">optimizer.setEndWhen(endWhen: Trigger)
</code></pre>

<p>Function setEndWhen is used to declare when to stop the training invoked by <code>optimize()</code>.<br />
<code>endWhen</code>: a trigger to stop the training.</p>
<h3 id="scala-example">Scala example</h3>
<p>Here is an example to new an Optimizer with SGD for optimizing LeNet5 model.</p>
<pre><code class="scala">val trainingRDD = ...
val valRDD = ...
val batchSize = 12
// Create an optimizer
val optimizer = Optimizer(
  model = LeNet5(classNum = 10),
  sampleRDD = trainingRDD,
  criterion = ClassNLLCriterion(),
  batchSize = batchSize
).setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy), batchSize) // set validation method
  .setEndWhen(Trigger.maxEpoch(15)) // set end trigger
  .setOptimMethod(new SGD(learningRate = 0.05)) // set optimize method, Since 0.2.0. Older version should use optimizer.setOptimMethod(new SGD()).setState(T(&quot;learningRate&quot; -&gt; 0.05))

val trainedModel = optimizer.optimize()
</code></pre>

<h3 id="python-api">Python API</h3>
<p><strong><em>Factory method</em></strong></p>
<pre><code class="python">optimizer =  Optimizer(model,
                 training_rdd,
                 criterion,
                 end_trigger,
                 batch_size,
                 optim_method=None,
                 bigdl_type=&quot;float&quot;)
</code></pre>

<p><code>model</code>: the model will be optimized.<br />
<code>training_rdd</code>: the training dataset.<br />
<code>criterion</code>: the Loss function.<br />
<code>end_trigger</code>: when to end the optimization.<br />
<code>batch_size</code>: size of minibatch.<br />
<code>optim_method</code>:  the algorithm to use for optimization, e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.<br />
<code>bigdl_type</code>: the numeric type(Float/Double).  </p>
<p><strong><em>Validation</em></strong></p>
<p>Function setValidation is to set a validate evaluation in the <code>optimizer</code>.</p>
<pre><code class="python">optimizer.set_validation(batch_size, val_rdd, trigger, val_method=[&quot;Top1Accuracy&quot;])
</code></pre>

<p><code>trigger</code>: how often to evaluation validation set.<br />
<code>val_rdd</code>: validate data set in type of RDD[Sample].<br />
<code>val_method</code>: a list of ValidationMethod, e.g. "Top1Accuracy", "Top5Accuracy", "Loss".<br />
<code>batch_size</code>: size of mini batch.</p>
<p><strong><em>Checkpoint</em></strong></p>
<pre><code class="python">optimizer.set_checkpoint(checkpoint_trigger,
                      checkpoint_path, isOverWrite=True)
</code></pre>

<p>Function setCheckPoint is used to set a check point saved at <code>path</code> triggered by <code>trigger</code>.<br />
<code>checkpoint_trigger</code>: how often to save the check point.
<code>checkpoint_path</code>: a local/HDFS directory to save checkpoint.<br />
<code>isOverWrite</code>: whether to overwrite existing snapshots in path.default is True</p>
<p><strong><em>Summary</em></strong></p>
<pre><code class="python">optimizer.set_train_summary(summary)
</code></pre>

<p>Set train summary. A TrainSummary object contains information necessary for the optimizer to know how often the logs are recorded, where to store the logs and how to retrieve them, etc. For details, refer to the docs of TrainSummary.
<code>summary</code>: an instance of TrainSummary.</p>
<pre><code class="python">optimizer.set_validation_summary(summary)
</code></pre>

<p>Function setValidationSummary is used to set validation summary. A ValidationSummary object contains information necessary for the optimizer to know how often the logs are recorded, where to store the logs and how to retrieve them, etc. For details, refer to the docs of ValidationSummary.
<code>summary</code>: an instance of ValidationSummary.</p>
<p><strong><em>Start Training</em></strong></p>
<pre><code class="python">trained_model = optimizer.optimize()
</code></pre>

<p>Function optimize will start the training.</p>
<p><strong><em>Set Model</em></strong></p>
<pre><code class="python">optimizer.set_model(model)
</code></pre>

<p>Function setModel will set a new model to the optimizer.<br />
<code>model</code>: a model will replace the old model in optimizer.</p>
<p><strong><em>Set Train Data</em></strong></p>
<pre><code class="python">optimizer.set_traindata(sample_rdd, batch_size)
</code></pre>

<p>set_traindata allows user to replace the train data (for optimizer reuse)</p>
<p><strong><em>Set Criterion</em></strong></p>
<pre><code class="python">optimizer.set_criterion(criterion)
</code></pre>

<p>set_criterion allows user to replace the criterion (for optimizer reuse)</p>
<h3 id="python-example">Python example</h3>
<p>Here is an example to new an Optimizer with SGD for optimizing LeNet5 model.</p>
<pre><code class="python">train_data = ...
test_data = ...
batch_size = 12
# Create an Optimizer, Since 0.2.0
optimizer = Optimizer(
  model=lenet_model,
  training_rdd=train_data,
  criterion=ClassNLLCriterion(),
  optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002), # set optim method
  end_trigger=MaxEpoch(15),
  batch_size=batch_size)

# Older version, before 0.2.0, use following code: 
# optimizer = Optimizer(
#   model=model,
#   training_rdd=train_data,
#   criterion=ClassNLLCriterion(),
#   optim_method=&quot;SGD&quot;,
#   state={&quot;learningRate&quot;: 0.05},
#   end_trigger=MaxEpoch(training_epochs),
#   batch_size=batch_size)

optimizer.set_validation(
    batch_size=2048,
    val_rdd=test_data,
    trigger=EveryEpoch(),
    val_method=[Top1Accuracy()]
)

# Older version, before 0.2.0, use following code: 
#optimizer.set_validation(
#    batch_size=2048,
#    val_rdd=test_data,
#    trigger=EveryEpoch(),
#    val_method=[&quot;Top1Accuracy&quot;]
#)

trained_model = optimizer.optimize()

</code></pre>

<h3 id="how-bigdl-train-models-in-a-distributed-cluster">How BigDL train models in a distributed cluster?</h3>
<p>BigDL distributed training is data parallelism. The training data is split among workers and cached in memory. A complete model is also cached on each worker. The model only uses the data of the same worker in the training.</p>
<p>BigDL employs a synchronous distributed training. In each iteration, each worker will sync the latest weights, calculate gradients with local data and local model, sync the gradients and update the weights with a given optimization method(e.g. SGD, Adagrad).</p>
<p>In gradients and weights sync, BigDL doesn't use the RDD APIs like(broadcast, reduce, aggregate, treeAggregate). The problem of these methods is every worker needs to communicate with driver, so the driver will become the bottleneck if the parameter is too large or the workers are too many. Instead, BigDL implement a P2P algorithm for parameter sync to remove the bottleneck. For detail of the algorithm, please see the <a href="https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/optim/DistriOptimizer.scala">code</a></p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>